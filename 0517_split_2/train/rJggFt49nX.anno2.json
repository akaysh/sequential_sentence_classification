{"review": [{"text_id": "rJggFt49nX", "sid": 0, "sentence": "The authors seek to make it practical to use the full-matrix version of Adagrad\u2019s adaptive preconditioner (usually one uses the diagonal version), by storing the r most recently-seen gradient vectors in a matrix G, and then showing that (GG^T)^(-\u00bd) can be calculated fairly efficiently (at the cost of one r*r matrix inversion, and two matrix multiplications by an r*d matrix)."}, {"text_id": "rJggFt49nX", "sid": 1, "sentence": "This is a really nice trick."}, {"text_id": "rJggFt49nX", "sid": 2, "sentence": "I\u2019m glad to see that the authors considered adding momentum (to adapt ADAM to this setting), and their experiments show a convincing benefit in terms of performance *per iteration*. Interestingly, they also show that the models found by their method also don\u2019t generalize poorly, which is noteworthy and slightly surprising."}, {"text_id": "rJggFt49nX", "sid": 3, "sentence": "However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version."}, {"text_id": "rJggFt49nX", "sid": 4, "sentence": "In Appendix B.1, they report mixed results in terms of wall-clock time, and I strongly feel that these results should be in the main body of the paper."}, {"text_id": "rJggFt49nX", "sid": 5, "sentence": "One would *expect* the proposed approach to work better than diagonal preconditioning on a per-iteration basis (at least in terms of training loss)."}, {"text_id": "rJggFt49nX", "sid": 6, "sentence": "A reader\u2019s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness."}, {"text_id": "rJggFt49nX", "sid": 7, "sentence": "Finally, the proposed approach seems to sort of straddle the line between traditional convex optimization algorithms, and the fast stochastic algorithms favored in machine learning."}, {"text_id": "rJggFt49nX", "sid": 8, "sentence": "In particular, I think that the proposed algorithm has a more-than-superficial resemblance to stochastic LBFGS: the main difference is that LBFGS approximates the inverse Hessian, instead of (GG^T)^(-\u00bd)."}, {"text_id": "rJggFt49nX", "sid": 9, "sentence": "It would be interesting to see how these two algorithms stack up."}, {"text_id": "rJggFt49nX", "sid": 10, "sentence": "Overall, I think that this is an elegant idea and I\u2019m convinced that it\u2019s a good algorithm, at least on a per-iteration basis."}, {"text_id": "rJggFt49nX", "sid": 11, "sentence": "However, it trades-off computational cost for progress-per-iteration, so I think that an explicit analysis of this trade-off (beyond what\u2019s in Appendix B.1) must be in the main body of the paper."}], "reviewlabels": [{"text_id": "rJggFt49nX", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 1, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 4, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 5, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 7, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 8, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggFt49nX", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "B1lP_IyMRm", "sid": 0, "sentence": "Thanks for the review."}, {"text_id": "B1lP_IyMRm", "sid": 1, "sentence": "@Wall-clock: We don\u2019t quite understand the question."}, {"text_id": "B1lP_IyMRm", "sid": 2, "sentence": "As mentioned in the response to Reviewer 3, our NLP example does answer the natural question about end-to-end gains. Is the reviewer only concerned with the location of the plots?"}, {"text_id": "B1lP_IyMRm", "sid": 3, "sentence": "- Another note: to perform a full wall-clock comparison with algorithms that have different per-iteration costs, one must disentangle and retune various hyperparameter choices, most notably the learning rate schedule."}, {"text_id": "B1lP_IyMRm", "sid": 4, "sentence": "Thus we decided to feature the per-iteration comparison in the main paper, as it is the cleanest one."}, {"text_id": "B1lP_IyMRm", "sid": 5, "sentence": "@L-BFGS: On a high level, we agree that GGT develops a similar window-based approximation to the gradient Gram matrix as L-BFGS does to the approximated Hessian."}, {"text_id": "B1lP_IyMRm", "sid": 6, "sentence": "While adaptive methods have proven effective in practice, quasi-Newton algorithms are not in general regarded as competitive for deep learning (despite recent efforts [1,2]), and that\u2019s why it is not compared to in the vast majority of deep learning papers."}, {"text_id": "B1lP_IyMRm", "sid": 7, "sentence": "- Quasi-Newton methods are suited for deterministic problems, while stochasticity is crucial in deep learning."}, {"text_id": "B1lP_IyMRm", "sid": 8, "sentence": "This is because they try to approximate the Hessian by finite differences, which seems unstable with stochastic gradients in practice."}, {"text_id": "B1lP_IyMRm", "sid": 9, "sentence": "- Direct second-order methods require significant modifications to converge in the non-convex setting (see [3,4])."}, {"text_id": "B1lP_IyMRm", "sid": 10, "sentence": "Even these have not been observed to work well in deep learning."}, {"text_id": "B1lP_IyMRm", "sid": 11, "sentence": "- One reason for the practical success of AdaGrad-like algorithms we believe is the difference of  -1/2 vs. -1 power on the Gram matrix, which seems to change the training dynamics dramatically."}, {"text_id": "B1lP_IyMRm", "sid": 12, "sentence": "With the gradient Gram matrix and a -1 power, meaningful end-to-end advances have only been claimed for niche tasks other than classification."}, {"text_id": "B1lP_IyMRm", "sid": 13, "sentence": "[1] Stochastic L-BFGS: Improved Convergence Rates and Practical Acceleration Strategies."}, {"text_id": "B1lP_IyMRm", "sid": 14, "sentence": "R. Zhao and W. Haskell and V. Tan. arXiv, 2017."}, {"text_id": "B1lP_IyMRm", "sid": 15, "sentence": "[2] A Stochastic Quasi-Newton Method for Large-Scale Optimization. R. Byrd, S. Hansen, and J. Nocedal, and Y. Singer SIAM Journal on Optimization, 2016."}, {"text_id": "B1lP_IyMRm", "sid": 16, "sentence": "[3] Accelerated methods for nonconvex optimization."}, {"text_id": "B1lP_IyMRm", "sid": 17, "sentence": "Y. Carmon, J. Duchi, O. Hinder, A. Sidford. SIAM Journal on Optimization, 2018."}, {"text_id": "B1lP_IyMRm", "sid": 18, "sentence": "[4] Finding approximate local minima faster than gradient descent."}, {"text_id": "B1lP_IyMRm", "sid": 19, "sentence": "N. Agarwal, Z. Allen-Zhu, B. Bullins, E. Hazan, and T. Ma. STOC 2017."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "B1lP_IyMRm", "sid": 0}, {"labels": {"alignments": [3, 4, 5, 6], "responsetype": "refute-question", "coarseresponse": "dispute"}, "text_id": "B1lP_IyMRm", "sid": 1}, {"labels": {"alignments": [3, 4, 5, 6], "responsetype": "followup", "coarseresponse": "nonarg"}, "text_id": "B1lP_IyMRm", "sid": 2}, {"labels": {"alignments": [3, 4, 5, 6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 3}, {"labels": {"alignments": [3, 4, 5, 6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 4}, {"labels": {"alignments": [7, 8, 9], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 5}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 6}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 7}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 8}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 9}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 10}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 11}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1lP_IyMRm", "sid": 12}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1lP_IyMRm", "sid": 13}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1lP_IyMRm", "sid": 14}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1lP_IyMRm", "sid": 15}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1lP_IyMRm", "sid": 16}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1lP_IyMRm", "sid": 17}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1lP_IyMRm", "sid": 18}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1lP_IyMRm", "sid": 19}], "metadata": {"anno": "anno2", "review": "rJggFt49nX", "rebuttal": "B1lP_IyMRm", "conference": "ICLR2019", "title": "The Case for Full-Matrix Adaptive Regularization", "reviewer": "AnonReviewer1", "forum_id": "rkxd2oR9Y7", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}