{"review": [{"text_id": "H1ly2z2RFS", "sid": 0, "sentence": "Summary:"}, {"text_id": "H1ly2z2RFS", "sid": 1, "sentence": "This paper proposes to impute multimodal data when certain modalities are present."}, {"text_id": "H1ly2z2RFS", "sid": 2, "sentence": "The authors present a variational selective autoencoder model that learns only from partially-observed data."}, {"text_id": "H1ly2z2RFS", "sid": 3, "sentence": "VSAE is capable of learning the joint"}, {"text_id": "H1ly2z2RFS", "sid": 4, "sentence": "distribution of observed and unobserved modalities as well as the imputation mask, resulting in a model that is suitable for various down-stream tasks including data generation and imputation"}, {"text_id": "H1ly2z2RFS", "sid": 5, "sentence": "."}, {"text_id": "H1ly2z2RFS", "sid": 6, "sentence": "The authors evaluate on both synthetic high-dimensional and challenging low-dimensional multimodal datasets and show improvement over the state-of-the-art data imputation models."}, {"text_id": "H1ly2z2RFS", "sid": 7, "sentence": "Strengths:"}, {"text_id": "H1ly2z2RFS", "sid": 8, "sentence": "- This is an interesting paper that is well written and motivated."}, {"text_id": "H1ly2z2RFS", "sid": 9, "sentence": "- The authors show good results on several multimodal datasets, improving upon several recent works in learning from missing multimodal data."}, {"text_id": "H1ly2z2RFS", "sid": 10, "sentence": "Weaknesses:"}, {"text_id": "H1ly2z2RFS", "sid": 11, "sentence": "- How multimodal are the datasets provided by UCI? It seems like they consist of different tabular datasets with numerical or categorical variables, but it was not clear what the modalities are (each variable is a modality?) and how correlated the modalities are. If they are not correlated at all and share no joint information I'm not sure how these experiments can represent multimodal data."}, {"text_id": "H1ly2z2RFS", "sid": 12, "sentence": "- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets."}, {"text_id": "H1ly2z2RFS", "sid": 13, "sentence": "They should consider larger-scale datasets including image and text-based like VQA/VCR, or video-based like the datasets in (Tsai et al., ICLR 2019)."}, {"text_id": "H1ly2z2RFS", "sid": 14, "sentence": "- In terms of prediction performance, the authors should also compare to [1] and [2] which either predict the other modalities completely during training or use tensor-based methods to learn from noisy or missing time-series data."}, {"text_id": "H1ly2z2RFS", "sid": 15, "sentence": "- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?"}, {"text_id": "H1ly2z2RFS", "sid": 16, "sentence": "[1] Pham et al. Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities, AAAI 2019"}, {"text_id": "H1ly2z2RFS", "sid": 17, "sentence": "[2] Liang et al. Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization, ACL 2019"}, {"text_id": "H1ly2z2RFS", "sid": 18, "sentence": "### Post rebuttal"}, {"text_id": "H1ly2z2RFS", "sid": 19, "sentence": "#"}, {"text_id": "H1ly2z2RFS", "sid": 20, "sentence": "##"}, {"text_id": "H1ly2z2RFS", "sid": 21, "sentence": "Thank you for your detailed answers to my questions."}], "reviewlabels": [{"text_id": "H1ly2z2RFS", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "H1ly2z2RFS", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "H1ly2z2RFS", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "H1ly2z2RFS", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 10, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 14, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 16, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 17, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1ly2z2RFS", "sid": 18, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "H1ly2z2RFS", "sid": 19, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "H1ly2z2RFS", "sid": 20, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "H1ly2z2RFS", "sid": 21, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "HJlxtt_Osr", "sid": 0, "sentence": "(1) Multimodal setting:"}, {"text_id": "HJlxtt_Osr", "sid": 1, "sentence": "We apologize for not describing experimental settings clearly."}, {"text_id": "HJlxtt_Osr", "sid": 2, "sentence": "In general, we believe multi-modal data is more general than simply image-text or video-text pair."}, {"text_id": "HJlxtt_Osr", "sid": 3, "sentence": "By unifying tabular data also as multi-modal data (with each attribute as one modality), we show that VASE provides us a principled way for imputation and is capable of generalizing to more data families."}, {"text_id": "HJlxtt_Osr", "sid": 4, "sentence": "We update additional multimodal dataset experiments in the point (3) below."}, {"text_id": "HJlxtt_Osr", "sid": 5, "sentence": "(2) Prediction and Representation learning:"}, {"text_id": "HJlxtt_Osr", "sid": 6, "sentence": "We consider conducting these experiments during the rebuttal but none of the paper's code has been released by the authors."}, {"text_id": "HJlxtt_Osr", "sid": 7, "sentence": "We agree deep latent variable models explicitly model the data distribution and provide a natural way for representation learning, but in our paper we evaluate the model from the perspective of imputation and generation."}, {"text_id": "HJlxtt_Osr", "sid": 8, "sentence": "(3) Additional experiments:"}, {"text_id": "HJlxtt_Osr", "sid": 9, "sentence": "We updated additional imputation experiments on multimodal datasets (see in Appendix C.5) : CMU-MOSI/ICT-MMMO (Tsai et al. 2019), FashionMNIST/MNIST (Wu et al. 2018)."}, {"text_id": "HJlxtt_Osr", "sid": 10, "sentence": "Each dataset contains two or three modalities."}, {"text_id": "HJlxtt_Osr", "sid": 11, "sentence": "VSAE outperforms other baselines on multimodal datasets under partially-observed setting."}, {"text_id": "HJlxtt_Osr", "sid": 12, "sentence": "(4) Require mask during training:"}, {"text_id": "HJlxtt_Osr", "sid": 13, "sentence": "In our experiments, the binary mask is always fully-observed as is the nature of partially-observed data."}, {"text_id": "HJlxtt_Osr", "sid": 14, "sentence": "A mask simply indicates which  modalities are observed and which are not."}, {"text_id": "HJlxtt_Osr", "sid": 15, "sentence": "We agree that it is very interesting to design a model with partially-observed or even unobserved mask."}, {"text_id": "HJlxtt_Osr", "sid": 16, "sentence": "However, it is beyond the scope of this work and we will consider it in future work."}, {"text_id": "HJlxtt_Osr", "sid": 17, "sentence": "[1] Wu et al. Multimodal Generative Models for Scalable Weakly-Supervised Learning, NeurIPS 2018."}, {"text_id": "HJlxtt_Osr", "sid": 18, "sentence": "[2] Tsai et al. Learning Factorized Multimodal Representation, ICLR 2019."}], "rebuttallabels": [{"labels": {"alignments": [11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJlxtt_Osr", "sid": 0}, {"labels": {"alignments": [11], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "HJlxtt_Osr", "sid": 1}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJlxtt_Osr", "sid": 2}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJlxtt_Osr", "sid": 3}, {"labels": {"alignments": [11], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "HJlxtt_Osr", "sid": 4}, {"labels": {"alignments": [14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJlxtt_Osr", "sid": 5}, {"labels": {"alignments": [14], "responsetype": "reject-request_scope_No", "coarseresponse": "dispute"}, "text_id": "HJlxtt_Osr", "sid": 6}, {"labels": {"alignments": [14], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "HJlxtt_Osr", "sid": 7}, {"labels": {"alignments": [12], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJlxtt_Osr", "sid": 8}, {"labels": {"alignments": [12], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "HJlxtt_Osr", "sid": 9}, {"labels": {"alignments": [12], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "HJlxtt_Osr", "sid": 10}, {"labels": {"alignments": [12], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "HJlxtt_Osr", "sid": 11}, {"labels": {"alignments": [15], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJlxtt_Osr", "sid": 12}, {"labels": {"alignments": [15], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJlxtt_Osr", "sid": 13}, {"labels": {"alignments": [15], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJlxtt_Osr", "sid": 14}, {"labels": {"alignments": [15], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "HJlxtt_Osr", "sid": 15}, {"labels": {"alignments": [15], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "HJlxtt_Osr", "sid": 16}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "HJlxtt_Osr", "sid": 17}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "HJlxtt_Osr", "sid": 18}], "metadata": {"anno": "anno2", "review": "H1ly2z2RFS", "rebuttal": "HJlxtt_Osr", "conference": "ICLR2020", "title": "Learning from Partially-Observed Multimodal Data with Variational Autoencoders", "reviewer": "AnonReviewer2", "forum_id": "rylT0AVtwH", "rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years."}}