{"review": [{"text_id": "r1xY4JK62Q", "sid": 0, "sentence": "I read the paper and understand it, for the most part."}, {"text_id": "r1xY4JK62Q", "sid": 1, "sentence": "The idea is to interpret some regularization technics as a from of noisy bottleneck, where the mutual information b tween learned parameters and the data is limited through the injection of noise."}, {"text_id": "r1xY4JK62Q", "sid": 2, "sentence": "While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion."}, {"text_id": "r1xY4JK62Q", "sid": 3, "sentence": "I'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156)"}, {"text_id": "r1xY4JK62Q", "sid": 4, "sentence": "Pro: nicely written, clear interpretation of regularization as a noise injection technics, explicit link with information theoery and Shanon capacity."}, {"text_id": "r1xY4JK62Q", "sid": 5, "sentence": "Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation"}], "reviewlabels": [{"text_id": "r1xY4JK62Q", "sid": 0, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1xY4JK62Q", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1xY4JK62Q", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1xY4JK62Q", "sid": 3, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1xY4JK62Q", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1xY4JK62Q", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "ryxS38C_am", "sid": 0, "sentence": "Thank you very much for your encouraging review."}, {"text_id": "ryxS38C_am", "sid": 1, "sentence": "> I read the paper and understand it, for the most part."}, {"text_id": "ryxS38C_am", "sid": 2, "sentence": "The idea is to interpret some regularization techniques as a form of noisy bottleneck, where the mutual information between learned parameters and the data is limited through the injection of noise."}, {"text_id": "ryxS38C_am", "sid": 3, "sentence": "While the paper is a pleasant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation."}, {"text_id": "ryxS38C_am", "sid": 4, "sentence": "Perhaps other referee will have a clearer opinion."}, {"text_id": "ryxS38C_am", "sid": 5, "sentence": "The main contribution of our paper is indeed to establish a connection between variational inference and regularization by observing that Gaussian mean field introduces an upper bound on the mutual information between data and model parameters."}, {"text_id": "ryxS38C_am", "sid": 6, "sentence": "Reinterpreting mean field as point estimation in a noisy model allows us to quantify observed regularizing effects."}, {"text_id": "ryxS38C_am", "sid": 7, "sentence": "We show links to existing regularization strategies and validate the usefulness for regularization in targeted experiments."}, {"text_id": "ryxS38C_am", "sid": 8, "sentence": "While the focus of our present work lies on establishing links between existing directions of research, we believe that our information-theoretic perspective on regularization opens up plenty of avenues for future work, both in supervised and unsupervised learning."}, {"text_id": "ryxS38C_am", "sid": 9, "sentence": "For example, we are interested in improving extraction of unsupervised representations by controlling the amount of extracted information."}, {"text_id": "ryxS38C_am", "sid": 10, "sentence": "In particular, we aim to mitigate latent collapse, a problem reported for example in language generation [1] and autoregressive image generation [2], which is currently mitigated with ad-hoc strategies such as KL annealing."}, {"text_id": "ryxS38C_am", "sid": 11, "sentence": "Intuitively, if all information can be stored in the model itself, there is little incentive to use a per-sample latent."}, {"text_id": "ryxS38C_am", "sid": 12, "sentence": "This is also known as the information preference problem, as briefly discussed at the end of section 2.1."}, {"text_id": "ryxS38C_am", "sid": 13, "sentence": "Therefore, limiting mutual information of the data with the model might offer a robust mitigation strategy."}, {"text_id": "ryxS38C_am", "sid": 14, "sentence": "Additionally, we believe that the approach can lead to improved representations through disentanglement, as done by beta-VAE [3]."}, {"text_id": "ryxS38C_am", "sid": 15, "sentence": "Our formal connection to beta-VAE derived in Appendix C offers a promising information-theoretic perspective on their empirical results."}, {"text_id": "ryxS38C_am", "sid": 16, "sentence": "More generally, we want to explore non-MAP inference on noise-injected models as this would allow for using highly expressive variational distributions while enjoying the information-theoretic guarantees of simpler approximate distributions, as motivated in section 3.3."}, {"text_id": "ryxS38C_am", "sid": 17, "sentence": "Since these directions are rather orthogonal, we think that sharing our theoretical framework with the community in an independent piece of work is the most effective way of communicating our ideas."}, {"text_id": "ryxS38C_am", "sid": 18, "sentence": "> I'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156)"}, {"text_id": "ryxS38C_am", "sid": 19, "sentence": "Reference priors are opposite to our work in the sense that they maximize the amount of information data provides about the parameters, while we aim to find models to limit it."}, {"text_id": "ryxS38C_am", "sid": 20, "sentence": "Also, see [4] for the relation of Fisher information to generalization."}, {"text_id": "ryxS38C_am", "sid": 21, "sentence": "References"}, {"text_id": "ryxS38C_am", "sid": 22, "sentence": "[1] Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R. & Bengio, S. (2015). Generating sentences from a continuous space."}, {"text_id": "ryxS38C_am", "sid": 23, "sentence": "arXiv preprint arXiv:1511.06349."}, {"text_id": "ryxS38C_am", "sid": 24, "sentence": "[2] Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D. & Courville, A. (2016). Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013."}, {"text_id": "ryxS38C_am", "sid": 25, "sentence": "[3] Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G. & Lerchner, A. (2018). Understanding disentangling in beta-VAE."}, {"text_id": "ryxS38C_am", "sid": 26, "sentence": "arXiv preprint arXiv:1804.03599."}, {"text_id": "ryxS38C_am", "sid": 27, "sentence": "[4] Ly, A., Marsman, M., Verhagen, J., Grasman, R. P. & Wagenmakers, E. J. (2017). A tutorial on Fisher information."}, {"text_id": "ryxS38C_am", "sid": 28, "sentence": "Journal of Mathematical Psychology, 80, 40-55, page 30"}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 0}, {"labels": {"alignments": [0], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 1}, {"labels": {"alignments": [1], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 2}, {"labels": {"alignments": [2], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 3}, {"labels": {"alignments": [2], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 4}, {"labels": {"alignments": [0, 1], "responsetype": "accept-praise", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 5}, {"labels": {"alignments": [0, 1], "responsetype": "accept-praise", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 6}, {"labels": {"alignments": [0, 1], "responsetype": "accept-praise", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 7}, {"labels": {"alignments": [0, 1, 2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 8}, {"labels": {"alignments": [0, 1, 2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 9}, {"labels": {"alignments": [0, 1, 2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 10}, {"labels": {"alignments": [0, 1, 2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 11}, {"labels": {"alignments": [0, 1, 2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 12}, {"labels": {"alignments": [0, 1, 2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 13}, {"labels": {"alignments": [0, 1, 2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 14}, {"labels": {"alignments": [0, 1, 2], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 15}, {"labels": {"alignments": [0, 1, 2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 16}, {"labels": {"alignments": [0, 1, 2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 17}, {"labels": {"alignments": [3], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 18}, {"labels": {"alignments": [3], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 19}, {"labels": {"alignments": [3], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxS38C_am", "sid": 20}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 21}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 22}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 23}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 24}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 25}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 26}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 27}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxS38C_am", "sid": 28}], "metadata": {"anno": "anno13", "review": "r1xY4JK62Q", "rebuttal": "ryxS38C_am", "conference": "ICLR2019", "title": "Noisy Information Bottlenecks for Generalization", "reviewer": "AnonReviewer3", "forum_id": "HJeQbnA5tm", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}