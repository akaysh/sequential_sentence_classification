{"review": [{"text_id": "BJlqUqW3tH", "sid": 0, "sentence": "Summary: the paper purposes a dataset of abductive language inference and generation."}, {"text_id": "BJlqUqW3tH", "sid": 1, "sentence": "The dataset is generated by human, while the testing set is adversarially selected using BERT."}, {"text_id": "BJlqUqW3tH", "sid": 2, "sentence": "The paper experiments the popular deep learning models on the dataset and observe shortcoming of deep learning on this task."}, {"text_id": "BJlqUqW3tH", "sid": 3, "sentence": "Comments: overall, the problem on abductive inference and abductive generation in language in very interesting and important."}, {"text_id": "BJlqUqW3tH", "sid": 4, "sentence": "This dataset seems valuable. And the paper is simple and well-written."}, {"text_id": "BJlqUqW3tH", "sid": 5, "sentence": "Concerns: I find the claim on deep networks kind of irresponsible."}, {"text_id": "BJlqUqW3tH", "sid": 6, "sentence": "1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked."}, {"text_id": "BJlqUqW3tH", "sid": 7, "sentence": "2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction."}, {"text_id": "BJlqUqW3tH", "sid": 8, "sentence": "To compare the author should use the average score of human."}, {"text_id": "BJlqUqW3tH", "sid": 9, "sentence": "3. The ground truth is selected by human."}, {"text_id": "BJlqUqW3tH", "sid": 10, "sentence": "On a high level, the main difficulty of abduction is to search in the exponentially large space of hypothesis."}, {"text_id": "BJlqUqW3tH", "sid": 11, "sentence": "Formulating the abduction task as a (binary) classification problem is less interesting."}, {"text_id": "BJlqUqW3tH", "sid": 12, "sentence": "The generative task is a better option."}, {"text_id": "BJlqUqW3tH", "sid": 13, "sentence": "Decision: despite the seeming unfair comparison, this task is novel. I vote for weak accept."}], "reviewlabels": [{"text_id": "BJlqUqW3tH", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 7, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 9, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 10, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 12, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlqUqW3tH", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "H1gU-PuqiS", "sid": 0, "sentence": "We appreciate AnonReviewer3 for encouraging comments about the importance of the proposed abductive inference and generation tasks and about the value of our proposed dataset."}, {"text_id": "H1gU-PuqiS", "sid": 1, "sentence": "We address the main concerns individually below:"}, {"text_id": "H1gU-PuqiS", "sid": 2, "sentence": "Adversarially filtering using BERT and GPT gives deep learning models a disadvantage:"}, {"text_id": "H1gU-PuqiS", "sid": 3, "sentence": "While BERT originally achieved high performance on the originally collected dataset, several recent studies [1][2][3][4] have found the presence of annotation artifacts in crowdsourced data that inadvertently leak information about the target label."}, {"text_id": "H1gU-PuqiS", "sid": 4, "sentence": "This subsequently leads to overestimation of the performance of AI systems on end tasks."}, {"text_id": "H1gU-PuqiS", "sid": 5, "sentence": "Our adversarial filtering (AF) algorithm aims to address the problem of overestimation of performance."}, {"text_id": "H1gU-PuqiS", "sid": 6, "sentence": "In spite of targeting GPT/BERT during AF, human performance on the AF resulting dataset is still high."}, {"text_id": "H1gU-PuqiS", "sid": 7, "sentence": "The significant gap between human and BERT performance leaves scope for inventing new methods for abductive reasoning."}, {"text_id": "H1gU-PuqiS", "sid": 8, "sentence": "Ensemble of BERT models:"}, {"text_id": "H1gU-PuqiS", "sid": 9, "sentence": "An ensemble of three BERT models achieves an accuracy of 68.9%, very close to a single model 68.6%."}, {"text_id": "H1gU-PuqiS", "sid": 10, "sentence": "Average score of human:"}, {"text_id": "H1gU-PuqiS", "sid": 11, "sentence": "The average score of human annotations is 89.4%."}, {"text_id": "H1gU-PuqiS", "sid": 12, "sentence": "This is directly comparable with BERT-Ft [Fully Connected] model\u2019s performance of 68.6% in Table 1."}, {"text_id": "H1gU-PuqiS", "sid": 13, "sentence": "Re. Ground Truth:"}, {"text_id": "H1gU-PuqiS", "sid": 14, "sentence": "The ground truth is assigned based on whether a hypothesis was collected during the plausible (Appendix A1 Task1) or implausible (Appendix A1 Task2) phase of the data collection procedure."}, {"text_id": "H1gU-PuqiS", "sid": 15, "sentence": "To measure human performance, we had three annotators select the correct hypothesis and measured human performance as the accuracy of their majority-vote."}, {"text_id": "H1gU-PuqiS", "sid": 16, "sentence": "Please let us know if this answers your question. If not, could you please clarify your question?"}, {"text_id": "H1gU-PuqiS", "sid": 17, "sentence": "Generative task vs classification:"}, {"text_id": "H1gU-PuqiS", "sid": 18, "sentence": "We completely agree."}, {"text_id": "H1gU-PuqiS", "sid": 19, "sentence": "While the generative task is more general and much more interesting, the challenge of evaluating generations is significant, particularly for this task."}, {"text_id": "H1gU-PuqiS", "sid": 20, "sentence": "This is due to the fact that there could be multiple distinct plausible explanations for a given pair of hypothesis."}, {"text_id": "H1gU-PuqiS", "sid": 21, "sentence": "Consider the following example:"}, {"text_id": "H1gU-PuqiS", "sid": 22, "sentence": "O1: Kelly and her friend wanted to take a train to the city."}, {"text_id": "H1gU-PuqiS", "sid": 23, "sentence": "O2: They had to wait for another one."}, {"text_id": "H1gU-PuqiS", "sid": 24, "sentence": "Plausible explanations:"}, {"text_id": "H1gU-PuqiS", "sid": 25, "sentence": "1. They read the timetable incorrectly and arrived at the station just after a train had left."}, {"text_id": "H1gU-PuqiS", "sid": 26, "sentence": "2. The train was full."}, {"text_id": "H1gU-PuqiS", "sid": 27, "sentence": "Both explanations are plausible, and explain the observations, but automated evaluation metrics are not reliable enough to capture this phenomenon based on their reliance on surface level similarities."}, {"text_id": "H1gU-PuqiS", "sid": 28, "sentence": "To simultaneously make progress on the novel abductive reasoning task and due to the ease of evaluation, we additionally introduce a discriminative version of the task."}, {"text_id": "H1gU-PuqiS", "sid": 29, "sentence": "Nonetheless, we agree that in its most general form, there could be any number of observations and models should be required to generate explanatory hypotheses in natural language (alpha-NLG task)."}, {"text_id": "H1gU-PuqiS", "sid": 30, "sentence": "[1] Gururangan et al. Annotation artifacts in natural language inference data."}, {"text_id": "H1gU-PuqiS", "sid": 31, "sentence": "[2] Poliak et al. Hypothesis only baselines in natural language inference."}, {"text_id": "H1gU-PuqiS", "sid": 32, "sentence": "[3] Tsuchiya et al. Performance impact caused by hidden bias of training data for recognizing textual entailment."}, {"text_id": "H1gU-PuqiS", "sid": 33, "sentence": "[4] Sakaguchi et al. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "accept-praise", "coarseresponse": "concur"}, "text_id": "H1gU-PuqiS", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 1}, {"labels": {"alignments": [6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 2}, {"labels": {"alignments": [6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 3}, {"labels": {"alignments": [6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 4}, {"labels": {"alignments": [6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 5}, {"labels": {"alignments": [6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 6}, {"labels": {"alignments": [6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 7}, {"labels": {"alignments": [7], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 8}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1gU-PuqiS", "sid": 9}, {"labels": {"alignments": [8], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 10}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1gU-PuqiS", "sid": 11}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1gU-PuqiS", "sid": 12}, {"labels": {"alignments": [9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 13}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1gU-PuqiS", "sid": 14}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1gU-PuqiS", "sid": 15}, {"labels": {"alignments": [9], "responsetype": "followup", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 16}, {"labels": {"alignments": [10, 11, 12], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 17}, {"labels": {"alignments": [10, 11, 12], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "H1gU-PuqiS", "sid": 18}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 19}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 20}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 21}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 22}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 23}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 24}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 25}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 26}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 27}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 28}, {"labels": {"alignments": [10, 11, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "H1gU-PuqiS", "sid": 29}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 30}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 31}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 32}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "H1gU-PuqiS", "sid": 33}], "metadata": {"anno": "anno2", "review": "BJlqUqW3tH", "rebuttal": "H1gU-PuqiS", "conference": "ICLR2020", "title": "Abductive Commonsense Reasoning", "reviewer": "AnonReviewer3", "forum_id": "Byg1v1HKDB", "rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area."}}