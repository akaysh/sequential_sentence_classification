{"review": [{"text_id": "rylkjAtu2m", "sid": 0, "sentence": "The paper examines an architectural feature in GAN generators -- self-modulation -- and presents empirical evidence supporting the claim that it helps improve modeling performance."}, {"text_id": "rylkjAtu2m", "sid": 1, "sentence": "The self-modulation mechanism itself is implemented via FiLM layers applied to all convolutional blocks in the generator and whose scaling and shifting parameters are predicted as a function of the noise vector z."}, {"text_id": "rylkjAtu2m", "sid": 2, "sentence": "Performance is measured in terms of Fr\u00e9chet Inception Distance (FID) for models trained with and without self-modulation on a fairly comprehensive range of model architectures (DCGAN-based, ResNet-based), discriminator regularization techniques (gradient penalty, spectral normalization), and datasets (CIFAR10, CelebA-HQ, LSUN-Bedroom, ImageNet)."}, {"text_id": "rylkjAtu2m", "sid": 3, "sentence": "The takeaway is that self-modulation is an architectural feature that helps improve modeling performance by a significant margin in most settings."}, {"text_id": "rylkjAtu2m", "sid": 4, "sentence": "An ablation study is also performed on the location where self-modulation is applied, showing that it is beneficial across all locations but has more impact towards the later layers of the generator."}, {"text_id": "rylkjAtu2m", "sid": 5, "sentence": "I am overall positive about the paper: the proposed idea is simple, but is well-explained and backed by rigorous evaluation."}, {"text_id": "rylkjAtu2m", "sid": 6, "sentence": "Here are the questions I would like the authors to discuss further:"}, {"text_id": "rylkjAtu2m", "sid": 7, "sentence": "- The proposed approach is a fairly specific form of self-modulation."}, {"text_id": "rylkjAtu2m", "sid": 8, "sentence": "In general, I think of self-modulation as a way for the network to interact with itself, which can be a local interaction, like for squeeze-and-excitation blocks."}, {"text_id": "rylkjAtu2m", "sid": 9, "sentence": "In the case of this paper, the self-interaction allows the noise vector z to interact with various intermediate features across the generation process, which for me appears to be different than allowing intermediate features to interact with themselves."}, {"text_id": "rylkjAtu2m", "sid": 10, "sentence": "This form of noise injection at various levels of the generator is also close in spirit to what BigGAN employs, except that in the case of BigGAN different parts of the noise vector are used to influence different parts of the generator."}, {"text_id": "rylkjAtu2m", "sid": 11, "sentence": "Can you clarify how you view the relationship between the approaches mentioned above?"}, {"text_id": "rylkjAtu2m", "sid": 12, "sentence": "- It\u2019s interesting to me that the ResNet architecture performs better with self-modulation in all settings, considering that one possible explanation for why self-modulation is helpful is that it allows the \u201cinformation\u201d contained in the noise vector to better propagate to and influence different parts of the generator."}, {"text_id": "rylkjAtu2m", "sid": 13, "sentence": "ResNets also have this ability to \u201cpropagate\u201d the noise signal more easily, but it appears that having a self-modulation mechanism on top of that is still beneficial."}, {"text_id": "rylkjAtu2m", "sid": 14, "sentence": "I\u2019m curious to hear the authors\u2019 thoughts in this."}, {"text_id": "rylkjAtu2m", "sid": 15, "sentence": "- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?"}], "reviewlabels": [{"text_id": "rylkjAtu2m", "sid": 0, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 1, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 2, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 3, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 4, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 8, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 9, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 10, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 12, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 13, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 14, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylkjAtu2m", "sid": 15, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rylaP-uP6m", "sid": 0, "sentence": "We would like to thank the reviewer for the time and useful feedback."}, {"text_id": "rylaP-uP6m", "sid": 1, "sentence": "Our response is given below."}, {"text_id": "rylaP-uP6m", "sid": 2, "sentence": "- Relationship to z-conditioning strategy in BigGAN."}, {"text_id": "rylaP-uP6m", "sid": 3, "sentence": "Thanks for pointing out the connection to this concurrent submission."}, {"text_id": "rylaP-uP6m", "sid": 4, "sentence": "We will discuss the connections in the related work section."}, {"text_id": "rylaP-uP6m", "sid": 5, "sentence": "The main differences are as follows:"}, {"text_id": "rylaP-uP6m", "sid": 6, "sentence": "1. BigGAN performs conditional generation, whilst we primarily focus on unconditional generation."}, {"text_id": "rylaP-uP6m", "sid": 7, "sentence": "BigGAN splits the latent vector z and concatenates it with the label embedding, whereas we transform z using a small MLP per layer, which is arguably more powerful."}, {"text_id": "rylaP-uP6m", "sid": 8, "sentence": "In the conditional case, we apply both additive and multiplicative interaction between the label and z, instead of concatenation as in BigGAN."}, {"text_id": "rylaP-uP6m", "sid": 9, "sentence": "2. Overall BigGAN focusses on scalability to demonstrate that one can train an impressive model for conditional generation."}, {"text_id": "rylaP-uP6m", "sid": 10, "sentence": "Instead, we focus on a single idea, and show that it can be applied very broadly."}, {"text_id": "rylaP-uP6m", "sid": 11, "sentence": "We provide a thorough empirical evaluation across critical design decisions in GANs and demonstrate that it is a robust and practically useful contribution."}, {"text_id": "rylaP-uP6m", "sid": 12, "sentence": "- Propagation of signal and ResNets."}, {"text_id": "rylaP-uP6m", "sid": 13, "sentence": "Indeed, ResNets provide a skip connection which helps signal propagation."}, {"text_id": "rylaP-uP6m", "sid": 14, "sentence": "Arguably, self-modulation has a similar effect."}, {"text_id": "rylaP-uP6m", "sid": 15, "sentence": "However, there are critical differences in these mechanisms which may explain the benefits of self-modulation in a resnet architecture:"}, {"text_id": "rylaP-uP6m", "sid": 16, "sentence": "1. Self-modulation applies a channel-wise additive and multiplicative operation to each layer."}, {"text_id": "rylaP-uP6m", "sid": 17, "sentence": "In contrast, residual connections perform only an element-wise addition in the same spatial locality."}, {"text_id": "rylaP-uP6m", "sid": 18, "sentence": "As a result, channel-wise modulation allows trainable re-weighting of all feature maps, which is not the case for classic residual connections."}, {"text_id": "rylaP-uP6m", "sid": 19, "sentence": "2. The ResNet skip-connection is either an identity function or a learnable 1x1 convolution, both of which are linear."}, {"text_id": "rylaP-uP6m", "sid": 20, "sentence": "In self-modulation, the connection from z to each layer is a learnable non-linear function (MLP)."}, {"text_id": "rylaP-uP6m", "sid": 21, "sentence": "- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?"}, {"text_id": "rylaP-uP6m", "sid": 22, "sentence": "Yes, we notice more improvements on the harder, more diverse datasets."}, {"text_id": "rylaP-uP6m", "sid": 23, "sentence": "These datasets also have more headroom for improvement."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rylaP-uP6m", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rylaP-uP6m", "sid": 1}, {"labels": {"alignments": [10, 11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rylaP-uP6m", "sid": 2}, {"labels": {"alignments": [10, 11], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rylaP-uP6m", "sid": 3}, {"labels": {"alignments": [10, 11], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rylaP-uP6m", "sid": 4}, {"labels": {"alignments": [10, 11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rylaP-uP6m", "sid": 5}, {"labels": {"alignments": [10, 11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 6}, {"labels": {"alignments": [10, 11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 7}, {"labels": {"alignments": [10, 11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 8}, {"labels": {"alignments": [10, 11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 9}, {"labels": {"alignments": [10, 11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 10}, {"labels": {"alignments": [10, 11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 11}, {"labels": {"alignments": [13, 14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rylaP-uP6m", "sid": 12}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 13}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 14}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 15}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 16}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 17}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 18}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 19}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 20}, {"labels": {"alignments": [15], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rylaP-uP6m", "sid": 21}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 22}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rylaP-uP6m", "sid": 23}], "metadata": {"anno": "anno8", "review": "rylkjAtu2m", "rebuttal": "rylaP-uP6m", "conference": "ICLR2019", "title": "On Self Modulation for Generative Adversarial Networks", "reviewer": "AnonReviewer1", "forum_id": "Hkl5aoR5tm", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}