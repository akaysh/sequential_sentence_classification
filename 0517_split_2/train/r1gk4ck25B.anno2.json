{"review": [{"text_id": "r1gk4ck25B", "sid": 0, "sentence": "This paper proposes variational selective autoencoders (VSAE) to learn the joint distribution model of full data (both observed and unobserved modalities) and the mask information from arbitrary partial-observation data."}, {"text_id": "r1gk4ck25B", "sid": 1, "sentence": "To infer latent variables from partial-observation data, they introduce the selective proposal distribution that switches encoders depending on whether each input modality is observed."}, {"text_id": "r1gk4ck25B", "sid": 2, "sentence": "This paper is well written, and the method proposed in this paper is nice."}, {"text_id": "r1gk4ck25B", "sid": 3, "sentence": "In particular, the idea of the selective proposal distribution is interesting and provides an effective solution to deal with the problem of missing modality in conventional multimodal learning."}, {"text_id": "r1gk4ck25B", "sid": 4, "sentence": "The experiment is also well structured and shows higher performance than the existing models."}, {"text_id": "r1gk4ck25B", "sid": 5, "sentence": "However, I have some questions and comments, so I\u2019d like you to answer them."}, {"text_id": "r1gk4ck25B", "sid": 6, "sentence": "Comments:"}, {"text_id": "r1gk4ck25B", "sid": 7, "sentence": "- The authors state that x_j is sampled from the \"prior network\" to calculate E_x_j in Equation 10, but I didn\u2019t understand how this network is set up. Could you explain it in detail?"}, {"text_id": "r1gk4ck25B", "sid": 8, "sentence": "- The authors claim that adding p(m|z) to the objective function (i.e., generating m from the decoder) allows the latent variable to have mask information."}, {"text_id": "r1gk4ck25B", "sid": 9, "sentence": "However, I don\u2019t know how effective this is in practice."}, {"text_id": "r1gk4ck25B", "sid": 10, "sentence": "Specifically, how performance differs compared to when p (m | z) is not used and the decoder p (x | z, m) is conditioned by the mask included in the training set instead of the generated mask?"}, {"text_id": "r1gk4ck25B", "sid": 11, "sentence": "- Why did you not do image inpainting in higher-dimensional experiments like Ivanov et al. (2019), i.e., considering each pixel as a different modality? Of course, I know that Ivanov et al. require the full data as input during training, but I\u2019m interested in whether VSAE can perform inpainting properly even if trained given imperfect images."}], "reviewlabels": [{"text_id": "r1gk4ck25B", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 5, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 7, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Quote", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1gk4ck25B", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "ryxqjgdujS", "sid": 0, "sentence": "(1) Prior Network:"}, {"text_id": "ryxqjgdujS", "sid": 1, "sentence": "During training phase, we sample from prior network to generate \"pseudo\" observations for unobserved modalities."}, {"text_id": "ryxqjgdujS", "sid": 2, "sentence": "The pseudo observations are then used to estimate the conditional likelihood for such modalities (E_x_j in the ELBO)."}, {"text_id": "ryxqjgdujS", "sid": 3, "sentence": "Practically, we follow a two-stage method in our implementation."}, {"text_id": "ryxqjgdujS", "sid": 4, "sentence": "At each iteration, the first stage imputes unobserved modalities (with latent code sampled from approximate posterior for observed modalities, and prior for unobserved modalities), followed by the second stage to estimate ELBO based on the imputation and backpropagate corresponding gradients."}, {"text_id": "ryxqjgdujS", "sid": 5, "sentence": "(2) Conditioning on Ground-Truth Mask:"}, {"text_id": "ryxqjgdujS", "sid": 6, "sentence": "We conduct experiments with decoder p(x|z, m) conditioned on the original mask in training set, and observe comparable performance and convergence time."}, {"text_id": "ryxqjgdujS", "sid": 7, "sentence": "The mask distribution might be easier to learn as compared to data distribution (since the mask is fully-observed)"}, {"text_id": "ryxqjgdujS", "sid": 8, "sentence": "."}, {"text_id": "ryxqjgdujS", "sid": 9, "sentence": "However, we argue that jointly learning the mask distribution and data distribution provides us an opportunity to further analyze the missing mechanism and potentially can facilitate other down-stream tasks."}, {"text_id": "ryxqjgdujS", "sid": 10, "sentence": "(3) Image Inpainting:"}, {"text_id": "ryxqjgdujS", "sid": 11, "sentence": "We appreciate the reviewer's suggestion on evaluate the effectiveness of our model on image inpainting task."}, {"text_id": "ryxqjgdujS", "sid": 12, "sentence": "However, with our current setup, an encoder is trained for each modality respectively, making it difficult to scale to inpainting task, if we treat each pixel as an individual modality."}, {"text_id": "ryxqjgdujS", "sid": 13, "sentence": "Nevertheless, we believe this is an interesting extension."}, {"text_id": "ryxqjgdujS", "sid": 14, "sentence": "The backbone models and mathematical formulations can be very similar, if not the same."}, {"text_id": "ryxqjgdujS", "sid": 15, "sentence": "A potential solution could be to employ patch level encoders to reduce the total number of encoders needed."}], "rebuttallabels": [{"labels": {"alignments": [7], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxqjgdujS", "sid": 0}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxqjgdujS", "sid": 1}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxqjgdujS", "sid": 2}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxqjgdujS", "sid": 3}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxqjgdujS", "sid": 4}, {"labels": {"alignments": [8, 9, 10], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxqjgdujS", "sid": 5}, {"labels": {"alignments": [8, 9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxqjgdujS", "sid": 6}, {"labels": {"alignments": [8, 9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxqjgdujS", "sid": 7}, {"labels": {"alignments": [8, 9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxqjgdujS", "sid": 8}, {"labels": {"alignments": [8, 9, 10], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryxqjgdujS", "sid": 9}, {"labels": {"alignments": [11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryxqjgdujS", "sid": 10}, {"labels": {"alignments": [11], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "ryxqjgdujS", "sid": 11}, {"labels": {"alignments": [11], "responsetype": "reject-request_scope_No", "coarseresponse": "dispute"}, "text_id": "ryxqjgdujS", "sid": 12}, {"labels": {"alignments": [11], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryxqjgdujS", "sid": 13}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxqjgdujS", "sid": 14}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryxqjgdujS", "sid": 15}], "metadata": {"anno": "anno2", "review": "r1gk4ck25B", "rebuttal": "ryxqjgdujS", "conference": "ICLR2020", "title": "Learning from Partially-Observed Multimodal Data with Variational Autoencoders", "reviewer": "AnonReviewer5", "forum_id": "rylT0AVtwH", "rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area."}}