{"review": [{"text_id": "SklHkeMohX", "sid": 0, "sentence": "The paper proposes doubly sparse, which is a sparse mixture of sparse experts and learns a two-level class hierarchy, for efficient softmax inference."}, {"text_id": "SklHkeMohX", "sid": 1, "sentence": "[+] It reduces computational cost compared to full softmax."}, {"text_id": "SklHkeMohX", "sid": 2, "sentence": "[+] Ablation study is done for group lasso, expert lasso and load balancing, which help understand the effect of different components of the proposed"}, {"text_id": "SklHkeMohX", "sid": 3, "sentence": "[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this."}, {"text_id": "SklHkeMohX", "sid": 4, "sentence": "Besides, in evaluation, the paper only compares Doubly Sparse with full softmax."}, {"text_id": "SklHkeMohX", "sid": 5, "sentence": "Why not compare with Sparsely-Gated MoE?"}, {"text_id": "SklHkeMohX", "sid": 6, "sentence": "Overall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE."}], "reviewlabels": [{"text_id": "SklHkeMohX", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklHkeMohX", "sid": 1, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklHkeMohX", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklHkeMohX", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklHkeMohX", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklHkeMohX", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklHkeMohX", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "HJgvmMlBTQ", "sid": 0, "sentence": "Dear reviewer:"}, {"text_id": "HJgvmMlBTQ", "sid": 1, "sentence": "We appreciate your comments but it appears that there is some misunderstanding regarding our contribution in this work."}, {"text_id": "HJgvmMlBTQ", "sid": 2, "sentence": "Our work is for softmax inference speedup"}, {"text_id": "HJgvmMlBTQ", "sid": 3, "sentence": "while Sparse-Gated MoE (MoE) was not designed to do so."}, {"text_id": "HJgvmMlBTQ", "sid": 4, "sentence": "It was designed to increase the model expressiveness."}, {"text_id": "HJgvmMlBTQ", "sid": 5, "sentence": "It cannot achieve speedup because each expert still contains full softmax space as we mentioned in the background section (page 2 line 21st) and method section (page 2 last 4th line)."}, {"text_id": "HJgvmMlBTQ", "sid": 6, "sentence": "And since it is slower than the standard softmax by definition, we chose not to compare with it in the paper."}, {"text_id": "HJgvmMlBTQ", "sid": 7, "sentence": "Our algorithm addresses speed up in softmax inference."}, {"text_id": "HJgvmMlBTQ", "sid": 8, "sentence": "This is fundamentally different from Sparse-gated MoE. We divide the output space into multiple overlapped subsets."}, {"text_id": "HJgvmMlBTQ", "sid": 9, "sentence": "To find top-k predictions, we only search a few subsets."}, {"text_id": "HJgvmMlBTQ", "sid": 10, "sentence": "While in full softmax or MoE, the complexity is linear with output dimension."}, {"text_id": "HJgvmMlBTQ", "sid": 11, "sentence": "Therefore, we did not include a comparison with Sparsely-Gated MoE in our article and only compare with full softmax."}, {"text_id": "HJgvmMlBTQ", "sid": 12, "sentence": "Just for additional reference, we tested Sparsely-Gated MoE with different experts in PTB dataset; we compared the results to DS-Softmax."}, {"text_id": "HJgvmMlBTQ", "sid": 13, "sentence": "As expected, the Sparsely-Gated MoE does not achieve speedup in terms of softmax inference."}, {"text_id": "HJgvmMlBTQ", "sid": 14, "sentence": "_____________________________________________"}, {"text_id": "HJgvmMlBTQ", "sid": 15, "sentence": "_"}, {"text_id": "HJgvmMlBTQ", "sid": 16, "sentence": "Method | Top 1 | Top 5 |Top 10| FLOPs|"}, {"text_id": "HJgvmMlBTQ", "sid": 17, "sentence": "DS-8       | 0.257 | 0.448 | 0.530 | 2.84x |"}, {"text_id": "HJgvmMlBTQ", "sid": 18, "sentence": "MoE-8    | 0.258 | 0.448 | 0.530 |  1x"}, {"text_id": "HJgvmMlBTQ", "sid": 19, "sentence": "|"}, {"text_id": "HJgvmMlBTQ", "sid": 20, "sentence": "DS-16     | 0.258 | 0.450 | 0.529 | 5.13x |"}, {"text_id": "HJgvmMlBTQ", "sid": 21, "sentence": "MoE-16  | 0.258 | 0.449 | 0.530 | 1x"}, {"text_id": "HJgvmMlBTQ", "sid": 22, "sentence": "|"}, {"text_id": "HJgvmMlBTQ", "sid": 23, "sentence": "DS-32     | 0.259 | 0.449 | 0.529 | 9.43x |"}, {"text_id": "HJgvmMlBTQ", "sid": 24, "sentence": "MoE-32  | 0.259 | 0.450 | 0.531 | 1x"}, {"text_id": "HJgvmMlBTQ", "sid": 25, "sentence": "|"}, {"text_id": "HJgvmMlBTQ", "sid": 26, "sentence": "DS-64     | 0.258 | 0.450 | 0.529 |15.99x|"}, {"text_id": "HJgvmMlBTQ", "sid": 27, "sentence": "MoE-64  | 0.260 | 0.451 | 0.531 | 1x"}, {"text_id": "HJgvmMlBTQ", "sid": 28, "sentence": "|"}, {"text_id": "HJgvmMlBTQ", "sid": 29, "sentence": "_____________________________________________"}, {"text_id": "HJgvmMlBTQ", "sid": 30, "sentence": "_"}, {"text_id": "HJgvmMlBTQ", "sid": 31, "sentence": "* FLOPs means FLOPs reduction (i.e. baseline's FLOPs / target method's FLOPs)."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 0}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 1}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 2}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 3}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 4}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 5}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 6}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 7}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 8}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 9}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 10}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 11}, {"labels": {"alignments": [3, 4, 5], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "HJgvmMlBTQ", "sid": 12}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 13}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 14}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 15}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 16}, {"labels": {"alignments": [3, 4, 5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJgvmMlBTQ", "sid": 17}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 18}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 19}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 20}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 21}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 22}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 23}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 24}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 25}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 26}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 27}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 28}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 29}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 30}, {"labels": {"alignments": [], "responsetype": "", "coarseresponse": "nonarg"}, "text_id": "HJgvmMlBTQ", "sid": 31}], "metadata": {"anno": "anno10", "review": "SklHkeMohX", "rebuttal": "HJgvmMlBTQ", "conference": "ICLR2019", "title": "Doubly Sparse: Sparse Mixture of Sparse Experts for Efficient Softmax Inference", "reviewer": "AnonReviewer3", "forum_id": "rJl2E3AcF7", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}