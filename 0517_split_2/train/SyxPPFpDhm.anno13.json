{"review": [{"text_id": "SyxPPFpDhm", "sid": 0, "sentence": "This paper presents a new way to represent a dense matrix in a compact format."}, {"text_id": "SyxPPFpDhm", "sid": 1, "sentence": "First, the method prunes a dense matrix based on the Viterbi-based pruning."}, {"text_id": "SyxPPFpDhm", "sid": 2, "sentence": "Then, the pruned matrix is quantized with alternating multi-bit quantization."}, {"text_id": "SyxPPFpDhm", "sid": 3, "sentence": "Finally, the binary vectors produced by the quantization algorithm are further compressed with the Viterbi-based algorithm."}, {"text_id": "SyxPPFpDhm", "sid": 4, "sentence": "It spots the problem of each existing approach and solve the problems by combining each method."}, {"text_id": "SyxPPFpDhm", "sid": 5, "sentence": "The combination is new and the result is encouraging."}, {"text_id": "SyxPPFpDhm", "sid": 6, "sentence": "I find this paper is interesting and I like the strong results."}, {"text_id": "SyxPPFpDhm", "sid": 7, "sentence": "It is an interesting combination of methods."}, {"text_id": "SyxPPFpDhm", "sid": 8, "sentence": "However, the experiments are not enough to show that the proposed method is really needed to achieve the results. If these are answered well, I'd be happy to change my evaluation."}, {"text_id": "SyxPPFpDhm", "sid": 9, "sentence": "1. The method should be compared with other combinations of components."}, {"text_id": "SyxPPFpDhm", "sid": 10, "sentence": "At least, it should be compared with \"Multi-bit quantization only (Xu et al., 2018)\" and \"Multi-bit-quantization + Viterbi-based binary code encoding\"."}, {"text_id": "SyxPPFpDhm", "sid": 11, "sentence": "2. The experiments with \"Don't Care\" should go to the experiment section, and the end-to-end results should be present but not the ratio of incorrect bits."}, {"text_id": "SyxPPFpDhm", "sid": 12, "sentence": "3. Similarly, the paper will become stronger if it has some experimental results that compare quantization methods."}, {"text_id": "SyxPPFpDhm", "sid": 13, "sentence": "In Section 3.3."}, {"text_id": "SyxPPFpDhm", "sid": 14, "sentence": ", it mentions that the conventional k-bit quantization was tried and significant accuracy drops were observed."}, {"text_id": "SyxPPFpDhm", "sid": 15, "sentence": "I feel that this is a kind of things which support the proposed method if it is properly assessed."}, {"text_id": "SyxPPFpDhm", "sid": 16, "sentence": "4. When you say \"slow\" form something and propose a method to address it, I'd like to see some benchmark numbers. There is an experiment with simulation, but that does not seem to simulate the slow \"sequential sparse matrix decoding process\"."}, {"text_id": "SyxPPFpDhm", "sid": 17, "sentence": "Minor comments:"}, {"text_id": "SyxPPFpDhm", "sid": 18, "sentence": "* It was a bit hard to understand how a matrix is processed through the flowchart in Fig. 1 at first glance."}, {"text_id": "SyxPPFpDhm", "sid": 19, "sentence": "It would help readers to understand it better if it has a corresponding figure which shows how a matrix is processed through the flowchart."}], "reviewlabels": [{"text_id": "SyxPPFpDhm", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 13, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "SyxPPFpDhm", "sid": 14, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "SyxPPFpDhm", "sid": 15, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 16, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 17, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 18, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SyxPPFpDhm", "sid": 19, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "SJxecAdFRX", "sid": 0, "sentence": "Thank you very much for the constructive comments."}, {"text_id": "SJxecAdFRX", "sid": 1, "sentence": "We tried to strengthen our claims by adding more experimental data which the Reviewer requested."}, {"text_id": "SJxecAdFRX", "sid": 2, "sentence": "1. The proposed \"Multi-bit-quantization + Viterbi-based binary code encoding\" requires slightly larger memory footprint than \"Multi-bit quantization only ([4])\" because some of the Viterbi encoded bits have different indices from their corresponding quantization bits."}, {"text_id": "SJxecAdFRX", "sid": 3, "sentence": "Hence, the \"Multi-bit quantization only\" requires 10 % to 20 % smaller memory footprint than \"Multi-bit-quantization + Viterbi-based binary code encoding\" case."}, {"text_id": "SJxecAdFRX", "sid": 4, "sentence": "However, the main reason why we apply the Viterbi weight encoding is that parallel sparse-to-dense matrix conversion can be done by applying same Viterbi encoding process to the non-zero values and indices of the non-zero values in parallel."}, {"text_id": "SJxecAdFRX", "sid": 5, "sentence": "This parallel sparse-to-dense conversion makes the speed of feeding parameters to PEs 10 % to 40 % faster compared to [1] (Figure 6c)."}, {"text_id": "SJxecAdFRX", "sid": 6, "sentence": "2. Per Reviewer\u2019s suggestion, the experimental results for the effectiveness of \"Don\u2019t Care\" term have been moved to Section 4.1."}, {"text_id": "SJxecAdFRX", "sid": 7, "sentence": "3. Per Reviewer's suggestion, we measured accuracy differences before and after Viterbi encoding for several quantization methods such as linear quantization ([2]), logarithmic quantization ([3]), and alternating quantization ([4]) methods with the same quantization bits (3-bit)."}, {"text_id": "SJxecAdFRX", "sid": 8, "sentence": "The result shows that combination with alternating quantization and Viterbi weight encoding had only 2 % validation accuracy degradation after the Viterbi encoding was applied first right after the quantization and the accuracy was easily recovered with retraining."}, {"text_id": "SJxecAdFRX", "sid": 9, "sentence": "On the other hand, the combination with the other quantization methods and Viterbi weight encoding showed accuracy degradation as much as 71 %, which was too large to recover the accuracy with retraining."}, {"text_id": "SJxecAdFRX", "sid": 10, "sentence": "The accuracy difference mainly results from the uneven weight distribution."}, {"text_id": "SJxecAdFRX", "sid": 11, "sentence": "Because weights of neural networks usually are normally distributed, the composition ratio of '0' and '1' is not equal when the linear or logarithmic quantization is applied to the weights of neural networks."}, {"text_id": "SJxecAdFRX", "sid": 12, "sentence": "As we stated in the manuscript, Viterbi encoder tends to produce similar number of '0' and '1'."}, {"text_id": "SJxecAdFRX", "sid": 13, "sentence": "Therefore, we can conclude that under the same bit condition, alternating quantization method shows best accuracy and compatibility with our bit-by-bit Viterbi encoding scheme regardless of the type of neural networks."}, {"text_id": "SJxecAdFRX", "sid": 14, "sentence": "4. We conducted additional simulations to compare sparse matrix reconstruction speed of [1] and the proposed method."}, {"text_id": "SJxecAdFRX", "sid": 15, "sentence": "We used a random 512-by-512 size matrix with various pruning rate ranging from 75 % to 95 %."}, {"text_id": "SJxecAdFRX", "sid": 16, "sentence": "We conducted the simulations under the assumptions described in Figure 6c."}, {"text_id": "SJxecAdFRX", "sid": 17, "sentence": "The simulation results are shown in Figure 6c in updated manuscript."}, {"text_id": "SJxecAdFRX", "sid": 18, "sentence": "We could observe that the proposed method could feed 10 % to 40 % more nonzero weights and input activations to PEs in same 10000 cycles compared to [1]."}, {"text_id": "SJxecAdFRX", "sid": 19, "sentence": "Proposed method could also feed parameters to PEs 20 % to 106 % faster compared to baseline method, which reads dense weight and activation matrices directly from DRAM."}, {"text_id": "SJxecAdFRX", "sid": 20, "sentence": "The improvement in the proposed scheme mainly comes from the parallelized process of assigning non-zero values to their corresponding indices in the weight matrix."}, {"text_id": "SJxecAdFRX", "sid": 21, "sentence": "While preparing addition data for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method."}, {"text_id": "SJxecAdFRX", "sid": 22, "sentence": "After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript."}, {"text_id": "SJxecAdFRX", "sid": 23, "sentence": "Therefore, we updated Figure 7 in original manuscript to Figure 6c in updated manuscript according to the new data."}, {"text_id": "SJxecAdFRX", "sid": 24, "sentence": "5. We added the change of the exact weight representation at each process in Figure 1 to clarify the flowchart."}, {"text_id": "SJxecAdFRX", "sid": 25, "sentence": "Reference"}, {"text_id": "SJxecAdFRX", "sid": 26, "sentence": "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018."}, {"text_id": "SJxecAdFRX", "sid": 27, "sentence": "[2] Darryl D. Lin, Sachin S. Talathi, and V. Sreekanth Annapureddy. Fixed point quantization of deep convolutional networks."}, {"text_id": "SJxecAdFRX", "sid": 28, "sentence": "In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pp. 2849\u20132858. 2016."}, {"text_id": "SJxecAdFRX", "sid": 29, "sentence": "[3] Daisuke Miyashita, Edward H. Lee, and Boris Murmann. Convolutional Neural Networks using Logarithmic Data Representation. CoRR, abs/1603.01025, 2016. URL https://arxiv.org/abs/1603.01025."}, {"text_id": "SJxecAdFRX", "sid": 30, "sentence": "[4] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SJxecAdFRX", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SJxecAdFRX", "sid": 1}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 2}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 3}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 4}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 5}, {"labels": {"alignments": [11], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 6}, {"labels": {"alignments": [12, 13, 14, 15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 7}, {"labels": {"alignments": [12, 13, 14, 15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 8}, {"labels": {"alignments": [12, 13, 14, 15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 9}, {"labels": {"alignments": [12, 13, 14, 15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 10}, {"labels": {"alignments": [12, 13, 14, 15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 11}, {"labels": {"alignments": [12, 13, 14, 15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 12}, {"labels": {"alignments": [12, 13, 14, 15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 13}, {"labels": {"alignments": [16], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 14}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 15}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 16}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 17}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 18}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 19}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 20}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 21}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 22}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 23}, {"labels": {"alignments": [18, 19], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SJxecAdFRX", "sid": 24}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SJxecAdFRX", "sid": 25}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "SJxecAdFRX", "sid": 26}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "SJxecAdFRX", "sid": 27}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "SJxecAdFRX", "sid": 28}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "SJxecAdFRX", "sid": 29}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "SJxecAdFRX", "sid": 30}], "metadata": {"anno": "anno13", "review": "SyxPPFpDhm", "rebuttal": "SJxecAdFRX", "conference": "ICLR2019", "title": "Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network", "reviewer": "AnonReviewer2", "forum_id": "HkfYOoCcYX", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}