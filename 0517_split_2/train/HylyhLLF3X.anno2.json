{"review": [{"text_id": "HylyhLLF3X", "sid": 0, "sentence": "In this paper, the authors presented a large experimental study of curiosity-driven reinforcement learning on various tasks."}, {"text_id": "HylyhLLF3X", "sid": 1, "sentence": "In the experimental studies, the authors also compared several feature space embedding methods, including identical mapping (pixels), random embedding, variational autoencoders and inverse dynamics features."}, {"text_id": "HylyhLLF3X", "sid": 2, "sentence": "The authors found that in many of the tasks, learning based on intrinsic rewards could generate good performance on extrinsic rewards, when the intrinsic rewards and extrinsic rewards are correlated."}, {"text_id": "HylyhLLF3X", "sid": 3, "sentence": "The authors also found that random features embedding, somewhat surprisingly, performs well in the tasks."}, {"text_id": "HylyhLLF3X", "sid": 4, "sentence": "Overall, the paper is well written with clarity."}, {"text_id": "HylyhLLF3X", "sid": 5, "sentence": "Experimental setup is easy to understand."}, {"text_id": "HylyhLLF3X", "sid": 6, "sentence": "The authors provided code, which could help other researchers reproduce their result."}, {"text_id": "HylyhLLF3X", "sid": 7, "sentence": "Weaknesses:"}, {"text_id": "HylyhLLF3X", "sid": 8, "sentence": "1) as an experimental study, it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards."}, {"text_id": "HylyhLLF3X", "sid": 9, "sentence": "The author is correct that in many tasks, well-behaved extrinsic rewards are hard to find."}, {"text_id": "HylyhLLF3X", "sid": 10, "sentence": "But for problems with well-defined extrinsic rewards, such a comparison could help readers understand the relative performance of curiosity-based learning and/or how much headroom there exists to improve the current methods."}, {"text_id": "HylyhLLF3X", "sid": 11, "sentence": "2) it is surprising that random features perform so well in the experiments."}, {"text_id": "HylyhLLF3X", "sid": 12, "sentence": "The authors did provide literature in classification that had similar findings, but it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning."}], "reviewlabels": [{"text_id": "HylyhLLF3X", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 9, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HylyhLLF3X", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "r1xr8BYm0m", "sid": 0, "sentence": "We thank you for the constructive feedback and discuss some of your comments below."}, {"text_id": "r1xr8BYm0m", "sid": 1, "sentence": "R3: \"it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards\""}, {"text_id": "r1xr8BYm0m", "sid": 2, "sentence": "=> We would like to highlight that evaluating success of pure curiosity-driven exploration (no extrinsic rewards for training) by measuring the extrinsic score of game is just a proxy to evaluate exploration."}, {"text_id": "r1xr8BYm0m", "sid": 3, "sentence": "Our results show that exploration via curiosity has striking correlation with game scores."}, {"text_id": "r1xr8BYm0m", "sid": 4, "sentence": "But we expect that when environments have a well-defined (and well-shaped!) extrinsic reward, a policy trained using that extrinsic reward should outperform the policy trained with only curiosity especially when the performance is measured by the extrinsic return."}, {"text_id": "r1xr8BYm0m", "sid": 5, "sentence": "There are, however, examples, such as the Bowling Atari game, where a policy trained with only curiosity does *better* than a policy trained with extrinsic rewards."}, {"text_id": "r1xr8BYm0m", "sid": 6, "sentence": "The purely curious agent learns to play the game better than agents trained to maximize the (clipped) extrinsic reward directly."}, {"text_id": "r1xr8BYm0m", "sid": 7, "sentence": "We think this is because the agent gets attracted to the difficult-to-predict flashing of the scoreboard occurring after the strikes."}, {"text_id": "r1xr8BYm0m", "sid": 8, "sentence": "We expect such examples to come from environments with misleading or poorly-shaped extrinsic rewards."}, {"text_id": "r1xr8BYm0m", "sid": 9, "sentence": "R3: \"...it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning.\""}, {"text_id": "r1xr8BYm0m", "sid": 10, "sentence": "=> In the paper, Section 2.1, we discuss that random features have advantages that they are they are stable, compact, and tend to include most relevant information about the observation."}, {"text_id": "r1xr8BYm0m", "sid": 11, "sentence": "However, in our opinion, a more interesting question is not why random features perform so well, but rather why the feature learning methods perform so poorly (relative to this baseline)."}, {"text_id": "r1xr8BYm0m", "sid": 12, "sentence": "Learning the features introduces non-stationarity that confounds the effects of learning the dynamics."}, {"text_id": "r1xr8BYm0m", "sid": 13, "sentence": "We believe that if methods are developed to address this non-stationarity, or environments that are more visually complex are used, then the benefits of the learning the features will become more noticeable."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "r1xr8BYm0m", "sid": 0}, {"labels": {"alignments": [8], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "r1xr8BYm0m", "sid": 1}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1xr8BYm0m", "sid": 2}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1xr8BYm0m", "sid": 3}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1xr8BYm0m", "sid": 4}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1xr8BYm0m", "sid": 5}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1xr8BYm0m", "sid": 6}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1xr8BYm0m", "sid": 7}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1xr8BYm0m", "sid": 8}, {"labels": {"alignments": [12], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "r1xr8BYm0m", "sid": 9}, {"labels": {"alignments": [12], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1xr8BYm0m", "sid": 10}, {"labels": {"alignments": [12], "responsetype": "reject-request_scope_No", "coarseresponse": "dispute"}, "text_id": "r1xr8BYm0m", "sid": 11}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1xr8BYm0m", "sid": 12}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1xr8BYm0m", "sid": 13}], "metadata": {"anno": "anno2", "review": "HylyhLLF3X", "rebuttal": "r1xr8BYm0m", "conference": "ICLR2019", "title": "Large-Scale Study of Curiosity-Driven Learning", "reviewer": "AnonReviewer3", "forum_id": "rJNwDjAqYX", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}