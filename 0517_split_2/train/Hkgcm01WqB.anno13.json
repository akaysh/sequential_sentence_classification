{"review": [{"text_id": "Hkgcm01WqB", "sid": 0, "sentence": "The paper paper proposes a mutual information maximization objective for discovering unsupervised robotic manipulation skills."}, {"text_id": "Hkgcm01WqB", "sid": 1, "sentence": "The paper assumes that the state space can be divided into two parts - the state of the robot (\u201ccontext states\u201d) which is controllable via actions and the state of an object (\u201cstates of interest\u201d) which must be manipulated by the robot."}, {"text_id": "Hkgcm01WqB", "sid": 2, "sentence": "Given these two categories of states, the proposed algorithm maximizes a lower bound on the mutual information between the two categories of states such that a policy is learnt that is able to manipulate the object with the robot meaningfully."}, {"text_id": "Hkgcm01WqB", "sid": 3, "sentence": "I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an \u201cobject\u201d or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks."}, {"text_id": "Hkgcm01WqB", "sid": 4, "sentence": "My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others."}, {"text_id": "Hkgcm01WqB", "sid": 5, "sentence": "The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption."}, {"text_id": "Hkgcm01WqB", "sid": 6, "sentence": "It doesn\u2019t seem like a surprising discovery that maximizing the mutual information between the robot state and object state will lead to skills that actually make the robot move the object."}, {"text_id": "Hkgcm01WqB", "sid": 7, "sentence": "Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper."}, {"text_id": "Hkgcm01WqB", "sid": 8, "sentence": "Can the authors elaborate on why this choice should intuitively be better than the proposed method alone?"}, {"text_id": "Hkgcm01WqB", "sid": 9, "sentence": "The paper does not talk about how these skills can be used as primitive actions by a higher level controller (in a hierarchical RL setup), which would help in demonstrating the usefulness of these skills - e.g.: are these skills sequentially composable such that they can solve a complex task?"}], "reviewlabels": [{"text_id": "Hkgcm01WqB", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hkgcm01WqB", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hkgcm01WqB", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hkgcm01WqB", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hkgcm01WqB", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hkgcm01WqB", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hkgcm01WqB", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hkgcm01WqB", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hkgcm01WqB", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hkgcm01WqB", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "HJeVtBG5sB", "sid": 0, "sentence": "Thank you for the comments!"}, {"text_id": "HJeVtBG5sB", "sid": 1, "sentence": "To reviewer\u2019s concerns:"}, {"text_id": "HJeVtBG5sB", "sid": 2, "sentence": "- First of all, the state of interest does not have to be the object state."}, {"text_id": "HJeVtBG5sB", "sid": 3, "sentence": "It can be the state of the robot, for example, the state of actuators."}, {"text_id": "HJeVtBG5sB", "sid": 4, "sentence": "Maximizing the mutual information between two sets of actuator states can help the agent to learn to control itself."}, {"text_id": "HJeVtBG5sB", "sid": 5, "sentence": "We did a new experiment in navigation environments, where train the agent to maximize the mutual information between its left wheel states and its right wheel states."}, {"text_id": "HJeVtBG5sB", "sid": 6, "sentence": "The agent learns to run in a straight line instead of in random directions."}, {"text_id": "HJeVtBG5sB", "sid": 7, "sentence": "The video showing experiment results is available at https://youtu.be/l5KaYJWWu70?t=134"}, {"text_id": "HJeVtBG5sB", "sid": 8, "sentence": "- Although we evaluated our method in robotic manipulation tasks, it does not mean it won\u2019t work for other tasks."}, {"text_id": "HJeVtBG5sB", "sid": 9, "sentence": "We added additional experiments in a new navigation task, see the video at https://youtu.be/l5KaYJWWu70?t=104"}, {"text_id": "HJeVtBG5sB", "sid": 10, "sentence": "We consider our algorithm as a general-purpose skill learning algorithm in the sense that it guides the agent to learn any skills to control the states of interests."}, {"text_id": "HJeVtBG5sB", "sid": 11, "sentence": "The states of interest could be any states, such as the robot states, the object states, or the states of the environment."}, {"text_id": "HJeVtBG5sB", "sid": 12, "sentence": "- The state of interest is specified by the user with little domain knowledge."}, {"text_id": "HJeVtBG5sB", "sid": 13, "sentence": "However, when there is no clear divide from the user, the agent can learn from different combinations of the states of interest and the context states."}, {"text_id": "HJeVtBG5sB", "sid": 14, "sentence": "In the end, the user can choose skills from the learned skill sets that are useful for the task at hand."}, {"text_id": "HJeVtBG5sB", "sid": 15, "sentence": "- The combination of our method and DIAYN enables DIAYN to learn manipulation skills efficiently, while DIAYN alone did not learn."}, {"text_id": "HJeVtBG5sB", "sid": 16, "sentence": "Furthermore, compared to MISC, the combined method enjoys the benefits brought by DIAYN, such as learning combinable motion primitive with skill-conditioned policy for hierarchical reinforcement learning [1]."}, {"text_id": "HJeVtBG5sB", "sid": 17, "sentence": "Reference:"}, {"text_id": "HJeVtBG5sB", "sid": 18, "sentence": "[1] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HJeVtBG5sB", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJeVtBG5sB", "sid": 1}, {"labels": {"alignments": [1, 2], "responsetype": "contradict-assertion", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 2}, {"labels": {"alignments": [1, 2], "responsetype": "contradict-assertion", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 3}, {"labels": {"alignments": [1, 2], "responsetype": "contradict-assertion", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 4}, {"labels": {"alignments": [1, 2], "responsetype": "contradict-assertion", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 5}, {"labels": {"alignments": [1, 2], "responsetype": "contradict-assertion", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 6}, {"labels": {"alignments": [1, 2], "responsetype": "contradict-assertion", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 7}, {"labels": {"alignments": [4, 5, 6], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 8}, {"labels": {"alignments": [4, 5, 6], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 9}, {"labels": {"alignments": [4, 5, 6], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 10}, {"labels": {"alignments": [4, 5, 6], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 11}, {"labels": {"alignments": [5], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 12}, {"labels": {"alignments": [5], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 13}, {"labels": {"alignments": [5], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "HJeVtBG5sB", "sid": 14}, {"labels": {"alignments": [7, 8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJeVtBG5sB", "sid": 15}, {"labels": {"alignments": [7, 8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJeVtBG5sB", "sid": 16}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJeVtBG5sB", "sid": 17}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJeVtBG5sB", "sid": 18}], "metadata": {"anno": "anno13", "review": "Hkgcm01WqB", "rebuttal": "HJeVtBG5sB", "conference": "ICLR2020", "title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "reviewer": "AnonReviewer2", "forum_id": "HygSq3VFvH", "rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area."}}