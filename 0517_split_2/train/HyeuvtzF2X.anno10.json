{"review": [{"text_id": "HyeuvtzF2X", "sid": 0, "sentence": "This paper proposed to use dropout to randomly choose only a subset of neural network as a potential way to perform exploration."}, {"text_id": "HyeuvtzF2X", "sid": 1, "sentence": "The dropout happens at the beginning of each episode, and thus leads to a temporally consistent exploration."}, {"text_id": "HyeuvtzF2X", "sid": 2, "sentence": "The paper shows that with small amount of Gaussian multiplicative dropout, the algorithm can achieve the state-of-the-art results on benchmark environments."}, {"text_id": "HyeuvtzF2X", "sid": 3, "sentence": "And it can significantly outperform vanilla PPO for environments with sparse rewards."}, {"text_id": "HyeuvtzF2X", "sid": 4, "sentence": "The paper is clearly written."}, {"text_id": "HyeuvtzF2X", "sid": 5, "sentence": "The introduced technique is interesting."}, {"text_id": "HyeuvtzF2X", "sid": 6, "sentence": "I wonder except for the difference of memory consumption, how different it is compared to parameter space exploration."}, {"text_id": "HyeuvtzF2X", "sid": 7, "sentence": "I feel that it is a straightforward extension/generalization of the parameter space exploration. But the stochastic alignment and policy space constraint seem novel and important."}, {"text_id": "HyeuvtzF2X", "sid": 8, "sentence": "The motivation of this paper is mostly about learning with sparse reward."}, {"text_id": "HyeuvtzF2X", "sid": 9, "sentence": "I am curious whether the paper has other good side effects."}, {"text_id": "HyeuvtzF2X", "sid": 10, "sentence": "For example, will the dropout cause the policy to be more robust?"}, {"text_id": "HyeuvtzF2X", "sid": 11, "sentence": "Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores."}, {"text_id": "HyeuvtzF2X", "sid": 12, "sentence": "In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well."}, {"text_id": "HyeuvtzF2X", "sid": 13, "sentence": "Overall, I like this paper. It is well written."}, {"text_id": "HyeuvtzF2X", "sid": 14, "sentence": "The method seems technically sound and achieves good results."}, {"text_id": "HyeuvtzF2X", "sid": 15, "sentence": "For this reason, I would recommend accepting this paper."}], "reviewlabels": [{"text_id": "HyeuvtzF2X", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 6, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 8, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Other", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeuvtzF2X", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Other", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "S1g0_Au4pm", "sid": 0, "sentence": "Glad to know that you like our paper!"}, {"text_id": "S1g0_Au4pm", "sid": 1, "sentence": "1) Difference from parameter noise except for memory consumption:"}, {"text_id": "S1g0_Au4pm", "sid": 2, "sentence": "As stated in Section 3.3, we believe NADPEx is a generalization of parameter noise, with not only flexible memory consumption but also lower variance in gradients."}, {"text_id": "S1g0_Au4pm", "sid": 3, "sentence": "This theory is examined in Section 4.2, where NADPEx shows faster convergence and lower variance in performance with different random seeds."}, {"text_id": "S1g0_Au4pm", "sid": 4, "sentence": "Besides, comparing with [1], our work provides a theoretical modeling for the idea \"a hierarchy of stochasticity for exploration\"."}, {"text_id": "S1g0_Au4pm", "sid": 5, "sentence": "We model the NADPEx policy as a joint distribution of dropout random variables and actions, such that it could be combined seamlessly with existing on-policy policy gradient methods."}, {"text_id": "S1g0_Au4pm", "sid": 6, "sentence": "One example is the policy space constraint stated in Section 3.2."}, {"text_id": "S1g0_Au4pm", "sid": 7, "sentence": "We also provide another distribution i.e. Bernoulli distribution for stochasticity at high level, for which we derive gradient alignment and policy space constraint, as well as empirical results."}, {"text_id": "S1g0_Au4pm", "sid": 8, "sentence": "As a minor point, in [1], the stochasticity at the high level i.e. the variance of parameter noise, is adjusted in a heuristic manner."}, {"text_id": "S1g0_Au4pm", "sid": 9, "sentence": "NADPEx, in contrast, aligns the stochasticity throughout the hierarchy with end-to-end gradient update."}, {"text_id": "S1g0_Au4pm", "sid": 10, "sentence": "2) Other good side effects:"}, {"text_id": "S1g0_Au4pm", "sid": 11, "sentence": "The robustness of the NADPEx policy is orthogonal to our current work, but will be an interesting direction for the future."}, {"text_id": "S1g0_Au4pm", "sid": 12, "sentence": "Currently we only have some preliminary results."}, {"text_id": "S1g0_Au4pm", "sid": 13, "sentence": "For example, it is more robust to adversarial neural attacks."}, {"text_id": "S1g0_Au4pm", "sid": 14, "sentence": "In the future we will investigate how robust NADPEx policies could be when the environment is perturbed, e.g. agents are dragged slightly by humans as in [2, 3]."}, {"text_id": "S1g0_Au4pm", "sid": 15, "sentence": "That temporally consistent exploration is fairly important for physical robots is one of our motivations for this whole project."}, {"text_id": "S1g0_Au4pm", "sid": 16, "sentence": "In the next step we will look for simulator environments with more authentic actuators to see how NADPEx could help solve that."}, {"text_id": "S1g0_Au4pm", "sid": 17, "sentence": "Our ultimate goal is to find a safer and more efficient way for on-policy exploration on physical robots."}, {"text_id": "S1g0_Au4pm", "sid": 18, "sentence": "We believe the application of NADPEx to off-policy exploration is straightforward."}, {"text_id": "S1g0_Au4pm", "sid": 19, "sentence": "However, as stated in Section 1, off-policy methods benefit from stronger flexibility for experience sampler."}, {"text_id": "S1g0_Au4pm", "sid": 20, "sentence": "This makes the gradient alignment and policy space constraint not as important as in the on-policy methods."}, {"text_id": "S1g0_Au4pm", "sid": 21, "sentence": "As off-policy methods have the potential to be much more data-efficient, we will compare in the future how NADPEx performs comparing with auto-correlated noise in [4] and separate sampler in [5]."}, {"text_id": "S1g0_Au4pm", "sid": 22, "sentence": "[1] Plappert et al., \"Parameter Space Noise for Exploration\", ICLR 2018."}, {"text_id": "S1g0_Au4pm", "sid": 23, "sentence": "[2] Tassa et al., \"Synthesis and stabilization of complex behaviors through online trajectory optimization\", IROS 2012."}, {"text_id": "S1g0_Au4pm", "sid": 24, "sentence": "[3] Clavera et al., \"Learning to Adapt: Meta-Learning for Model-based Control\", arXiv 2018."}, {"text_id": "S1g0_Au4pm", "sid": 25, "sentence": "[4] Lillicrap et al., \"Continuous control with deep reinforcement learning\", ICLR 2016."}, {"text_id": "S1g0_Au4pm", "sid": 26, "sentence": "[5] Xu et al., \"Learning to explore via meta-policy gradient\", ICML 2018."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 0}, {"labels": {"alignments": [6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 1}, {"labels": {"alignments": [6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 2}, {"labels": {"alignments": [6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 3}, {"labels": {"alignments": [6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 4}, {"labels": {"alignments": [6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 5}, {"labels": {"alignments": [6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 6}, {"labels": {"alignments": [6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 7}, {"labels": {"alignments": [6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 8}, {"labels": {"alignments": [6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 9}, {"labels": {"alignments": [9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 10}, {"labels": {"alignments": [9], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "S1g0_Au4pm", "sid": 11}, {"labels": {"alignments": [9], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 12}, {"labels": {"alignments": [9], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 13}, {"labels": {"alignments": [9], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 14}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1g0_Au4pm", "sid": 15}, {"labels": {"alignments": [11], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "S1g0_Au4pm", "sid": 16}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1g0_Au4pm", "sid": 17}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1g0_Au4pm", "sid": 18}, {"labels": {"alignments": [12], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 19}, {"labels": {"alignments": [12], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 20}, {"labels": {"alignments": [12], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "S1g0_Au4pm", "sid": 21}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 22}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 23}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 24}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 25}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "S1g0_Au4pm", "sid": 26}], "metadata": {"anno": "anno10", "review": "HyeuvtzF2X", "rebuttal": "S1g0_Au4pm", "conference": "ICLR2019", "title": "NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning", "reviewer": "AnonReviewer1", "forum_id": "rkxciiC9tm", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}