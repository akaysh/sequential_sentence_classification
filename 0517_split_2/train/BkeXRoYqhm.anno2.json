{"review": [{"text_id": "BkeXRoYqhm", "sid": 0, "sentence": "The paper proposed a hierarchical framework for problem embedding and intended to apply it to adaptive tutoring."}, {"text_id": "BkeXRoYqhm", "sid": 1, "sentence": "The system first used a rule-based method to extract the concepts for problems and then learned the concept embeddings and used them for problem representation."}, {"text_id": "BkeXRoYqhm", "sid": 2, "sentence": "In addition, the paper further proposed negative pre-training for training with imbalanced data sets to decrease false negatives and positives."}, {"text_id": "BkeXRoYqhm", "sid": 3, "sentence": "The methods are compared with some other word-embedding based methods and showed 100% accuracy in a similarity detection test on a very small dataset."}, {"text_id": "BkeXRoYqhm", "sid": 4, "sentence": "In sum, the paper has a very good application but not good enough as a research paper."}, {"text_id": "BkeXRoYqhm", "sid": 5, "sentence": "Some of the problems are listed as follows:"}, {"text_id": "BkeXRoYqhm", "sid": 6, "sentence": "1.\tLack of technical novelty."}, {"text_id": "BkeXRoYqhm", "sid": 7, "sentence": "It seems to me just a combination of several mature techniques."}, {"text_id": "BkeXRoYqhm", "sid": 8, "sentence": "I do not see much insight into the problem."}, {"text_id": "BkeXRoYqhm", "sid": 9, "sentence": "For example, if the rule-based concept extractor can already extract concepts very well, the \u201cproblem retrieval\u201d should be solved by searching with the concepts as queries."}, {"text_id": "BkeXRoYqhm", "sid": 10, "sentence": "Why should we use embedding to compare the similarity?"}, {"text_id": "BkeXRoYqhm", "sid": 11, "sentence": "Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap."}, {"text_id": "BkeXRoYqhm", "sid": 12, "sentence": "2.\tData size is too small, and the baselines"}, {"text_id": "BkeXRoYqhm", "sid": 13, "sentence": "are not state-of-the-art. There are some unsupervised sentence embedding methods other than the word-embedding based models."}, {"text_id": "BkeXRoYqhm", "sid": 14, "sentence": "Some clarity issues."}, {"text_id": "BkeXRoYqhm", "sid": 15, "sentence": "For example, Page 6. \u201cis pre-trained on a pure set of negative samples\u201d\u2014 what is the objective function? How to train on only negative samples?"}], "reviewlabels": [{"text_id": "BkeXRoYqhm", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "BkeXRoYqhm", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "BkeXRoYqhm", "sid": 14, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkeXRoYqhm", "sid": 15, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "S1etFWjga7", "sid": 0, "sentence": "1- There are two reasons that concept and problem embedding are performed in this work."}, {"text_id": "S1etFWjga7", "sid": 1, "sentence": "Considering concept continuity is an important matter in education."}, {"text_id": "S1etFWjga7", "sid": 2, "sentence": "Having concept embedding, concept continuity can be reached as is discussed in the last paragraph on page 7 and some other examples are given in table 2."}, {"text_id": "S1etFWjga7", "sid": 3, "sentence": "By just having the most sophisticated concept extractor, the concept continuity cannot be retrieved."}, {"text_id": "S1etFWjga7", "sid": 4, "sentence": "Furthermore, problem embedding is used by the recommender system to project the performance of students on the problems they solved onto other problems that they have not solved."}, {"text_id": "S1etFWjga7", "sid": 5, "sentence": "This way, we have an idea of what problems should be recommended to them and which problems should not by having an evaluation of their ability to solve unseen problems and recommend problems in the boundary of their capacity, not way beyond, and to recommend problems in a way that covers all concepts necessary for students to learn."}, {"text_id": "S1etFWjga7", "sid": 6, "sentence": "We have observed interesting patterns, e.g. similar problems are more likely to be solved correctly at the same time or wrong at the same time."}, {"text_id": "S1etFWjga7", "sid": 7, "sentence": "Note that by just having the concepts of problems that are not in numerical form, performance projection may not be feasible and there is a need for using other methods like embedding."}, {"text_id": "S1etFWjga7", "sid": 8, "sentence": "2- The data size being small"}, {"text_id": "S1etFWjga7", "sid": 9, "sentence": "is just the nature of the application."}, {"text_id": "S1etFWjga7", "sid": 10, "sentence": "Creating new problems is a creative process and is not easy, given that with the insight we have on the application, the data size seems to suffice."}, {"text_id": "S1etFWjga7", "sid": 11, "sentence": "Furthermore, since Prob2Vec is performing well for not a relatively big data set, it would definitely do well for big data sets since the more data we have, the more precise the concept and problem embedding are."}, {"text_id": "S1etFWjga7", "sid": 12, "sentence": "The easy-tough-to-beat method proposed by Arora et al. is the state of the art in unsupervised sentence embedding that we compared our algorithm with."}, {"text_id": "S1etFWjga7", "sid": 13, "sentence": "Please let us know if we missed anything."}, {"text_id": "S1etFWjga7", "sid": 14, "sentence": "Pre-training is a common practice in transfer learning (one-shot learning)."}, {"text_id": "S1etFWjga7", "sid": 15, "sentence": "The objective function does not differ from the objective function used for post training."}, {"text_id": "S1etFWjga7", "sid": 16, "sentence": "Training on only negative samples with lower training epochs than the training epochs in post training just adjusts the weights of the neural network to a better starting point."}, {"text_id": "S1etFWjga7", "sid": 17, "sentence": "If the training epochs in pre-training is relatively smaller than the training epochs in post training, due to curse of dimensionality, the warm start for post training results in better performance for NN classifier."}, {"text_id": "S1etFWjga7", "sid": 18, "sentence": "To make it more clear what it means to train the neural network on a pure set of negative data samples, think about batch training."}, {"text_id": "S1etFWjga7", "sid": 19, "sentence": "It's not likely, but possible, that a batch only has negative or positive samples."}, {"text_id": "S1etFWjga7", "sid": 20, "sentence": "In the pre-training phase of our method, we intentionally used a pure set of negative samples (with fewer training epochs) to have a warm start for post training."}, {"text_id": "S1etFWjga7", "sid": 21, "sentence": "As table 3 shows, our proposed method outperforms one-shot learning."}, {"text_id": "S1etFWjga7", "sid": 22, "sentence": "Please look at part 1 of our response to reviewer2 and part 2 of comment titled \"Response to Question on Negative Pre-Training\" below."}], "rebuttallabels": [{"labels": {"alignments": [6, 7, 8, 9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 0}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 1}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 2}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 3}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 4}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 5}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 6}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 7}, {"labels": {"alignments": [12, 13], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 8}, {"labels": {"alignments": [12, 13], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 9}, {"labels": {"alignments": [12, 13], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 10}, {"labels": {"alignments": [12, 13], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 11}, {"labels": {"alignments": [12, 13], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1etFWjga7", "sid": 12}, {"labels": {"alignments": [12, 13], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "S1etFWjga7", "sid": 13}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1etFWjga7", "sid": 14}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1etFWjga7", "sid": 15}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1etFWjga7", "sid": 16}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1etFWjga7", "sid": 17}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1etFWjga7", "sid": 18}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1etFWjga7", "sid": 19}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1etFWjga7", "sid": 20}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1etFWjga7", "sid": 21}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "S1etFWjga7", "sid": 22}], "metadata": {"anno": "anno2", "review": "BkeXRoYqhm", "rebuttal": "S1etFWjga7", "conference": "ICLR2019", "title": "Prob2Vec: Mathematical Semantic Embedding for Problem Retrieval in Adaptive Tutoring", "reviewer": "AnonReviewer3", "forum_id": "SJl8gnAqtX", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}