{"review": [{"text_id": "SJgzxEO5hQ", "sid": 0, "sentence": "adaptive versions of sgd are commonly used in machine learning."}, {"text_id": "SJgzxEO5hQ", "sid": 1, "sentence": "adagrad, adadelta are both popular adaptive variations of sgd."}, {"text_id": "SJgzxEO5hQ", "sid": 2, "sentence": "These algorithms can be seen as preconditioned versions of gradient descent where the preconditioner applied is a matrix of second-order moments of the gradients."}, {"text_id": "SJgzxEO5hQ", "sid": 3, "sentence": "However, because this matrix turns out to be a pxp matrix where p is the number of parameters in the model, maintaining and performing linear algebra with this pxp matrix is computationally intensive."}, {"text_id": "SJgzxEO5hQ", "sid": 4, "sentence": "In this paper, the authors show how to maintain and update this pxp matrix by storing only smaller matrices of size pxr and rxr, and performing 1. an SVD of a small matrix of size rxr"}, {"text_id": "SJgzxEO5hQ", "sid": 5, "sentence": "2. matrix-vector multiplication between a pxr matrix and rx1 vector."}, {"text_id": "SJgzxEO5hQ", "sid": 6, "sentence": "Given that rxr is a small constant sized matrix and that matrix-vector multiplication can be efficiently computed on GPUs, this matrix adapted SGD can be made scalable."}, {"text_id": "SJgzxEO5hQ", "sid": 7, "sentence": "The authors also discuss how to adapt the proposed algorithm with Adam style updates that incorporate momentum."}, {"text_id": "SJgzxEO5hQ", "sid": 8, "sentence": "Experiments are shown on various architectures (CNN, RNN) and comparisons are made against SGD, ADAM."}, {"text_id": "SJgzxEO5hQ", "sid": 9, "sentence": "General comments: THe appendix has some good discussion and it would be great if some of that discussion was moved to the main paper."}, {"text_id": "SJgzxEO5hQ", "sid": 10, "sentence": "Pros:  Shows how to make full matrix preconditioning efficient, via the use of clever linear algebra, and GPU computations."}, {"text_id": "SJgzxEO5hQ", "sid": 11, "sentence": "Shows improvements on LSTM tasks, and is comparable with SGD, matching accuracy with time."}, {"text_id": "SJgzxEO5hQ", "sid": 12, "sentence": "Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver."}, {"text_id": "SJgzxEO5hQ", "sid": 13, "sentence": "This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good."}, {"text_id": "SJgzxEO5hQ", "sid": 14, "sentence": "It might be possible that if one performs few steps of GGT optimizer in the initial stages and then switches to SGD/ADAM in the later stages, then some of the computational concerns that arise are eliminated."}, {"text_id": "SJgzxEO5hQ", "sid": 15, "sentence": "Have the authors tried out such techniques?"}], "reviewlabels": [{"text_id": "SJgzxEO5hQ", "sid": 0, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 1, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 2, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 3, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 14, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgzxEO5hQ", "sid": 15, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "S1xoNIkzAQ", "sid": 0, "sentence": "Thanks for the review."}, {"text_id": "S1xoNIkzAQ", "sid": 1, "sentence": "@Update overhead: We argue that per-iteration performance is a worthwhile objective in itself, which is less significant in some scenarios (e.g. costly function evaluation, like in RL, or expensive backprops, like in RNNs)."}, {"text_id": "S1xoNIkzAQ", "sid": 2, "sentence": "That said, we were indeed not able to demonstrate end-to-end gains in vision."}, {"text_id": "S1xoNIkzAQ", "sid": 3, "sentence": "Please note that in the NLP benchmark our algorithm finds a better solution and wins in wall-clock time."}, {"text_id": "S1xoNIkzAQ", "sid": 4, "sentence": "@Switching: This is a good suggestion, and we indeed do cite one of the papers attempting to approach optimizer-switching in a principled way."}, {"text_id": "S1xoNIkzAQ", "sid": 5, "sentence": "We found that we could squeeze out some wall-clock gains by applying the expensive update more sparingly, but the value of including this in the paper was unclear (effectively adding a host of hyperparameters orthogonal to the central idea)."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "S1xoNIkzAQ", "sid": 0}, {"labels": {"alignments": [12, 13], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1xoNIkzAQ", "sid": 1}, {"labels": {"alignments": [12, 13], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "S1xoNIkzAQ", "sid": 2}, {"labels": {"alignments": [12, 13], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "S1xoNIkzAQ", "sid": 3}, {"labels": {"alignments": [14, 15], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "S1xoNIkzAQ", "sid": 4}, {"labels": {"alignments": [14, 15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "S1xoNIkzAQ", "sid": 5}], "metadata": {"anno": "anno2", "review": "SJgzxEO5hQ", "rebuttal": "S1xoNIkzAQ", "conference": "ICLR2019", "title": "The Case for Full-Matrix Adaptive Regularization", "reviewer": "AnonReviewer3", "forum_id": "rkxd2oR9Y7", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}