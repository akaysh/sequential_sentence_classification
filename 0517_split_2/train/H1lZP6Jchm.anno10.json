{"review": [{"text_id": "H1lZP6Jchm", "sid": 0, "sentence": "This paper proposed Compound Density Networks (CDNs), a neural network architecture that parametrises conditional distributions as infinite mixtures, thus generalising the traditional finite mixture density networks (MDNs)."}, {"text_id": "H1lZP6Jchm", "sid": 1, "sentence": "The authors realise CDNs by treating the weights of each neural network layer probabilistically, and letting them be matrix variate Gaussians (MVGs) with their parameters given as a function of the layer input via a hypernetwork."}, {"text_id": "H1lZP6Jchm", "sid": 2, "sentence": "CDNs can then be straightforwardly optimised with SGD for a particular task by using the reparametrization trick."}, {"text_id": "H1lZP6Jchm", "sid": 3, "sentence": "The authors further argue that in case that overfitting is present at CDNs, then an extra KL-divergence term can be employed such that the input dependent MVG distribution is close to a simple prior that is input agnostic."}, {"text_id": "H1lZP6Jchm", "sid": 4, "sentence": "They then proceed to evaluate the predictive uncertainty that CDNs offer on three tasks: a toy regression problem, out-of-distribution example detection on MNIST/notMNIST and adversarial example detection on MNIST and CIFAR 10."}, {"text_id": "H1lZP6Jchm", "sid": 5, "sentence": "The objective of this work is to provide a method for better uncertainty estimates from deep learning models."}, {"text_id": "H1lZP6Jchm", "sid": 6, "sentence": "This is an important research area and relevant for ICLR."}, {"text_id": "H1lZP6Jchm", "sid": 7, "sentence": "The paper is generally well written with a clear presentation of the proposed model."}, {"text_id": "H1lZP6Jchm", "sid": 8, "sentence": "The generalisation from the finite MDN to the continuous CDN seems straightforward, the model is relatively easy to implement and it is evaluated extensively against several modern baselines."}, {"text_id": "H1lZP6Jchm", "sid": 9, "sentence": "Nevertheless, I believe that it still has to address some points in order to be better suited for publication:"}, {"text_id": "H1lZP6Jchm", "sid": 10, "sentence": "- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1)."}, {"text_id": "H1lZP6Jchm", "sid": 11, "sentence": "Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?"}, {"text_id": "H1lZP6Jchm", "sid": 12, "sentence": "- How many samples did you use from p(theta|x) during training?"}, {"text_id": "H1lZP6Jchm", "sid": 13, "sentence": "It seems that with a single sample the method becomes an instance of VIB [1], only considering the weights of the network as latent variables rather than the hidden units."}, {"text_id": "H1lZP6Jchm", "sid": 14, "sentence": "- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture."}, {"text_id": "H1lZP6Jchm", "sid": 15, "sentence": "- Have you checked / visualised what type of weight distributions do CDNs capture? It would be interesting to see if e.g. the marginal (across the dataset) weight distribution at each layer has any multimodality as that could hint that the network learns to properly specialise to individual data points."}, {"text_id": "H1lZP6Jchm", "sid": 16, "sentence": "- The authors mention that in order to avoid overfitting they add an extra (weighted) KL-divergence term to the log-likelihood of the dataset, that encourages the weight distributions for specific points to be close to simple priors."}, {"text_id": "H1lZP6Jchm", "sid": 17, "sentence": "How influential is that extra term to the uncertainty quality that you obtain in the end?"}, {"text_id": "H1lZP6Jchm", "sid": 18, "sentence": "How does this term affect the learned distributions in case of CDNs?"}, {"text_id": "H1lZP6Jchm", "sid": 19, "sentence": "Furthermore, the way that CDNs are constructed seems to be more appropriate at capturing input specific uncertainty (i.e. aleatoric) rather than global uncertainty about the data (i.e. epistemic)."}, {"text_id": "H1lZP6Jchm", "sid": 20, "sentence": "I believe that for the specific uncertainty evaluation tasks this paper considers the latter is more appropriate."}, {"text_id": "H1lZP6Jchm", "sid": 21, "sentence": "More discussion on both of these aspects can help in improving this paper."}, {"text_id": "H1lZP6Jchm", "sid": 22, "sentence": "-"}, {"text_id": "H1lZP6Jchm", "sid": 23, "sentence": "As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground."}, {"text_id": "H1lZP6Jchm", "sid": 24, "sentence": "For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks."}, {"text_id": "H1lZP6Jchm", "sid": 25, "sentence": "For KFLA a hyper parameter \u201ctau\u201d was tuned; this hyperparameter instead corresponds to the precision of the Gaussian prior on the parameters."}, {"text_id": "H1lZP6Jchm", "sid": 26, "sentence": "In this case, KFLA always optimises a \u201ccorrect\u201d Bayesian model for every value of the hyperparameter whereas MNF and noisy K-FAC do not."}, {"text_id": "H1lZP6Jchm", "sid": 27, "sentence": "Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior."}, {"text_id": "H1lZP6Jchm", "sid": 28, "sentence": "[1] Deep Variational Information Bottleneck"}], "reviewlabels": [{"text_id": "H1lZP6Jchm", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Other", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 14, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 15, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 16, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 17, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 18, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 19, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 20, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 21, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 22, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 23, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 24, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 25, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 26, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 27, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lZP6Jchm", "sid": 28, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rJxslWKrCX", "sid": 0, "sentence": "We thank the reviewer for the valuable feedback!"}, {"text_id": "rJxslWKrCX", "sid": 1, "sentence": "The suggestion comments were very helpful and led to a clear improvement of our manuscript."}, {"text_id": "rJxslWKrCX", "sid": 2, "sentence": "We reply to the answers and comments in the order they were raised:"}, {"text_id": "rJxslWKrCX", "sid": 3, "sentence": "(1) While indeed we need more samples of weight matrices than e.g. for applying VI for BNNs for due to the input dependency, we do not believe this makes our method unscalable to real world scenarios."}, {"text_id": "rJxslWKrCX", "sid": 4, "sentence": "Note, that input dependent samples are also needed in the variational training of VAEs (where the number of hidden variables is of course much smaller than the number of weight parameters in our setting)."}, {"text_id": "rJxslWKrCX", "sid": 5, "sentence": "While we present the training algorithm naively in an online version for clearness in Algorithm 1, in practice mini-batching can be done efficiently, due to the availability of batched linear algebra operations, at least in the framework we use (PyTorch), e.g. torch.bmm, broadcasting semantics, etc."}, {"text_id": "rJxslWKrCX", "sid": 6, "sentence": "For convolution layers, we can simply use a different type of mixing distribution, e.g. a fully-factorized multivariate normal instead of matrix-variate normal."}, {"text_id": "rJxslWKrCX", "sid": 7, "sentence": "(2) Thank you very much for the pointer to VIB!"}, {"text_id": "rJxslWKrCX", "sid": 8, "sentence": "We have added a section in the updated manuscript to compare the objective of CDNs with that used in VIB and VI for Bayesian neural networks (see new Section 4)."}, {"text_id": "rJxslWKrCX", "sid": 9, "sentence": "Furthermore, while we  always used 1 sample during training in the original submission (which indeed makes the CDN an instance of VIB) we now added experiments using 10 samples (see Section 6.4) in an experimental analysis of the different objectives."}, {"text_id": "rJxslWKrCX", "sid": 10, "sentence": "The results show that the CDN objective produces superior results compared to VI and VIB."}, {"text_id": "rJxslWKrCX", "sid": 11, "sentence": "(3) Of course! We have moved the test accuracy (which previously was only given in the Appendix and thus hard to find) to the legends of the plots to make it more easily accessible."}, {"text_id": "rJxslWKrCX", "sid": 12, "sentence": "CDNs give better uncertainty estimates while still having similar predictive power compared to the baselines."}, {"text_id": "rJxslWKrCX", "sid": 13, "sentence": "(4) Thank you for the great suggestion."}, {"text_id": "rJxslWKrCX", "sid": 14, "sentence": "We performed the following 2 experiments for the revised version: First, we picked a weight of a CDN trained on a toy regression experiment (with heteroscedastic  noise) at random and visualized its conditional distributions given different values of x. We found that the means and variances vary for different x.  Furthermore, we picked a weight of a CDN trained on a toy classification dataset (created by sampling x ~ 1/2*N(-3, 1) + 1/2*N(3, 1), and assign y=0 if x comes from the first Gaussian and y=1, otherwise) at random and visualized its marginal distributions."}, {"text_id": "rJxslWKrCX", "sid": 15, "sentence": "We found that CDNs indeed capable of learning multimodal weight distribution and to learn input specific mixing distributions.. We detail this in Appendix G."}, {"text_id": "rJxslWKrCX", "sid": 16, "sentence": "(5) We found that the regularization term has a significant impact on the quality of the prediction and the uncertainty estimate (we found that the uncertainty estimates are worse with small \\lambda)."}, {"text_id": "rJxslWKrCX", "sid": 17, "sentence": "It makes sure that the variance of \\theta is not shrinking too much, i.e. encouraging the mixing distribution to be close to the prior implies it should have similar variance to the prior (which was chosen to be large)."}, {"text_id": "rJxslWKrCX", "sid": 18, "sentence": "Naturally, the coefficient \\lambda controls this behavior: as \\lambda increases the validation accuracy is decreasing while the uncertainty is increasing (and vice versa)."}, {"text_id": "rJxslWKrCX", "sid": 19, "sentence": "This gives rise to the selection heuristic for \\lambda we applied: pick the highest \\lambda that still gives high accuracy on the validation set (e.g. > 0.97 in MNIST)."}, {"text_id": "rJxslWKrCX", "sid": 20, "sentence": "We found that this works very well in the experiments we did (on OOD and adversarial examples)."}, {"text_id": "rJxslWKrCX", "sid": 21, "sentence": "Furthermore, indeed CDNs are rather designed to capture the (heteroscedastic) aleatoric uncertainty."}, {"text_id": "rJxslWKrCX", "sid": 22, "sentence": "We have revised the toy experiments to better account for that."}, {"text_id": "rJxslWKrCX", "sid": 23, "sentence": "However, curiously, CDNs also work well in tasks that are usually shown as prime examples of epistemic uncertainty, e.g. OOD classification and adversarial attack."}, {"text_id": "rJxslWKrCX", "sid": 24, "sentence": "(6) Thank you for this feedback."}, {"text_id": "rJxslWKrCX", "sid": 25, "sentence": "You are right! We have revised the baseline experiments with Bayesian models so that they either use \\lambda = 1 or the settings that the original authors recommended, i.e. we only tune \\tau in KFLA and set \\tau = 0.01 in noisy-KFAC as these are the settings suggested in their respective publications."}, {"text_id": "rJxslWKrCX", "sid": 26, "sentence": "Note, that the conclusions keep unchanged."}, {"text_id": "rJxslWKrCX", "sid": 27, "sentence": "References:"}, {"text_id": "rJxslWKrCX", "sid": 28, "sentence": "[1] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016."}, {"text_id": "rJxslWKrCX", "sid": 29, "sentence": "[2] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Variational dropout and the local reparameterization trick.\" Advances in Neural Information Processing Systems. 2015"}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rJxslWKrCX", "sid": 0}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rJxslWKrCX", "sid": 1}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJxslWKrCX", "sid": 2}, {"labels": {"alignments": [10], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rJxslWKrCX", "sid": 3}, {"labels": {"alignments": [10], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rJxslWKrCX", "sid": 4}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 5}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 6}, {"labels": {"alignments": [12, 13], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 7}, {"labels": {"alignments": [12, 13], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 8}, {"labels": {"alignments": [12, 13], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 9}, {"labels": {"alignments": [12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 10}, {"labels": {"alignments": [14], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 11}, {"labels": {"alignments": [14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 12}, {"labels": {"alignments": [15], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 13}, {"labels": {"alignments": [15], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 14}, {"labels": {"alignments": [15], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 15}, {"labels": {"alignments": [16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 16}, {"labels": {"alignments": [16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 17}, {"labels": {"alignments": [16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 18}, {"labels": {"alignments": [16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 19}, {"labels": {"alignments": [16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 20}, {"labels": {"alignments": [16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 21}, {"labels": {"alignments": [16, 17, 18, 19, 20, 21], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 22}, {"labels": {"alignments": [16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 23}, {"labels": {"alignments": [23, 24, 25, 26, 27], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 24}, {"labels": {"alignments": [23, 24, 25, 26, 27], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 25}, {"labels": {"alignments": [23, 24, 25, 26, 27], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJxslWKrCX", "sid": 26}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJxslWKrCX", "sid": 27}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "rJxslWKrCX", "sid": 28}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "rJxslWKrCX", "sid": 29}], "metadata": {"anno": "anno10", "review": "H1lZP6Jchm", "rebuttal": "rJxslWKrCX", "conference": "ICLR2019", "title": "Compound Density Networks", "reviewer": "AnonReviewer1", "forum_id": "rkgv9oRqtQ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}