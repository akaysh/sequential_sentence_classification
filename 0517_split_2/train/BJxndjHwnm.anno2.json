{"review": [{"text_id": "BJxndjHwnm", "sid": 0, "sentence": "This paper provides a new dynamic perspective on deep neural network."}, {"text_id": "BJxndjHwnm", "sid": 1, "sentence": "Based on Gaussian weights and biases, the paper investigates the evolution of the covariance matrix along with the layers."}, {"text_id": "BJxndjHwnm", "sid": 2, "sentence": "Eventually the matrices achieve a stationary point, i.e., fixed point of the dynamic system."}, {"text_id": "BJxndjHwnm", "sid": 3, "sentence": "Local performance around the fixed point is explored."}, {"text_id": "BJxndjHwnm", "sid": 4, "sentence": "Extensions are provided to include the batch normalization."}, {"text_id": "BJxndjHwnm", "sid": 5, "sentence": "I believe this paper may stimulate some interesting ideas for other researchers."}, {"text_id": "BJxndjHwnm", "sid": 6, "sentence": "Two technical questions:"}, {"text_id": "BJxndjHwnm", "sid": 7, "sentence": "1. When the layers tends to infinity, the covariance matrix reaches stationary (fixed) point."}, {"text_id": "BJxndjHwnm", "sid": 8, "sentence": "How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?"}, {"text_id": "BJxndjHwnm", "sid": 9, "sentence": "This somewhat conflicts the commonsense of \"the deeper the better?\""}, {"text_id": "BJxndjHwnm", "sid": 10, "sentence": "2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold."}], "reviewlabels": [{"text_id": "BJxndjHwnm", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 7, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxndjHwnm", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "SkxSjPODaQ", "sid": 0, "sentence": "Thank you for the review and careful reading of our paper! We\u2019re glad that you found it of interest. On revision we will fix the typos that you identified."}, {"text_id": "SkxSjPODaQ", "sid": 1, "sentence": "Regarding the first point, your intuition is exactly correct and a slightly simpler discussion of this phenomenon can be found in [1]."}, {"text_id": "SkxSjPODaQ", "sid": 2, "sentence": "When the network is deep enough that the covariance matrix has reached its fixed point, the distribution of the outputs of the network will be independent of the inputs."}, {"text_id": "SkxSjPODaQ", "sid": 3, "sentence": "At this point the network becomes untrainable."}, {"text_id": "SkxSjPODaQ", "sid": 4, "sentence": "To reconcile this with the commonsense intuition that \u201cdeeper is better\u201d, our answer is twofold."}, {"text_id": "SkxSjPODaQ", "sid": 5, "sentence": "1) As in [1] and [2] it is often possible to find configurations or architectural modifications where the covariance matrix doesn\u2019t approach its fixed point over depths often considered in machine learning."}, {"text_id": "SkxSjPODaQ", "sid": 6, "sentence": "When this is the case one can safely increase the depth without sacrificing accuracy."}, {"text_id": "SkxSjPODaQ", "sid": 7, "sentence": "2) It seems that the role of depth in performance is more subtle than standard intuition would dictate."}, {"text_id": "SkxSjPODaQ", "sid": 8, "sentence": "For example, in [3] note that although the authors were able to train a 10k hidden layer network, they did not observe any improvement in accuracy."}, {"text_id": "SkxSjPODaQ", "sid": 9, "sentence": "In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical."}, {"text_id": "SkxSjPODaQ", "sid": 10, "sentence": "[1] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein."}, {"text_id": "SkxSjPODaQ", "sid": 11, "sentence": "Deep Information Propagation (https://arxiv.org/abs/1611.01232)"}, {"text_id": "SkxSjPODaQ", "sid": 12, "sentence": "[2] G. Yang and S. S. Schoenholz. Mean Field Residual Networks (https://arxiv.org/abs/1712.08969)"}, {"text_id": "SkxSjPODaQ", "sid": 13, "sentence": "[3] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks (https://arxiv.org/abs/1806.05393)"}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SkxSjPODaQ", "sid": 0}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxSjPODaQ", "sid": 1}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxSjPODaQ", "sid": 2}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxSjPODaQ", "sid": 3}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxSjPODaQ", "sid": 4}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxSjPODaQ", "sid": 5}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxSjPODaQ", "sid": 6}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxSjPODaQ", "sid": 7}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxSjPODaQ", "sid": 8}, {"labels": {"alignments": [7, 8, 9], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "SkxSjPODaQ", "sid": 9}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "SkxSjPODaQ", "sid": 10}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "SkxSjPODaQ", "sid": 11}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "SkxSjPODaQ", "sid": 12}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "SkxSjPODaQ", "sid": 13}], "metadata": {"anno": "anno2", "review": "BJxndjHwnm", "rebuttal": "SkxSjPODaQ", "conference": "ICLR2019", "title": "A Mean Field Theory of Batch Normalization", "reviewer": "AnonReviewer3", "forum_id": "SyMDXnCcF7", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}