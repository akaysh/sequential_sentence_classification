{"review": [{"text_id": "r1labrqphm", "sid": 0, "sentence": "Summary:"}, {"text_id": "r1labrqphm", "sid": 1, "sentence": "This paper introduces Amortized Proximal Optimization (APO) that optimizes a proximal objective at each optimization step."}, {"text_id": "r1labrqphm", "sid": 2, "sentence": "The optimization hyperparameters are optimized to best minimize the proximal objective."}, {"text_id": "r1labrqphm", "sid": 3, "sentence": "The objective is represented using a regularization style parameter lambda and a distance metric D that, depending on its definition, reduces the optimization procedure to Gauss-Newton, General Gauss Newton or Natural Gradient Descent."}, {"text_id": "r1labrqphm", "sid": 4, "sentence": "There are two key convergence results which are dependent on the meta-objective being optimized directly which, while not practical, gives some insight into the inner workings of the algorithm."}, {"text_id": "r1labrqphm", "sid": 5, "sentence": "The first result indicates strong convergence when using the Euclidean distance as the distance measure D."}, {"text_id": "r1labrqphm", "sid": 6, "sentence": "The second result shows strong convergence when D is set as the Bregman divergence."}, {"text_id": "r1labrqphm", "sid": 7, "sentence": "The algorithm optimizes the base optimizer on a number of domains and shows state-of-the-art results over a grid search of the hyperparameters on the same optimizer."}, {"text_id": "r1labrqphm", "sid": 8, "sentence": "Clarity and Quality: The paper is well written."}, {"text_id": "r1labrqphm", "sid": 9, "sentence": "Originality: It appears to be a novel application of meta-learning."}, {"text_id": "r1labrqphm", "sid": 10, "sentence": "I wonder why the authors didn\u2019t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well."}, {"text_id": "r1labrqphm", "sid": 11, "sentence": "Also how does this compare to adaptive hyperparameter training techniques such as population based training?"}, {"text_id": "r1labrqphm", "sid": 12, "sentence": "Significance:"}, {"text_id": "r1labrqphm", "sid": 13, "sentence": "Overall it appears to be a novel and interesting contribution."}, {"text_id": "r1labrqphm", "sid": 14, "sentence": "I am concerned though why the authors didn\u2019t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques."}, {"text_id": "r1labrqphm", "sid": 15, "sentence": "Also, your convergence results appear to rely on strong convexity of the loss."}, {"text_id": "r1labrqphm", "sid": 16, "sentence": "How is this a reasonable assumption?"}, {"text_id": "r1labrqphm", "sid": 17, "sentence": "These are my major concerns."}, {"text_id": "r1labrqphm", "sid": 18, "sentence": "Question: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?"}], "reviewlabels": [{"text_id": "r1labrqphm", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 5, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 6, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 12, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 15, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 16, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 17, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1labrqphm", "sid": 18, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rkxxb72KCX", "sid": 0, "sentence": "Thank you for your insightful comments. We have incorporated your suggestions into the revised version of the paper."}, {"text_id": "rkxxb72KCX", "sid": 1, "sentence": "Q: Relationship to optimizers with adaptive learning rates, and comparison between Adam and Adam-APO."}, {"text_id": "rkxxb72KCX", "sid": 2, "sentence": "While Adam and Adagrad are often described as having \u201cadaptive learning rates,\u201d they still have a global learning rate that is just as critical to tune as for SGD."}, {"text_id": "rkxxb72KCX", "sid": 3, "sentence": "In our experiments, we consider tuning the learning rate for RMSprop, which also maintains adaptive learning rates for each parameter, and is closely related to Adam/Adagrad."}, {"text_id": "rkxxb72KCX", "sid": 4, "sentence": "Adam is essentially RMSprop with momentum; APO can be applied to Adam by applying momentum on top of the updates computed by APO."}, {"text_id": "rkxxb72KCX", "sid": 5, "sentence": "To address your question about Adam, we added experiments for tuning the global learning rate of Adam with APO in appendix Section G, Figure 14, where Adam-APO achieves better performance than Adam with a fixed global learning rate, and achieves comparable performance as Adam with a manual schedule."}, {"text_id": "rkxxb72KCX", "sid": 6, "sentence": "Q: Comparison with population-based training (PBT)"}, {"text_id": "rkxxb72KCX", "sid": 7, "sentence": "We have added a comparison between APO and PBT in appendix Section H, Figure 15."}, {"text_id": "rkxxb72KCX", "sid": 8, "sentence": "For population-based training, one must carefully select many hyperparameters, including the size of the population, the perturbation strategy (e.g., randomly perturb the learning rate by multiplying it by 1.2 or 0.8), the exploration interval (e.g., the number of training iterations to run before exploiting other members of the population)."}, {"text_id": "rkxxb72KCX", "sid": 9, "sentence": "We used PBT and APO to tune the learning rate of RMSprop while training a ResNet34 model on CIFAR-10."}, {"text_id": "rkxxb72KCX", "sid": 10, "sentence": "For PBT, we used a population of size 4, and chose to exploit/explore after each epoch of training."}, {"text_id": "rkxxb72KCX", "sid": 11, "sentence": "We tried multiple exploration strategies, and found that it was critical to set the probability of resampling a learning rate from an underlying distribution to be 0; otherwise, the learning rates could jump from small to large values, and yield unstable training."}, {"text_id": "rkxxb72KCX", "sid": 12, "sentence": "In contrast, APO only requires a simple grid search over lambda, and all other hyperparameters can be kept at their default settings."}, {"text_id": "rkxxb72KCX", "sid": 13, "sentence": "We found that  APO substantially outperformed PBT, achieving a lower final training loss and equal test accuracy in much less wall-clock time; this shows the advantage of gradient-based methods for tuning learning rates, such as APO, compared to evolutionary methods based on random perturbations such as PBT."}, {"text_id": "rkxxb72KCX", "sid": 14, "sentence": "Q: The convergence results appear to rely on strong convexity of the loss."}, {"text_id": "rkxxb72KCX", "sid": 15, "sentence": "How is this a reasonable assumption?"}, {"text_id": "rkxxb72KCX", "sid": 16, "sentence": "Note that we assume strong convexity of the loss as a function of the output units, not as a function of the weights."}, {"text_id": "rkxxb72KCX", "sid": 17, "sentence": "Hence, our assumption is fairly realistic in the neural net setting."}, {"text_id": "rkxxb72KCX", "sid": 18, "sentence": "The loss function on top of the network output is usually defined as a simple convex function; for instance, in regression, a common choice of loss function is the quadratic loss (i.e, the squared distance between the network output and the true label), which is strongly convex."}, {"text_id": "rkxxb72KCX", "sid": 19, "sentence": "In fact, even without assuming that the loss function is strongly convex and that the output manifold is dense, we are still able to show a fast convergence rate."}, {"text_id": "rkxxb72KCX", "sid": 20, "sentence": "In the updated version of the paper, we show that our algorithm with an oracle converges to stationary point globally with a fast rate, which provides insight into why APO works well."}, {"text_id": "rkxxb72KCX", "sid": 21, "sentence": "Q: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?"}, {"text_id": "rkxxb72KCX", "sid": 22, "sentence": "APO is robust to the initial learning rate of the base optimizer, using the default meta learning rate suggested in our updated paper."}, {"text_id": "rkxxb72KCX", "sid": 23, "sentence": "We have added a section to the appendix in which we include RMSprop-APO experiments on Rosenbrock, MNIST, and CIFAR-10 to show that the training loss, test accuracy, and learning rate trajectories are nearly identical when starting with initial learning rates {1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7}, spanning 5 orders of magnitude."}, {"text_id": "rkxxb72KCX", "sid": 24, "sentence": "Note that 1e-2 is quite a large initial learning rate for RMSprop."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rkxxb72KCX", "sid": 0}, {"labels": {"alignments": [10, 14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rkxxb72KCX", "sid": 1}, {"labels": {"alignments": [10, 14], "responsetype": "refute-question", "coarseresponse": "dispute"}, "text_id": "rkxxb72KCX", "sid": 2}, {"labels": {"alignments": [10, 14], "responsetype": "refute-question", "coarseresponse": "dispute"}, "text_id": "rkxxb72KCX", "sid": 3}, {"labels": {"alignments": [10, 14], "responsetype": "refute-question", "coarseresponse": "dispute"}, "text_id": "rkxxb72KCX", "sid": 4}, {"labels": {"alignments": [14], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 5}, {"labels": {"alignments": [14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rkxxb72KCX", "sid": 6}, {"labels": {"alignments": [14], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 7}, {"labels": {"alignments": [14], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 8}, {"labels": {"alignments": [14], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 9}, {"labels": {"alignments": [14], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 10}, {"labels": {"alignments": [14], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 11}, {"labels": {"alignments": [14], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 12}, {"labels": {"alignments": [14], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 13}, {"labels": {"alignments": [15, 16], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rkxxb72KCX", "sid": 14}, {"labels": {"alignments": [15, 16], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rkxxb72KCX", "sid": 15}, {"labels": {"alignments": [15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 16}, {"labels": {"alignments": [15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 17}, {"labels": {"alignments": [15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 18}, {"labels": {"alignments": [15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 19}, {"labels": {"alignments": [15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 20}, {"labels": {"alignments": [18], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rkxxb72KCX", "sid": 21}, {"labels": {"alignments": [18], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 22}, {"labels": {"alignments": [18], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 23}, {"labels": {"alignments": [18], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkxxb72KCX", "sid": 24}], "metadata": {"anno": "anno14", "review": "r1labrqphm", "rebuttal": "rkxxb72KCX", "conference": "ICLR2019", "title": "Online Hyperparameter Adaptation via Amortized Proximal Optimization", "reviewer": "AnonReviewer1", "forum_id": "rJl6M2C5Y7", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}