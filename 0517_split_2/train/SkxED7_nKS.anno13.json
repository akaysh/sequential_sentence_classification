{"review": [{"text_id": "SkxED7_nKS", "sid": 0, "sentence": "I take issue with the usage of the phrase \"skill discovery\"."}, {"text_id": "SkxED7_nKS", "sid": 1, "sentence": "In prior work (e.g. VIC, DIAYN), this meant learning a skill-conditional policy."}, {"text_id": "SkxED7_nKS", "sid": 2, "sentence": "Here, there is only a single (unconditioned) policy, and the different \"skills\" come from modifications of the environment -- the number of skills is tied to the number of environments."}, {"text_id": "SkxED7_nKS", "sid": 3, "sentence": "This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work."}, {"text_id": "SkxED7_nKS", "sid": 4, "sentence": "Skill discovery in this context implies being able to have a single agent execute a variety of learned skills, rather than having one agent per environment with each environment designed to elicit a specific skill."}, {"text_id": "SkxED7_nKS", "sid": 5, "sentence": "Rather than \"skill discovery\", I suggest the authors position MISC relative to earlier work on empowerment, wherein a single policy was used to maximize mutual information of the form I(a; s_t | s_{t-1})."}, {"text_id": "SkxED7_nKS", "sid": 6, "sentence": "Modifying the objective to incorporate domain knowledge (as done in your DIAYN baseline) yields I(a; s_i | s_{t-1}) and is amenable to maximization by either of the lower bounds considered here."}, {"text_id": "SkxED7_nKS", "sid": 7, "sentence": "Indeed, your DIAYN baseline with skill length set to 1 and the number of skills equal to the number of actions (or same parameterization in the case of continuous actions) should recover this approach."}, {"text_id": "SkxED7_nKS", "sid": 8, "sentence": "I believe this would be a much more appropriate baseline, and I'd be curious to hear the intuition for why I(s_c ; s_i) should be superior."}, {"text_id": "SkxED7_nKS", "sid": 9, "sentence": "Apart from this missing baseline, the experimental results seem convincing."}, {"text_id": "SkxED7_nKS", "sid": 10, "sentence": "However, it is unclear whether or not VIME and PER were modified to incorporate domain knowledge (i.e. s_i/s_c distinction)."}, {"text_id": "SkxED7_nKS", "sid": 11, "sentence": "Indeed, an appendix would be greatly appreciated, as many experimental details were omitted."}, {"text_id": "SkxED7_nKS", "sid": 12, "sentence": "Ideally, an experimental setup with previously published results (e.g. control suite for DIAYN, Seaquest for DISCERN) would be considered, but I can understand why this wasn't done as incorporating domain knowledge is the main contribution of the paper."}, {"text_id": "SkxED7_nKS", "sid": 13, "sentence": "That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g. states of interest vs context are given, not learned)."}, {"text_id": "SkxED7_nKS", "sid": 14, "sentence": "Rebuttal EDIT:"}, {"text_id": "SkxED7_nKS", "sid": 15, "sentence": "The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking."}, {"text_id": "SkxED7_nKS", "sid": 16, "sentence": "Needing new environment variations to obtain new skills is a large step backwards from things like DIAYN (the MISC/DIAYN combination needs more evidence to be considered a possible solution), and the s_i/s_c distinction is non-trivial to specify or learn for harder problems (e.g. pixel observations)."}, {"text_id": "SkxED7_nKS", "sid": 17, "sentence": "That said, in the sort of settings under consideration (low dimensional state variables and environmental variations are simple to create) MISC does appear to be superior to prior work."}, {"text_id": "SkxED7_nKS", "sid": 18, "sentence": "The empowerment baseline is much appreciated, and while modifications of PER and VIME that incorporate prior knowledge would've also been nice, the experimental results pass the bar for acceptance in my view."}], "reviewlabels": [{"text_id": "SkxED7_nKS", "sid": 0, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 1, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 4, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 14, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 16, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 17, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkxED7_nKS", "sid": 18, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rkgrTHMcjB", "sid": 0, "sentence": "Thank you for the comments!"}, {"text_id": "rkgrTHMcjB", "sid": 1, "sentence": "To review\u2019s feedback:"}, {"text_id": "rkgrTHMcjB", "sid": 2, "sentence": "- We pay attention to the term \u201cskill discovery\u201d and made it more clear about the connection between prior works and the current work in the revised version."}, {"text_id": "rkgrTHMcjB", "sid": 3, "sentence": "Our method can also be combined with DIAYN to learn the skill-conditioned policy as mentioned in the paper."}, {"text_id": "rkgrTHMcjB", "sid": 4, "sentence": "- We added both a theoretical connection and new experimental results to compare MISC and the empowerment method in the revised version."}, {"text_id": "rkgrTHMcjB", "sid": 5, "sentence": "In the navigation tasks, we show that our method outperforms the empowerment method."}, {"text_id": "rkgrTHMcjB", "sid": 6, "sentence": "- An intuition for why I(s_c, s_i) could be superior to I(a, s_i) is that in robotic tasks, the mutual information between the robotic sates, s_c, and the object states, s_i, could be easier to be estimated than the mutual information between the action, a, and the object states, s_i, as shown in Figure 4 in the paper."}, {"text_id": "rkgrTHMcjB", "sid": 7, "sentence": "Therefore, the agent receives a higher MI reward more easily and learns to control s_i more efficiently."}, {"text_id": "rkgrTHMcjB", "sid": 8, "sentence": "The context states can be seen as the summary information of the agent\u2019s action and the transition model of the environment, which could be more relevant in terms of estimating the object states in comparison to the agent\u2019s actions."}, {"text_id": "rkgrTHMcjB", "sid": 9, "sentence": "- VIME and PER are used as described in their original papers."}, {"text_id": "rkgrTHMcjB", "sid": 10, "sentence": "- We have added an appendix to provide more information about experiment details."}, {"text_id": "rkgrTHMcjB", "sid": 11, "sentence": "- We also newly evaluated our method on gazebo-based robotic simulations, including the cases when there is no object, a single object of interest, and multiple objects of interests."}, {"text_id": "rkgrTHMcjB", "sid": 12, "sentence": "A video showing new experimental results is available at https://youtu.be/l5KaYJWWu70?t=104"}, {"text_id": "rkgrTHMcjB", "sid": 13, "sentence": "In this experiment, we also compare MISC with two additional baselines, including ICM and empowerment (with state of interest), see Figure 4 in the paper."}, {"text_id": "rkgrTHMcjB", "sid": 14, "sentence": "- We now mention that the states of interests vs context are given in the revised paper."}, {"text_id": "rkgrTHMcjB", "sid": 15, "sentence": "However, when they are not given. They can also be automatically learned/selected by iterating over all possible combinations"}, {"text_id": "rkgrTHMcjB", "sid": 16, "sentence": "."}, {"text_id": "rkgrTHMcjB", "sid": 17, "sentence": "Afterwards, an optimal combination can be chosen by the user via testing in the task at hand."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rkgrTHMcjB", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rkgrTHMcjB", "sid": 1}, {"labels": {"alignments": [0, 1, 2, 3, 4], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "rkgrTHMcjB", "sid": 2}, {"labels": {"alignments": [0, 1, 2, 3, 4], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "rkgrTHMcjB", "sid": 3}, {"labels": {"alignments": [5, 6, 7, 9], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 4}, {"labels": {"alignments": [5, 6, 7, 9], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 5}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 6}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 7}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 8}, {"labels": {"alignments": [10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 9}, {"labels": {"alignments": [11], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 10}, {"labels": {"alignments": [], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 11}, {"labels": {"alignments": [], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 12}, {"labels": {"alignments": [], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 13}, {"labels": {"alignments": [], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 14}, {"labels": {"alignments": [], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 15}, {"labels": {"alignments": [], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 16}, {"labels": {"alignments": [], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "rkgrTHMcjB", "sid": 17}], "metadata": {"anno": "anno13", "review": "SkxED7_nKS", "rebuttal": "rkgrTHMcjB", "conference": "ICLR2020", "title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "reviewer": "AnonReviewer3", "forum_id": "HygSq3VFvH", "rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years."}}