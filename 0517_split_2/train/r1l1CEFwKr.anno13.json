{"review": [{"text_id": "r1l1CEFwKr", "sid": 0, "sentence": "The paper attempts to clarify the debate on large-batch neural network training, particularly on the relationship between learning rate, batch sizes and test performance."}, {"text_id": "r1l1CEFwKr", "sid": 1, "sentence": "The authors claim two contributions towards understanding how the hyper-parameters of SGD affect final training and test performance: (1) SGD exhibits two regimes with different behaviours and (2) large-batch training leads to degradation of test performance even with same step budgets."}, {"text_id": "r1l1CEFwKr", "sid": 2, "sentence": "Overall, the authors did a comprehensive study on large-batch training with the support of extensive experiments."}, {"text_id": "r1l1CEFwKr", "sid": 3, "sentence": "But I'm concerned with the novelty and contributions of this paper."}, {"text_id": "r1l1CEFwKr", "sid": 4, "sentence": "I tend to reject this paper because (1) the first contribution of the paper is not new as it has already been recognized by a few paper that SGD exhibits two different regimes; (2) this paper makes the debate of large-batch training even muddier."}, {"text_id": "r1l1CEFwKr", "sid": 5, "sentence": "Main argument:"}, {"text_id": "r1l1CEFwKr", "sid": 6, "sentence": "The paper does not do a great job in clarify the debate."}, {"text_id": "r1l1CEFwKr", "sid": 7, "sentence": "Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper."}, {"text_id": "r1l1CEFwKr", "sid": 8, "sentence": "For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again."}, {"text_id": "r1l1CEFwKr", "sid": 9, "sentence": "Also, I find the experiments done in section 3 and 4 are similar to previous works and even the conclusions are similar."}, {"text_id": "r1l1CEFwKr", "sid": 10, "sentence": "The only new observation I'm aware of in these two sections is that the training loss and test accuracy are independent of batch size in the noise dominated regime."}, {"text_id": "r1l1CEFwKr", "sid": 11, "sentence": "Back to introduction section, the goal of this paper (as claimed in the beginning of second paragraph) is to clarify the debate."}, {"text_id": "r1l1CEFwKr", "sid": 12, "sentence": "But does this paper really achieves this goal?"}, {"text_id": "r1l1CEFwKr", "sid": 13, "sentence": "In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018)."}, {"text_id": "r1l1CEFwKr", "sid": 14, "sentence": "In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime."}, {"text_id": "r1l1CEFwKr", "sid": 15, "sentence": "I think the authors should instead focus on the discussion of generalization performance and the observation that training loss and test accuracy are independent of batch size in noise dominated regime."}, {"text_id": "r1l1CEFwKr", "sid": 16, "sentence": "To my knowledge, this part is novel and interesting."}, {"text_id": "r1l1CEFwKr", "sid": 17, "sentence": "In summary, I'm inclined to reject this paper given the current version."}, {"text_id": "r1l1CEFwKr", "sid": 18, "sentence": "However, I think the paper is still worth reading if the authors can reorganize the paper and I might increase my score if my concerns get resolved."}], "reviewlabels": [{"text_id": "r1l1CEFwKr", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 10, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 17, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1l1CEFwKr", "sid": 18, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "ryltuFZ7sr", "sid": 0, "sentence": "We thank the reviewer for their helpful comments."}, {"text_id": "ryltuFZ7sr", "sid": 1, "sentence": "Please could the reviewer clarify why they felt our work muddies the debate regarding large-batch training?"}, {"text_id": "ryltuFZ7sr", "sid": 2, "sentence": "We demonstrate that one can initially increase the batch size with no loss in test accuracy by simultaneously increasing the learning rate."}, {"text_id": "ryltuFZ7sr", "sid": 3, "sentence": "However for very large batch sizes the test accuracy degrades under both constant epoch and constant step budgets."}, {"text_id": "ryltuFZ7sr", "sid": 4, "sentence": "We agree that some of our observations under constant epoch budgets in sections 2 and 3 have been made in previous work."}, {"text_id": "ryltuFZ7sr", "sid": 5, "sentence": "However there are also several important differences:"}, {"text_id": "ryltuFZ7sr", "sid": 6, "sentence": "1."}, {"text_id": "ryltuFZ7sr", "sid": 7, "sentence": "Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs)."}, {"text_id": "ryltuFZ7sr", "sid": 8, "sentence": "As we show in sections 4 and 5, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy."}, {"text_id": "ryltuFZ7sr", "sid": 9, "sentence": "A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small."}, {"text_id": "ryltuFZ7sr", "sid": 10, "sentence": "To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B)."}, {"text_id": "ryltuFZ7sr", "sid": 11, "sentence": "2. Zhang et al. argued that Momentum only helps in the large batch limit."}, {"text_id": "ryltuFZ7sr", "sid": 12, "sentence": "However, their analysis is based on the noisy quadratic model, which cannot explain the results we observed on the test set in sections 4 and 5."}, {"text_id": "ryltuFZ7sr", "sid": 13, "sentence": "These experiments clearly demonstrate that, unlike the SDE perspective, the noisy quadratic model is not an appropriate model for predicting test set performance in deep learning."}, {"text_id": "ryltuFZ7sr", "sid": 14, "sentence": "Their work also does not clarify the assumptions under which linear scaling of the learning rate should arise."}, {"text_id": "ryltuFZ7sr", "sid": 15, "sentence": "3. Our empirical results in section 3 are similar to Shallue et al., however their work argues that there is no reliable relationship between learning rate and batch size."}, {"text_id": "ryltuFZ7sr", "sid": 16, "sentence": "We draw a very different conclusion: the learning rate usually obeys linear scaling, but linear scaling only holds theoretically when the assumptions we specify are satisfied."}, {"text_id": "ryltuFZ7sr", "sid": 17, "sentence": "Linear scaling may not hold in cases where these assumptions break down (e.g., language modelling)."}, {"text_id": "ryltuFZ7sr", "sid": 18, "sentence": "4. The observation that the test accuracy is independent of batch size in the noise dominated regime is a natural consequence of the SDE analogy, since any two training runs which integrate the same SDE should sample final parameters from the same probability distribution."}, {"text_id": "ryltuFZ7sr", "sid": 19, "sentence": "We will clarify this in the updated text."}, {"text_id": "ryltuFZ7sr", "sid": 20, "sentence": "Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works."}, {"text_id": "ryltuFZ7sr", "sid": 21, "sentence": "We apologise for this."}, {"text_id": "ryltuFZ7sr", "sid": 22, "sentence": "It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text."}, {"text_id": "ryltuFZ7sr", "sid": 23, "sentence": "Turning to our generalization experiments in sections 4 and 5."}, {"text_id": "ryltuFZ7sr", "sid": 24, "sentence": "It is true that a number of papers in recent years have claimed that SGD noise enhances generalization."}, {"text_id": "ryltuFZ7sr", "sid": 25, "sentence": "However Shallue et al. recently argued no previous work had provided convincing empirical evidence for this claim."}, {"text_id": "ryltuFZ7sr", "sid": 26, "sentence": "Indeed in their abstract, they state \u2018We find no evidence that larger batch sizes degrade out-of-sample performance\u2019."}, {"text_id": "ryltuFZ7sr", "sid": 27, "sentence": "In another recent paper, Zhang et al. argued that optimization in deep learning is well described by a noisy quadratic model which predicts that increasing the batch size should always enhance performance under constant step budgets."}, {"text_id": "ryltuFZ7sr", "sid": 28, "sentence": "Crucially, to establish that SGD noise enhances generalization, one must show that small batch sizes generalize better than large batch sizes under constant step budgets, with realistic learning rate decay schedules, and one must independently tune the learning rate at each batch size."}, {"text_id": "ryltuFZ7sr", "sid": 29, "sentence": "In section 4, we are the first authors to perform this experiment and confirm that the final test accuracy of SGD does degrade for very large batch sizes under both constant epoch and constant step budgets, contradicting the claims of both Shallue et al and Zhang et al."}, {"text_id": "ryltuFZ7sr", "sid": 30, "sentence": "Furthermore, we show in section 5 that the optimal SGD temperature which maximizes the test accuracy is almost independent of the epoch budget."}, {"text_id": "ryltuFZ7sr", "sid": 31, "sentence": "These results provide the first convincing empirical evidence that SGD noise does enhance generalization in well-tuned networks with learning rate decay schedules."}, {"text_id": "ryltuFZ7sr", "sid": 32, "sentence": "We believe this is an important contribution."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "ryltuFZ7sr", "sid": 0}, {"labels": {"alignments": [4], "responsetype": "followup", "coarseresponse": "nonarg"}, "text_id": "ryltuFZ7sr", "sid": 1}, {"labels": {"alignments": [3, 4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 2}, {"labels": {"alignments": [3, 4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 3}, {"labels": {"alignments": [6, 7, 8, 9], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "ryltuFZ7sr", "sid": 4}, {"labels": {"alignments": [6, 7, 8, 9], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 5}, {"labels": {"alignments": [6, 7, 8, 9], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 6}, {"labels": {"alignments": [6, 7, 8, 9], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 7}, {"labels": {"alignments": [6, 7, 8, 9], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 8}, {"labels": {"alignments": [6, 7, 8, 9], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 9}, {"labels": {"alignments": [6, 7, 8, 9], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 10}, {"labels": {"alignments": [14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 11}, {"labels": {"alignments": [14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 12}, {"labels": {"alignments": [14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 13}, {"labels": {"alignments": [14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 14}, {"labels": {"alignments": [13], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 15}, {"labels": {"alignments": [13], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 16}, {"labels": {"alignments": [13], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 17}, {"labels": {"alignments": [15], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 18}, {"labels": {"alignments": [15], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 19}, {"labels": {"alignments": [15], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "ryltuFZ7sr", "sid": 20}, {"labels": {"alignments": [15], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "ryltuFZ7sr", "sid": 21}, {"labels": {"alignments": [15], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "ryltuFZ7sr", "sid": 22}, {"labels": {"alignments": [15], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryltuFZ7sr", "sid": 23}, {"labels": {"alignments": [14], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "ryltuFZ7sr", "sid": 24}, {"labels": {"alignments": [13, 14], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "ryltuFZ7sr", "sid": 25}, {"labels": {"alignments": [13, 14], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 26}, {"labels": {"alignments": [13, 14], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 27}, {"labels": {"alignments": [13, 14], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 28}, {"labels": {"alignments": [13, 14], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 29}, {"labels": {"alignments": [13, 14], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 30}, {"labels": {"alignments": [13, 14], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryltuFZ7sr", "sid": 31}, {"labels": {"alignments": [13, 14], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "ryltuFZ7sr", "sid": 32}], "metadata": {"anno": "anno13", "review": "r1l1CEFwKr", "rebuttal": "ryltuFZ7sr", "conference": "ICLR2020", "title": "Hyperparameter Tuning and Implicit Regularization in Minibatch SGD", "reviewer": "AnonReviewer1", "forum_id": "ryGWhJBtDB", "rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area."}}