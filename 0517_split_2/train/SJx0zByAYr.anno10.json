{"review": [{"text_id": "SJx0zByAYr", "sid": 0, "sentence": "The main contributions of the submission are:"}, {"text_id": "SJx0zByAYr", "sid": 1, "sentence": "1. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search)."}, {"text_id": "SJx0zByAYr", "sid": 2, "sentence": "2. The introduction of a novel metric that tries to capture the \"tunability\" of an optimizer."}, {"text_id": "SJx0zByAYr", "sid": 3, "sentence": "This metric attempts to trade off the performance of an optimizer when tuned only with a small number of hyper-parameter trials, and its performance when carefully tuned."}, {"text_id": "SJx0zByAYr", "sid": 4, "sentence": "The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K."}, {"text_id": "SJx0zByAYr", "sid": 5, "sentence": "The weights of this weighted average and K are \"hyper-parameters\" of the metric itself."}, {"text_id": "SJx0zByAYr", "sid": 6, "sentence": "They use K=100 and suggest 3 possible choices of weights."}, {"text_id": "SJx0zByAYr", "sid": 7, "sentence": "The paper appears to treat 2. as the main contribution."}, {"text_id": "SJx0zByAYr", "sid": 8, "sentence": "However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters)."}, {"text_id": "SJx0zByAYr", "sid": 9, "sentence": "The reason is that simpler methods provide just as much information, and do not rely on the need of interpreting the choice of the weights and K. This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset."}, {"text_id": "SJx0zByAYr", "sid": 10, "sentence": "Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics."}, {"text_id": "SJx0zByAYr", "sid": 11, "sentence": "Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples)."}, {"text_id": "SJx0zByAYr", "sid": 12, "sentence": "A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past."}, {"text_id": "SJx0zByAYr", "sid": 13, "sentence": "Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work."}, {"text_id": "SJx0zByAYr", "sid": 14, "sentence": "Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability."}, {"text_id": "SJx0zByAYr", "sid": 15, "sentence": "They also suggest that when the tuning budget is low, using Adam but tuning only the learning rate is beneficial, which could be a valuable and practical suggestion."}, {"text_id": "SJx0zByAYr", "sid": 16, "sentence": "I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop."}, {"text_id": "SJx0zByAYr", "sid": 17, "sentence": "Other comments/notes:"}, {"text_id": "SJx0zByAYr", "sid": 18, "sentence": "* One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials."}, {"text_id": "SJx0zByAYr", "sid": 19, "sentence": "I think it would be worth discussing this more."}, {"text_id": "SJx0zByAYr", "sid": 20, "sentence": "* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)"}, {"text_id": "SJx0zByAYr", "sid": 21, "sentence": "* similarly to the above, if the configurations are always sampled from the same 100, confidence intervals in the graphs become less reliable as the budget increases."}], "reviewlabels": [{"text_id": "SJx0zByAYr", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 17, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 18, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 19, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 20, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJx0zByAYr", "sid": 21, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "B1iEvPOjB", "sid": 0, "sentence": "Thank you for your review of our work."}, {"text_id": "B1iEvPOjB", "sid": 1, "sentence": "The following are your concerns of our work:"}, {"text_id": "B1iEvPOjB", "sid": 2, "sentence": "a. Limited contribution to the qualitative understanding of the optimizers"}, {"text_id": "B1iEvPOjB", "sid": 3, "sentence": "a.i. Informativeness of the proposed w-tunability metric"}, {"text_id": "B1iEvPOjB", "sid": 4, "sentence": "b. Using wall-clock time instead of number of HPO oracle calls"}, {"text_id": "B1iEvPOjB", "sid": 5, "sentence": "We address these concerns one-by-one."}, {"text_id": "B1iEvPOjB", "sid": 6, "sentence": "a. Limited contribution to the qualitative understanding of the optimizers:"}, {"text_id": "B1iEvPOjB", "sid": 7, "sentence": "We consider the three main contributions of our work to be 1) a systematic evaluation protocol of optimizers, with off-the-shelf HPO to account for the cost of tuning of hyperparameters."}, {"text_id": "B1iEvPOjB", "sid": 8, "sentence": "This is missing in existing papers, which consider best attained performance alone."}, {"text_id": "B1iEvPOjB", "sid": 9, "sentence": "The importance of a proper hyperparameter search protocol is emphasized by Choi et al., 2019 (published after our submission and under review at ICLR)."}, {"text_id": "B1iEvPOjB", "sid": 10, "sentence": "2) a \u201cw-tunability\u201d measure of the cost of hyperparameter optimization, and 3) under the experiments considered we find that Adam (with default beta and epsilon values) is the most tunable."}, {"text_id": "B1iEvPOjB", "sid": 11, "sentence": "a.i. Informativeness of the tunability metric:"}, {"text_id": "B1iEvPOjB", "sid": 12, "sentence": "We propose w-tunability as a metric to incorporate the HPO tuning too in reporting the performance of an optimizer, and compute it as a linear combination of the incumbents of the HPO algorithm, though one can use an arbitrarily complex function trading off interpretability."}, {"text_id": "B1iEvPOjB", "sid": 13, "sentence": "It is true that Figures 4-7 essentially contain all the information needed to judge about the optimizer\u2019s tunability."}, {"text_id": "B1iEvPOjB", "sid": 14, "sentence": "However, a metric that is easy to compute, interpret and compare optimizers across tasks is crucial, for which we propose w-tunability."}, {"text_id": "B1iEvPOjB", "sid": 15, "sentence": "This is analogous to computing specific quantities like accuracy, FPR, TPR from the confusion matrix, even though a confusion matrix contains all the information (and is quite cumbersome to compare)."}, {"text_id": "B1iEvPOjB", "sid": 16, "sentence": "The summary metric in Figure 2 provides a different interpretation: It reports a normalized performance i.e., the  normalized incumbent performance at iteration $k$. This doesn\u2019t explicitly include information about the previous $k-1$ iterations (which is our central argument)."}, {"text_id": "B1iEvPOjB", "sid": 17, "sentence": "Thus our proposed tunability metric provides more information than the summary statistics plot (figure 2)."}, {"text_id": "B1iEvPOjB", "sid": 18, "sentence": "Due to this novelty, we argue that our setup does contribute to the qualitative understanding of the optimizers."}, {"text_id": "B1iEvPOjB", "sid": 19, "sentence": "In fact, it yields to a drastically different valuation of adaptive gradient methods than popular previous work (Wilson et al, Shah et al)."}, {"text_id": "B1iEvPOjB", "sid": 20, "sentence": "You mention that our work is incremental to the work on benchmarking of optimizers."}, {"text_id": "B1iEvPOjB", "sid": 21, "sentence": "Can you please provide respective references?"}, {"text_id": "B1iEvPOjB", "sid": 22, "sentence": "We have modified parts of our paper to reflect these arguments better."}, {"text_id": "B1iEvPOjB", "sid": 23, "sentence": "b. Using wall-clock time instead of HPO oracle calls:"}, {"text_id": "B1iEvPOjB", "sid": 24, "sentence": "Our reason for using a number of configuration trials instead of a time budget is that measuring number of hyperparameter configuration searches required is more relevant to understand the optimizers\u2019 dependence on the hyperparameters."}, {"text_id": "B1iEvPOjB", "sid": 25, "sentence": "However, we completely agree with you that computational budget is a relevant factor from the practitioner\u2019s point of view, and added a discussion of this in Appendix E of the paper."}, {"text_id": "B1iEvPOjB", "sid": 26, "sentence": "As you rightfully point out, the adaptive optimizers tend to converge in fewer number of epochs, amplifying the results that favor Adam over the variants of SGD."}, {"text_id": "B1iEvPOjB", "sid": 27, "sentence": "References:"}, {"text_id": "B1iEvPOjB", "sid": 28, "sentence": "Wilson, Ashia C., et al. \"The marginal value of adaptive gradient methods in machine learning.\" Advances in Neural Information Processing Systems. 2017."}, {"text_id": "B1iEvPOjB", "sid": 29, "sentence": "Shah, Vatsal, Anastasios Kyrillidis, and Sujay Sanghavi. \"Minimum norm solutions do not always generalize well for over-parameterized problems.\" arXiv preprint arXiv:1811.07055 (2018)."}, {"text_id": "B1iEvPOjB", "sid": 30, "sentence": "Choi, Dami, et al. \"On Empirical Comparisons of Optimizers for Deep Learning.\" arXiv preprint arXiv:1910.05446 (2019)."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 1}, {"labels": {"alignments": [14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 2}, {"labels": {"alignments": [9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 3}, {"labels": {"alignments": [18], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 4}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 5}, {"labels": {"alignments": [14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 6}, {"labels": {"alignments": [14], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 7}, {"labels": {"alignments": [14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "B1iEvPOjB", "sid": 8}, {"labels": {"alignments": [14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "B1iEvPOjB", "sid": 9}, {"labels": {"alignments": [14], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 10}, {"labels": {"alignments": [9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 11}, {"labels": {"alignments": [9], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 12}, {"labels": {"alignments": [10], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "B1iEvPOjB", "sid": 13}, {"labels": {"alignments": [10], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 14}, {"labels": {"alignments": [10], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 15}, {"labels": {"alignments": [9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "B1iEvPOjB", "sid": 16}, {"labels": {"alignments": [9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "B1iEvPOjB", "sid": 17}, {"labels": {"alignments": [9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "B1iEvPOjB", "sid": 18}, {"labels": {"alignments": [9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "B1iEvPOjB", "sid": 19}, {"labels": {"alignments": [12], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 20}, {"labels": {"alignments": [12], "responsetype": "followup", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 21}, {"labels": {"alignments": [9, 10, 11], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "B1iEvPOjB", "sid": 22}, {"labels": {"alignments": [18], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 23}, {"labels": {"alignments": [18], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1iEvPOjB", "sid": 24}, {"labels": {"alignments": [18], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "B1iEvPOjB", "sid": 25}, {"labels": {"alignments": [18], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 26}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 27}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 28}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 29}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "B1iEvPOjB", "sid": 30}], "metadata": {"anno": "anno10", "review": "SJx0zByAYr", "rebuttal": "B1iEvPOjB", "conference": "ICLR2020", "title": "On the Tunability of Optimizers in Deep Learning", "reviewer": "AnonReviewer1", "forum_id": "H1gEP6NFwr", "rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area."}}