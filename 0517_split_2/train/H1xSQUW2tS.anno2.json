{"review": [{"text_id": "H1xSQUW2tS", "sid": 0, "sentence": "This paper introduced a latent space model for reinforcement learning in vision-based control tasks."}, {"text_id": "H1xSQUW2tS", "sid": 1, "sentence": "It first learns a latent dynamics model, in which the transition model and the reward model can be learned on the latent state representations."}, {"text_id": "H1xSQUW2tS", "sid": 2, "sentence": "Using the learned latent state representations, it used an actor-critic model to learn a reactive policy to optimize the agent's behaviors in long-horizon continuous control tasks."}, {"text_id": "H1xSQUW2tS", "sid": 3, "sentence": "The method is applied to vision-based continuous control in 20 tasks in the Deepmind control suite."}, {"text_id": "H1xSQUW2tS", "sid": 4, "sentence": "Pros:"}, {"text_id": "H1xSQUW2tS", "sid": 5, "sentence": "1. The method used a latent dynamics model, which avoids reconstruction of the future images during inference."}, {"text_id": "H1xSQUW2tS", "sid": 6, "sentence": "2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner."}, {"text_id": "H1xSQUW2tS", "sid": 7, "sentence": "3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet."}, {"text_id": "H1xSQUW2tS", "sid": 8, "sentence": "Cons:"}, {"text_id": "H1xSQUW2tS", "sid": 9, "sentence": "1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet."}, {"text_id": "H1xSQUW2tS", "sid": 10, "sentence": "In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method."}, {"text_id": "H1xSQUW2tS", "sid": 11, "sentence": "However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below)."}, {"text_id": "H1xSQUW2tS", "sid": 12, "sentence": "2. Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates."}, {"text_id": "H1xSQUW2tS", "sid": 13, "sentence": "3. The world model is fixed while learning the action and value models, meaning that reinforcement learning of the actor-critic model cannot be used to improve the latent state model."}, {"text_id": "H1xSQUW2tS", "sid": 14, "sentence": "It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model."}, {"text_id": "H1xSQUW2tS", "sid": 15, "sentence": "Typos:"}, {"text_id": "H1xSQUW2tS", "sid": 16, "sentence": "Reward prediction along --> Reward prediction alone"}, {"text_id": "H1xSQUW2tS", "sid": 17, "sentence": "this limitation in latenby?"}], "reviewlabels": [{"text_id": "H1xSQUW2tS", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 10, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 13, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 14, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 15, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 16, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1xSQUW2tS", "sid": 17, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "HJezKUjisr", "sid": 0, "sentence": "Thank you for your review!"}, {"text_id": "HJezKUjisr", "sid": 1, "sentence": "> Pros:"}, {"text_id": "HJezKUjisr", "sid": 2, "sentence": "> 1."}, {"text_id": "HJezKUjisr", "sid": 3, "sentence": "The method used a latent dynamics model, which avoids reconstruction of the future images during inference."}, {"text_id": "HJezKUjisr", "sid": 4, "sentence": "> 2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner."}, {"text_id": "HJezKUjisr", "sid": 5, "sentence": "> 3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet."}, {"text_id": "HJezKUjisr", "sid": 6, "sentence": "This is an accurate summary."}, {"text_id": "HJezKUjisr", "sid": 7, "sentence": "We would like to highlight two additional points."}, {"text_id": "HJezKUjisr", "sid": 8, "sentence": "First, the improved performance is attributed to a novel actor-critic algorithm that uses analytic multi-step gradients of predicted state-values (not Q-values)."}, {"text_id": "HJezKUjisr", "sid": 9, "sentence": "Second, in addition to outperforming previous latent space planning methods, the proposed algorithm also outperforms the model-free D4PG algorithm, the previous state-of-the-art on this benchmark suite."}, {"text_id": "HJezKUjisr", "sid": 10, "sentence": "> 1."}, {"text_id": "HJezKUjisr", "sid": 11, "sentence": "The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet."}, {"text_id": "HJezKUjisr", "sid": 12, "sentence": "In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method."}, {"text_id": "HJezKUjisr", "sid": 13, "sentence": "However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below)."}, {"text_id": "HJezKUjisr", "sid": 14, "sentence": "Dreamer is a novel algorithm that belongs to the family of actor critic methods."}, {"text_id": "HJezKUjisr", "sid": 15, "sentence": "At a high level, previous approaches can be grouped into those using Reinforce gradients with V baselines (A3C, PPO, ACER) and those using deterministic or reparameterization gradients of learned Q functions (DDPG, SAC, MVE, STEVE)."}, {"text_id": "HJezKUjisr", "sid": 16, "sentence": "In comparison, Dreamer uses reparameterization gradients of V functions by backpropagating the value estimates through the latent dynamics."}, {"text_id": "HJezKUjisr", "sid": 17, "sentence": "Specifically, while Reinforce estimators typically learn V functions, these are only used to reduce the variance of the gradient estimate rather than directly maximizing them with respect to the actor."}, {"text_id": "HJezKUjisr", "sid": 18, "sentence": "Actor-critic algorithms that use analytic gradients of Q critics differ from Dreamer in two ways."}, {"text_id": "HJezKUjisr", "sid": 19, "sentence": "First, they learn a Q function rather than just a V function."}, {"text_id": "HJezKUjisr", "sid": 20, "sentence": "Second, the actor only maximizes the Q value predicted for the current time step rather than maximizing multi-step value estimates."}, {"text_id": "HJezKUjisr", "sid": 21, "sentence": "While MVE and STEVE learn dynamics models (from proprioceptive inputs), the dynamics are not directly used to update the policy."}, {"text_id": "HJezKUjisr", "sid": 22, "sentence": "Instead, they only serve for computing multi-step Q targets for learning the Q critic."}, {"text_id": "HJezKUjisr", "sid": 23, "sentence": "Thus, no gradients are backpropagated through the dynamics model for learning the actor or critic."}, {"text_id": "HJezKUjisr", "sid": 24, "sentence": "Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion."}, {"text_id": "HJezKUjisr", "sid": 25, "sentence": "> 2."}, {"text_id": "HJezKUjisr", "sid": 26, "sentence": "Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates."}, {"text_id": "HJezKUjisr", "sid": 27, "sentence": "As summarized above, Dreamer differs from previous actor-critic algorithms not just by using latent dynamics but also by using analytic multi-step gradients of a V function rather than one-step gradients Q function."}, {"text_id": "HJezKUjisr", "sid": 28, "sentence": "This renders Dreamer conceptually distinct from DDPG, SAC, MVE, and STEVE."}, {"text_id": "HJezKUjisr", "sid": 29, "sentence": "We have run experiments with MVE in the latent space of the same dynamics model and tuned the learning rate for actor and Q function."}, {"text_id": "HJezKUjisr", "sid": 30, "sentence": "We did not find an improvement over Dreamer (MVE worked worse across tasks) in these experiments, possibly because it only updates the Q function at the initial state of the imagination rollout."}, {"text_id": "HJezKUjisr", "sid": 31, "sentence": "Note that with a model, Q values can be computed by combining the dynamics with a value function, so learning Q is not necessary anymore."}, {"text_id": "HJezKUjisr", "sid": 32, "sentence": "Since using V in Dreamer outperforms the state-of-the-art D4PG agent and is simpler than the Q function in DDPG and MVE and substantially simpler than STEVE (ensemble of models) and SAC (two Q functions, one V function), we argue for this design choice."}, {"text_id": "HJezKUjisr", "sid": 33, "sentence": "> 3. [...] It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model."}, {"text_id": "HJezKUjisr", "sid": 34, "sentence": "We have run these experiments and it prevented learning completely."}, {"text_id": "HJezKUjisr", "sid": 35, "sentence": "Using gradients of the action or value models to shape the dynamics allows them to \"cheat\"."}, {"text_id": "HJezKUjisr", "sid": 36, "sentence": "Specifically, the actions maximize value estimates; using these to update the dynamics results in overly optimistic dynamics."}, {"text_id": "HJezKUjisr", "sid": 37, "sentence": "The values maximize Bellman consistency; using these to update the dynamics can encourage collapse of the latent space."}, {"text_id": "HJezKUjisr", "sid": 38, "sentence": "As a result, we suggest the perspective of viewing the dynamics as a fixed MDP during imagination training."}, {"text_id": "HJezKUjisr", "sid": 39, "sentence": "We will add a discussion of this to the paper."}, {"text_id": "HJezKUjisr", "sid": 40, "sentence": "If we addressed your concerns satisfactorily, we would be happy if you would consider updating your score."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 0}, {"labels": {"alignments": [4], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 1}, {"labels": {"alignments": [5], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 2}, {"labels": {"alignments": [5], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 3}, {"labels": {"alignments": [6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 4}, {"labels": {"alignments": [7], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 5}, {"labels": {"alignments": [4, 5, 6, 7], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 6}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 7}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 8}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 9}, {"labels": {"alignments": [9, 10, 11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 10}, {"labels": {"alignments": [9, 10, 11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 11}, {"labels": {"alignments": [9, 10, 11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 12}, {"labels": {"alignments": [9, 10, 11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 13}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 14}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 15}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 16}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 17}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 18}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 19}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 20}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 21}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 22}, {"labels": {"alignments": [9, 10, 11], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 23}, {"labels": {"alignments": [9, 10, 11], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 24}, {"labels": {"alignments": [12], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 25}, {"labels": {"alignments": [12], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 26}, {"labels": {"alignments": [12], "responsetype": "refute-question", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 27}, {"labels": {"alignments": [12], "responsetype": "refute-question", "coarseresponse": "dispute"}, "text_id": "HJezKUjisr", "sid": 28}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 29}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 30}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 31}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 32}, {"labels": {"alignments": [13, 14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 33}, {"labels": {"alignments": [13, 14], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 34}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 35}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 36}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 37}, {"labels": {"alignments": [13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 38}, {"labels": {"alignments": [13, 14], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "HJezKUjisr", "sid": 39}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HJezKUjisr", "sid": 40}], "metadata": {"anno": "anno2", "review": "H1xSQUW2tS", "rebuttal": "HJezKUjisr", "conference": "ICLR2020", "title": "Dream to Control: Learning Behaviors by Latent Imagination", "reviewer": "AnonReviewer1", "forum_id": "S1lOTC4tDS", "rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area."}}