{"review": [{"text_id": "S1lzrrz637", "sid": 0, "sentence": "The authors introduce a  novel  on-policy  temporally  consistent  exploration  strategy, named Neural  AdaptiveDropout Policy Exploration (NADPEx), for deep reinforcement learning agents."}, {"text_id": "S1lzrrz637", "sid": 1, "sentence": "The main idea is to sample from a distribution of plausible subnetworks modeling the temporally consistent exploration."}, {"text_id": "S1lzrrz637", "sid": 2, "sentence": "For this, the authors use the ideas of the standard dropout for deep networks."}, {"text_id": "S1lzrrz637", "sid": 3, "sentence": "Using the proposed  dropout transformation that is differentiable, the authors show that the KL regularizers on policy-space play an important role in stabilizing its learning."}, {"text_id": "S1lzrrz637", "sid": 4, "sentence": "The experimental validation is performed on continuous control learning tasks, showing the benefits of the proposed."}, {"text_id": "S1lzrrz637", "sid": 5, "sentence": "This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works."}, {"text_id": "S1lzrrz637", "sid": 6, "sentence": "This poses a challenge in evaluating this paper."}, {"text_id": "S1lzrrz637", "sid": 7, "sentence": "Nevertheless, this paper clearly explores and offers a novel approach for more efficient on-policy exploration which allows for more stable learning compared to traditional approaches."}, {"text_id": "S1lzrrz637", "sid": 8, "sentence": "Even though the authors answer positively to each of their four questions in the experiments section"}, {"text_id": "S1lzrrz637", "sid": 9, "sentence": ","}, {"text_id": "S1lzrrz637", "sid": 10, "sentence": "it would like that the authors provide more intuition why these improvements occur and also outline the limitations of their approach."}], "reviewlabels": [{"text_id": "S1lzrrz637", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1lzrrz637", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1lzrrz637", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1lzrrz637", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1lzrrz637", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1lzrrz637", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1lzrrz637", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1lzrrz637", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1lzrrz637", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "S1lzrrz637", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "S1lzrrz637", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}], "rebuttal": [{"text_id": "rkxN4ktETm", "sid": 0, "sentence": "Thank you very much for your strong recommendation!"}, {"text_id": "rkxN4ktETm", "sid": 1, "sentence": "1) Intuition about the improvement"}, {"text_id": "rkxN4ktETm", "sid": 2, "sentence": "Though not explained in Section 4."}, {"text_id": "rkxN4ktETm", "sid": 3, "sentence": "The intuition for NADPEx is given in Section 3."}, {"text_id": "rkxN4ktETm", "sid": 4, "sentence": "Interpretation for as efficient or even faster exploration in dense environment (4.1) is that NADPEx could encourage more diverse exploration, while absorb experience from it in a relatively efficient way."}, {"text_id": "rkxN4ktETm", "sid": 5, "sentence": "For sparse environments (4.2), where temporally consistent exploration is crucial for learning signal acquisition, NADPEx outperforms vanilla PPO."}, {"text_id": "rkxN4ktETm", "sid": 6, "sentence": "It could also beat parameter noise if difficulty is increased, because intuitively low variance in gradients is a boon for faster learning."}, {"text_id": "rkxN4ktETm", "sid": 7, "sentence": "Improvement in 4.3 and 4.4 are basically from the theoretical grounding of NADPEx, which we believe is one of our contributions."}, {"text_id": "rkxN4ktETm", "sid": 8, "sentence": "Specifically, improvement in 4.3 is from high level stochasticity's adaptation to the low level; while that in 4.4 could be interpreted with the idea of trust region, that policy should be updated to somewhere near the sampling policy in the policy space, such that collected experience are usable (on-policy)."}, {"text_id": "rkxN4ktETm", "sid": 9, "sentence": "In NADPEx, trust region also contains the meaning that dropout policies are close to each other for more efficient exploration."}, {"text_id": "rkxN4ktETm", "sid": 10, "sentence": "2) Limitation of NADPEx"}, {"text_id": "rkxN4ktETm", "sid": 11, "sentence": "One of the limitation we see from NADPEx is that dropout policies are not directly interpretable from their network structures, while interpretability and composibility are prerequisites for reusing them in more complicated tasks."}, {"text_id": "rkxN4ktETm", "sid": 12, "sentence": "Luckily, modeled as latent random variables, an information term could be added to the objective as in [1, 2]."}, {"text_id": "rkxN4ktETm", "sid": 13, "sentence": "This is also a direction for future research work."}, {"text_id": "rkxN4ktETm", "sid": 14, "sentence": "[1] Florensa et al., \"Stochastic neural networks for hierarchical reinforcement learning\", ICLR 2017."}, {"text_id": "rkxN4ktETm", "sid": 15, "sentence": "[2] Hausman et al., \"Learning an Embedding Space for Transferable Robot Skills\", ICLR 2018."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 0}, {"labels": {"alignments": [8, 10], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 1}, {"labels": {"alignments": [8, 10], "responsetype": "refute-question", "coarseresponse": "dispute"}, "text_id": "rkxN4ktETm", "sid": 2}, {"labels": {"alignments": [8, 10], "responsetype": "refute-question", "coarseresponse": "dispute"}, "text_id": "rkxN4ktETm", "sid": 3}, {"labels": {"alignments": [8, 10], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 4}, {"labels": {"alignments": [8, 10], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 5}, {"labels": {"alignments": [8, 10], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 6}, {"labels": {"alignments": [8, 10], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 7}, {"labels": {"alignments": [8, 10], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 8}, {"labels": {"alignments": [8, 10], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 9}, {"labels": {"alignments": [8, 10], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 10}, {"labels": {"alignments": [8, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxN4ktETm", "sid": 11}, {"labels": {"alignments": [8, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxN4ktETm", "sid": 12}, {"labels": {"alignments": [8, 10], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "rkxN4ktETm", "sid": 13}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 14}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "rkxN4ktETm", "sid": 15}], "metadata": {"anno": "anno10", "review": "S1lzrrz637", "rebuttal": "rkxN4ktETm", "conference": "ICLR2019", "title": "NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning", "reviewer": "AnonReviewer2", "forum_id": "rkxciiC9tm", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}