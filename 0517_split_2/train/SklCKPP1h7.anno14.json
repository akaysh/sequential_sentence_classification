{"review": [{"text_id": "SklCKPP1h7", "sid": 0, "sentence": "I raised my rating. After the rebuttal."}, {"text_id": "SklCKPP1h7", "sid": 1, "sentence": "- the authors address most of my concerns."}, {"text_id": "SklCKPP1h7", "sid": 2, "sentence": "- it's better to show time v.s. testing accuracy as well."}, {"text_id": "SklCKPP1h7", "sid": 3, "sentence": "the per-epoch time for each method is different."}, {"text_id": "SklCKPP1h7", "sid": 4, "sentence": "- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic."}, {"text_id": "SklCKPP1h7", "sid": 5, "sentence": "-------------------------------------------------------------"}, {"text_id": "SklCKPP1h7", "sid": 6, "sentence": "This paper presents a method to update hyper-parameters (e.g. learning rate) before updating of model parameters."}, {"text_id": "SklCKPP1h7", "sid": 7, "sentence": "The idea is simple but intuitive. I am conservative about my rating now, I will consider raising it after the rebuttal."}, {"text_id": "SklCKPP1h7", "sid": 8, "sentence": "1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters."}, {"text_id": "SklCKPP1h7", "sid": 9, "sentence": "- no need to write so much in section 2.1, the surrogate is simple and common in optimization for parameters."}, {"text_id": "SklCKPP1h7", "sid": 10, "sentence": "After all, newton method and natural gradients method are not used in experiments."}, {"text_id": "SklCKPP1h7", "sid": 11, "sentence": "- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed."}, {"text_id": "SklCKPP1h7", "sid": 12, "sentence": "2. No need to write so much decorated bounds in section 3."}, {"text_id": "SklCKPP1h7", "sid": 13, "sentence": "The convergence analysis is on Z, not on parameters x and hyper-parameters theta."}, {"text_id": "SklCKPP1h7", "sid": 14, "sentence": "So, bounds here can not be used to explain empirical observations in Section 5."}, {"text_id": "SklCKPP1h7", "sid": 15, "sentence": "3. Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?"}, {"text_id": "SklCKPP1h7", "sid": 16, "sentence": "4. Authors have done a good comparison in the context of deep nets."}, {"text_id": "SklCKPP1h7", "sid": 17, "sentence": "However,"}, {"text_id": "SklCKPP1h7", "sid": 18, "sentence": "- could the authors compare with changing step-size?"}, {"text_id": "SklCKPP1h7", "sid": 19, "sentence": "In most of experiments, the baseline methods, i.e. RMSProp are used with fixed rates."}, {"text_id": "SklCKPP1h7", "sid": 20, "sentence": "Is it better to decay learning rates for toy data sets?"}, {"text_id": "SklCKPP1h7", "sid": 21, "sentence": "It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems."}, {"text_id": "SklCKPP1h7", "sid": 22, "sentence": "- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., \"For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01\", \"while for RMSprop-APO, the best lambda was 0.0001"}, {"text_id": "SklCKPP1h7", "sid": 23, "sentence": "\""}, {"text_id": "SklCKPP1h7", "sid": 24, "sentence": "."}, {"text_id": "SklCKPP1h7", "sid": 25, "sentence": "What are reasons for these?"}, {"text_id": "SklCKPP1h7", "sid": 26, "sentence": "- In Section 5.2, it is said lambda is tuned by grid-search."}, {"text_id": "SklCKPP1h7", "sid": 27, "sentence": "Tuning a good lambda v.s. tuning a good step-size, which one costs more?"}], "reviewlabels": [{"text_id": "SklCKPP1h7", "sid": 0, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 1, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 2, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 3, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 4, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 7, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Other", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Other", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 15, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 17, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "SklCKPP1h7", "sid": 18, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "SklCKPP1h7", "sid": 19, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 20, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 21, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 22, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "SklCKPP1h7", "sid": 23, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "SklCKPP1h7", "sid": 24, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "SklCKPP1h7", "sid": 25, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 26, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SklCKPP1h7", "sid": 27, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "Bkg51P6KCX", "sid": 0, "sentence": "Thank you for your helpful comments."}, {"text_id": "Bkg51P6KCX", "sid": 1, "sentence": "We have improved the writing to incorporate your feedback."}, {"text_id": "Bkg51P6KCX", "sid": 2, "sentence": "We have also performed more experiments to compare APO to manual learning rate schedules."}, {"text_id": "Bkg51P6KCX", "sid": 3, "sentence": "Q: Please explain more how gradients w.r.t hyper-parameters are computed."}, {"text_id": "Bkg51P6KCX", "sid": 4, "sentence": "We implemented custom versions of the optimizers we consider (SGD, RMSprop, and K-FAC) that treat the optimization hyperparameters as variables in the computation graph for an optimization step."}, {"text_id": "Bkg51P6KCX", "sid": 5, "sentence": "We then use automatic differentiation to compute the gradient of the meta-objective with respect to the hyperparameters (e.g., the learning rate)."}, {"text_id": "Bkg51P6KCX", "sid": 6, "sentence": "Q: Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?"}, {"text_id": "Bkg51P6KCX", "sid": 7, "sentence": "Each meta-optimization step requires approximately the same amount of computation as a parameter update for the model."}, {"text_id": "Bkg51P6KCX", "sid": 8, "sentence": "By using the default meta learning rate suggested in our updated paper, we can amortize the meta-optimization by performing 1 meta-update for every K steps of the base optimization."}, {"text_id": "Bkg51P6KCX", "sid": 9, "sentence": "We found that K=10 works well across our settings, while reducing the computational requirements of APO to just a small fraction more than the original training procedure."}, {"text_id": "Bkg51P6KCX", "sid": 10, "sentence": "We have added a discussion of our meta-optimization setup and the efficiency of APO in Section 5 of the updated paper."}, {"text_id": "Bkg51P6KCX", "sid": 11, "sentence": "Q: No need to write so much decorated bounds in section 3."}, {"text_id": "Bkg51P6KCX", "sid": 12, "sentence": "The convergence analysis is on Z, not on parameters x and hyper-parameters theta."}, {"text_id": "Bkg51P6KCX", "sid": 13, "sentence": "So, bounds here cannot be used to explain empirical observations in Section 5."}, {"text_id": "Bkg51P6KCX", "sid": 14, "sentence": "The convergence of the network output Z directly indicates the rate of decrease of the loss function, which is exactly what we observe in practice."}, {"text_id": "Bkg51P6KCX", "sid": 15, "sentence": "Although the assumption of a global optimization oracle is not realistic, we believe our theoretical justification provides insight into why the method works."}, {"text_id": "Bkg51P6KCX", "sid": 16, "sentence": "One important takeaway from the theoretical analysis is that running gradient descent on output space can potentially accelerate the optimization (since the convergence bounds have better constants)."}, {"text_id": "Bkg51P6KCX", "sid": 17, "sentence": "This directly motivates the regularization term in our meta objective to be defined as the discrepancy of network outputs instead of the network parameters, which is essential to our technique."}, {"text_id": "Bkg51P6KCX", "sid": 18, "sentence": "Q: Could the authors compare with changing step-size?"}, {"text_id": "Bkg51P6KCX", "sid": 19, "sentence": "Thank you for the suggestion."}, {"text_id": "Bkg51P6KCX", "sid": 20, "sentence": "We have added comparisons with custom learning rate schedules for CIFAR-10 and CIFAR-100."}, {"text_id": "Bkg51P6KCX", "sid": 21, "sentence": "We updated our results for CIFAR-10/100 using a larger network, ResNet34, instead of the VGG11 model used in the previous version, and we used a manual learning rate decay schedule where we trained for 200 epochs, decaying the learning rate by a factor of 5 three times during training."}, {"text_id": "Bkg51P6KCX", "sid": 22, "sentence": "We found that APO is competitive with the custom schedule, achieving similar training loss and test accuracy."}, {"text_id": "Bkg51P6KCX", "sid": 23, "sentence": "We provide results in our response to all reviewers at the top."}, {"text_id": "Bkg51P6KCX", "sid": 24, "sentence": "Q: How to tune lambda?"}, {"text_id": "Bkg51P6KCX", "sid": 25, "sentence": "Tuning a good lambda v.s. tuning a good step-size, which one costs more?"}, {"text_id": "Bkg51P6KCX", "sid": 26, "sentence": "We tune lambda by performing a grid search over the range {1e-1, 1e-2, 1e-3, 1e-4, 1e-5}. Because each lambda value gives rise to a learning rate schedule, tuning lambda yields significantly more value than tuning a fixed learning rate."}, {"text_id": "Bkg51P6KCX", "sid": 27, "sentence": "Instead of trying to come up with a custom learning rate schedule, which would require deciding how frequently to decay the learning rate, and by what factor it should be decayed, all one needs to do is perform a grid search over a fixed set of lambdas to find an automated schedule that is competitive with hand-designed schedules (which are the result of years of accumulated experience in the field)."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 0}, {"labels": {"alignments": [], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 1}, {"labels": {"alignments": [], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 2}, {"labels": {"alignments": [11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 3}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 4}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 5}, {"labels": {"alignments": [15], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 6}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 7}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 8}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 9}, {"labels": {"alignments": [15], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 10}, {"labels": {"alignments": [12], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 11}, {"labels": {"alignments": [13], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 12}, {"labels": {"alignments": [13, 14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 13}, {"labels": {"alignments": [13, 14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Bkg51P6KCX", "sid": 14}, {"labels": {"alignments": [13, 14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Bkg51P6KCX", "sid": 15}, {"labels": {"alignments": [13, 14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Bkg51P6KCX", "sid": 16}, {"labels": {"alignments": [13, 14], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Bkg51P6KCX", "sid": 17}, {"labels": {"alignments": [18], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 18}, {"labels": {"alignments": [18], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 19}, {"labels": {"alignments": [18], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 20}, {"labels": {"alignments": [18], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 21}, {"labels": {"alignments": [18], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 22}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 23}, {"labels": {"alignments": [22], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 24}, {"labels": {"alignments": [27], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg51P6KCX", "sid": 25}, {"labels": {"alignments": [22], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 26}, {"labels": {"alignments": [22], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bkg51P6KCX", "sid": 27}], "metadata": {"anno": "anno14", "review": "SklCKPP1h7", "rebuttal": "Bkg51P6KCX", "conference": "ICLR2019", "title": "Online Hyperparameter Adaptation via Amortized Proximal Optimization", "reviewer": "AnonReviewer3", "forum_id": "rJl6M2C5Y7", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}