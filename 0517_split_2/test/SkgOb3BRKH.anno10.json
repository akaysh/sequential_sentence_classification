{"review": [{"text_id": "SkgOb3BRKH", "sid": 0, "sentence": "This paper proposes PowerSGD for improving SGD to train deep neural networks."}, {"text_id": "SkgOb3BRKH", "sid": 1, "sentence": "The main idea is to raise the stochastic gradient to a certain power."}, {"text_id": "SkgOb3BRKH", "sid": 2, "sentence": "Convergence analysis and experimental results on CIFAR-10/CIFAR-100/Imagenet and classical CNN architectures are given."}, {"text_id": "SkgOb3BRKH", "sid": 3, "sentence": "Overall, this is a clearly-written paper with comprehensive experiments."}, {"text_id": "SkgOb3BRKH", "sid": 4, "sentence": "My major concern is whether the results are significant enough to deserve acceptance."}, {"text_id": "SkgOb3BRKH", "sid": 5, "sentence": "The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum)."}, {"text_id": "SkgOb3BRKH", "sid": 6, "sentence": "I am not sure how novel the convergence analysis for PowerSGD is, and it would be nice if the authors could discuss technical challenges they overcome in the introduction."}], "reviewlabels": [{"text_id": "SkgOb3BRKH", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkgOb3BRKH", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkgOb3BRKH", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkgOb3BRKH", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkgOb3BRKH", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkgOb3BRKH", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SkgOb3BRKH", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "S1eCuflBiS", "sid": 0, "sentence": "We thank the reviewer for the comments."}, {"text_id": "S1eCuflBiS", "sid": 1, "sentence": "We justify the novelty and significance of the contributions made by this paper as follows."}, {"text_id": "S1eCuflBiS", "sid": 2, "sentence": "1) Novelty of the convergence analysis: The paper by Yuan et al. did not present proof of convergence in the discrete-time setting."}, {"text_id": "S1eCuflBiS", "sid": 3, "sentence": "The authors only provided convergence of the ODE models."}, {"text_id": "S1eCuflBiS", "sid": 4, "sentence": "On the other hand, convergence analysis of momentum methods in non-convex setting is an important but under-explored area  (Yan et al., 2018)."}, {"text_id": "S1eCuflBiS", "sid": 5, "sentence": "In the current paper, the convergence results are proved for non-convex objective functions satisfying mild assumptions."}, {"text_id": "S1eCuflBiS", "sid": 6, "sentence": "Appropriate use of some sharp estimates allowed us to obtain concise bounds on convergence of the entire class of PoweredSGD methods for $\\gamma\\in[0,1]$ and the bounds continuously depend on parameters $\\gamma$ and $\\beta$. In the special cases ($\\gamma=0,1$, $\\beta=0$), these bounds matches the best known bounds for GD/SGD/SGDM in the non-convex setting."}, {"text_id": "S1eCuflBiS", "sid": 7, "sentence": "More specifically, we would like to draw the reviewer's attention to the following two papers:"}, {"text_id": "S1eCuflBiS", "sid": 8, "sentence": "*"}, {"text_id": "S1eCuflBiS", "sid": 9, "sentence": "[Yan18] Yan, Y., T. Yang, Z. Li, Q. Lin, and Y. Yang. \"A unified analysis of stochastic momentum methods for deep learning.\" In IJCAI International Joint Conference on Artificial Intelligence. 2018."}, {"text_id": "S1eCuflBiS", "sid": 10, "sentence": "*  [Bernstein18] Bernstein, Jeremy, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. \"SIGNSGD: Compressed Optimisation for Non-Convex Problems.\" In International Conference on Machine Learning, pp. 559-568. 2018. (Theorem 3)"}, {"text_id": "S1eCuflBiS", "sid": 11, "sentence": "We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers."}, {"text_id": "S1eCuflBiS", "sid": 12, "sentence": "Please take a look at Theorems 1 and 2 in [Yan18] and Theorem 3 in [Bernstein18]."}, {"text_id": "S1eCuflBiS", "sid": 13, "sentence": "We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3)."}, {"text_id": "S1eCuflBiS", "sid": 14, "sentence": "2) Novelty of experiments: The current paper presents substantially more comprehensive experiments for benchmarking the proposed class of optimizers against other popular optimization methods for deep learning tasks."}, {"text_id": "S1eCuflBiS", "sid": 15, "sentence": "In particular, we highlight the experiments on vanishing gradients and learning rate schedules."}, {"text_id": "S1eCuflBiS", "sid": 16, "sentence": "This, in addition to the potential to accelerate initial convergence, makes the proposed PoweredSGD methods useful in many potential applications."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "S1eCuflBiS", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "S1eCuflBiS", "sid": 1}, {"labels": {"alignments": [5, 6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1eCuflBiS", "sid": 2}, {"labels": {"alignments": [5, 6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1eCuflBiS", "sid": 3}, {"labels": {"alignments": [5, 6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1eCuflBiS", "sid": 4}, {"labels": {"alignments": [5, 6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1eCuflBiS", "sid": 5}, {"labels": {"alignments": [5, 6], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1eCuflBiS", "sid": 6}, {"labels": {"alignments": [5, 6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "S1eCuflBiS", "sid": 7}, {"labels": {"alignments": [5, 6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "S1eCuflBiS", "sid": 8}, {"labels": {"alignments": [5, 6], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "S1eCuflBiS", "sid": 9}, {"labels": {"alignments": [5, 6], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "S1eCuflBiS", "sid": 10}, {"labels": {"alignments": [5, 6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1eCuflBiS", "sid": 11}, {"labels": {"alignments": [5, 6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1eCuflBiS", "sid": 12}, {"labels": {"alignments": [5, 6], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "S1eCuflBiS", "sid": 13}, {"labels": {"alignments": [4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1eCuflBiS", "sid": 14}, {"labels": {"alignments": [4], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "S1eCuflBiS", "sid": 15}, {"labels": {"alignments": [4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "S1eCuflBiS", "sid": 16}], "metadata": {"anno": "anno10", "review": "SkgOb3BRKH", "rebuttal": "S1eCuflBiS", "conference": "ICLR2020", "title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "reviewer": "AnonReviewer2", "forum_id": "rJlqoTEtDB", "rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area."}}