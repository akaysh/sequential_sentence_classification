{"review": [{"text_id": "HkeEorv1hm", "sid": 0, "sentence": "In their abstract, the authors claim to provide state-of-the-art perplexity on Penn Treebank, which is not true."}, {"text_id": "HkeEorv1hm", "sid": 1, "sentence": "As the authors state, their notion of \"state-of-the-art\" excludes exactly that earlier work, which does provide state-of-the-art perplexity on Penn Treebank (Yang et al. 2017), as stated in Sec. 4.1."}, {"text_id": "HkeEorv1hm", "sid": 2, "sentence": "The question is, why one would exlude the mixture-of-softmax approach here?"}, {"text_id": "HkeEorv1hm", "sid": 3, "sentence": "This is clearly misleading."}, {"text_id": "HkeEorv1hm", "sid": 4, "sentence": "The authors introduce the idea of past decoding for the purpose of regularization."}, {"text_id": "HkeEorv1hm", "sid": 5, "sentence": "It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general."}, {"text_id": "HkeEorv1hm", "sid": 6, "sentence": "The results obtained show moderate improvements of approx. 1 point in perplexity on top of their best current result on Penn Treebank."}, {"text_id": "HkeEorv1hm", "sid": 7, "sentence": "Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets."}, {"text_id": "HkeEorv1hm", "sid": 8, "sentence": "The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications."}, {"text_id": "HkeEorv1hm", "sid": 9, "sentence": "Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches."}, {"text_id": "HkeEorv1hm", "sid": 10, "sentence": "It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications."}], "reviewlabels": [{"text_id": "HkeEorv1hm", "sid": 0, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 1, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 4, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 6, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeEorv1hm", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "BJe0cdJfC7", "sid": 0, "sentence": "We thank the reviewer for reading the paper and the comments."}, {"text_id": "BJe0cdJfC7", "sid": 1, "sentence": "As already stated in the comments below, our claim of state-of-the-art in the original manuscript pertains to models with a single softmax, which we clearly state in section 4.1."}, {"text_id": "BJe0cdJfC7", "sid": 2, "sentence": "We will update the abstract to remove any confusion."}, {"text_id": "BJe0cdJfC7", "sid": 3, "sentence": "As suggested by multiple reviewers, we have performed further experiments by incorporating our Past Decode Regularization (PDR) in the mixture-of-softmax (AWD-LSTM-MoS) model of (Yang et al. 2017)."}, {"text_id": "BJe0cdJfC7", "sid": 4, "sentence": "We use the same model sizes as used in the paper."}, {"text_id": "BJe0cdJfC7", "sid": 5, "sentence": "As shown below, we observe gains of 0.4 and 1.0 perplexity points for PTB and WT2, while with dynamic evaluation the gains are 0.4 in both cases."}, {"text_id": "BJe0cdJfC7", "sid": 6, "sentence": "AWD-LSTM-MoS+PDR  || AWD-LSTM-MoS (Yang et al. 2017)"}, {"text_id": "BJe0cdJfC7", "sid": 7, "sentence": "Penn Treebank with finetuning -"}, {"text_id": "BJe0cdJfC7", "sid": 8, "sentence": "56.2/53.8"}, {"text_id": "BJe0cdJfC7", "sid": 9, "sentence": "||  56.5/54.4"}, {"text_id": "BJe0cdJfC7", "sid": 10, "sentence": "Penn Treebank with dynamic evaluation -"}, {"text_id": "BJe0cdJfC7", "sid": 11, "sentence": "48.0/47.3"}, {"text_id": "BJe0cdJfC7", "sid": 12, "sentence": "||  48.3/47.7"}, {"text_id": "BJe0cdJfC7", "sid": 13, "sentence": "WikiText-2 with finetuning -"}, {"text_id": "BJe0cdJfC7", "sid": 14, "sentence": "63.0/60.5"}, {"text_id": "BJe0cdJfC7", "sid": 15, "sentence": "||  63.9/61.5"}, {"text_id": "BJe0cdJfC7", "sid": 16, "sentence": "WikiText-2 with dynamic evaluation -"}, {"text_id": "BJe0cdJfC7", "sid": 17, "sentence": "42.0/40.3"}, {"text_id": "BJe0cdJfC7", "sid": 18, "sentence": "||  42.4/40.7"}, {"text_id": "BJe0cdJfC7", "sid": 19, "sentence": "Note that, we performed very limited hyperparameter tuning in the vicinity of the hyperparameters used by (Yang et al. 2017) and a more exhaustive search is likely to lead to better gains."}, {"text_id": "BJe0cdJfC7", "sid": 20, "sentence": "Thus, the gains due to PDR generalize to more complex models like AWD-LSTM-MoS+PDR."}, {"text_id": "BJe0cdJfC7", "sid": 21, "sentence": "We can justify PDR theoretically as an inductive bias on the language model."}, {"text_id": "BJe0cdJfC7", "sid": 22, "sentence": "The observed bigrams in a language are not random and the distribution of the second word given the first word in a bigram is not uniform."}, {"text_id": "BJe0cdJfC7", "sid": 23, "sentence": "Similarly, the distribution of the first word given the second word will be far from uniform."}, {"text_id": "BJe0cdJfC7", "sid": 24, "sentence": "A RNN based language model models the first dependence (and more long range ones) and our proposed PDR tries to model the second one."}, {"text_id": "BJe0cdJfC7", "sid": 25, "sentence": "In a unidirectional language model, we cannot look into the future tokens and hence we use the output distribution as a proxy for the \"true second word\" and decode the distribution of the first word."}, {"text_id": "BJe0cdJfC7", "sid": 26, "sentence": "Thus the PDR term can be thought of as biasing the language model to retain more information about the distribution of the first word given the second word in a bigram."}, {"text_id": "BJe0cdJfC7", "sid": 27, "sentence": "We believe language modeling is a fundamental problem in NLP and our work continues a long stream of papers that have achieved steadily lower perplexities over the past few years."}, {"text_id": "BJe0cdJfC7", "sid": 28, "sentence": "We evaluated our approach on two standard datasets that have been used as a benchmark in most of these papers."}, {"text_id": "BJe0cdJfC7", "sid": 29, "sentence": "As suggested by multiple reviewers, we have conducted further experiments on the Gigaword corpus to test PDR on larger corpora."}, {"text_id": "BJe0cdJfC7", "sid": 30, "sentence": "Specifically,  we use a 2-layer LSTM with hidden dimension 1024 and a word embedding dimension of 1024."}, {"text_id": "BJe0cdJfC7", "sid": 31, "sentence": "We truncated the vocabulary by keeping approximately 100k words with the highest frequency and used the same validation and test sets as (Yang et al. 2017)."}, {"text_id": "BJe0cdJfC7", "sid": 32, "sentence": "We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 points in the test perplexity."}, {"text_id": "BJe0cdJfC7", "sid": 33, "sentence": "Note that we tuned the PDR loss coefficient very coarsely and tuning it further could lead to higher gains."}, {"text_id": "BJe0cdJfC7", "sid": 34, "sentence": "We will update the manuscript with these additional results and discussion and post it shortly."}, {"text_id": "BJe0cdJfC7", "sid": 35, "sentence": "Yang et al. 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv:1711.03953."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "BJe0cdJfC7", "sid": 0}, {"labels": {"alignments": [0, 1, 2, 3], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "BJe0cdJfC7", "sid": 1}, {"labels": {"alignments": [0, 1, 2, 3], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 2}, {"labels": {"alignments": [], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 3}, {"labels": {"alignments": [], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 4}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 5}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 6}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 7}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 8}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 9}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 10}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 11}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 12}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 13}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 14}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 15}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 16}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 17}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 18}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 19}, {"labels": {"alignments": [], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 20}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 21}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 22}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 23}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 24}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 25}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 26}, {"labels": {"alignments": [6, 7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 27}, {"labels": {"alignments": [6, 7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 28}, {"labels": {"alignments": [6, 7], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 29}, {"labels": {"alignments": [6, 7, 8, 9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 30}, {"labels": {"alignments": [6, 7, 8, 9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 31}, {"labels": {"alignments": [6, 7, 8, 9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 32}, {"labels": {"alignments": [6, 7, 8, 9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 33}, {"labels": {"alignments": [6, 7, 8, 9, 10], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "BJe0cdJfC7", "sid": 34}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "BJe0cdJfC7", "sid": 35}], "metadata": {"anno": "anno10", "review": "HkeEorv1hm", "rebuttal": "BJe0cdJfC7", "conference": "ICLR2019", "title": "Improved Language Modeling by Decoding the Past", "reviewer": "AnonReviewer2", "forum_id": "SklckhR5Ym", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}