{"review": [{"text_id": "rJe9E2ITYr", "sid": 0, "sentence": "The authors propose to augment NMT with a grounded inventory of images."}, {"text_id": "rJe9E2ITYr", "sid": 1, "sentence": "The intuition is clear and the premise is very tempting."}, {"text_id": "rJe9E2ITYr", "sid": 2, "sentence": "The key architectural choice is to allow the transformer to use language embeddings to attend into a topic-image lookup table."}, {"text_id": "rJe9E2ITYr", "sid": 3, "sentence": "The proportion is learned to balance how much signal comes from each source."}, {"text_id": "rJe9E2ITYr", "sid": 4, "sentence": "Figure 4, attempts to investigate the importance of this sharing and its effects on performance."}, {"text_id": "rJe9E2ITYr", "sid": 5, "sentence": "While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking."}, {"text_id": "rJe9E2ITYr", "sid": 6, "sentence": "For example, \"The old system of private arbitration courts is off the table\" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture."}, {"text_id": "rJe9E2ITYr", "sid": 7, "sentence": "It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets."}, {"text_id": "rJe9E2ITYr", "sid": 8, "sentence": "I trust that the authors did in fact achieve these results but I cannot figure out how or why."}, {"text_id": "rJe9E2ITYr", "sid": 9, "sentence": "This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances."}, {"text_id": "rJe9E2ITYr", "sid": 10, "sentence": "In contrast, it does make sense that Multi30K would benefit from this architecture."}, {"text_id": "rJe9E2ITYr", "sid": 11, "sentence": "As a minor note, were different feature extractors compared?"}, {"text_id": "rJe9E2ITYr", "sid": 12, "sentence": "The recent flurry of papers on multimodal transformers indicate that deeper resnet stacks correspond to improved downstream performance."}, {"text_id": "rJe9E2ITYr", "sid": 13, "sentence": "Is that also true in this domain?"}], "reviewlabels": [{"text_id": "rJe9E2ITYr", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 12, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJe9E2ITYr", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rJgabTX4sr", "sid": 0, "sentence": "Thanks for your insightful comments."}, {"text_id": "rJgabTX4sr", "sid": 1, "sentence": "1. How or why is the benefit."}, {"text_id": "rJgabTX4sr", "sid": 2, "sentence": "This comment is insightful and we also considered about it."}, {"text_id": "rJgabTX4sr", "sid": 3, "sentence": "Intuitively, we would easily fall into the connections between each sentence and image."}, {"text_id": "rJgabTX4sr", "sid": 4, "sentence": "However, it is nearly impossible to pair sentence with images with completely the same meaning all the time."}, {"text_id": "rJgabTX4sr", "sid": 5, "sentence": "According to our investigation, we conclude that the major contribution would be more effective contextualized sentence encoding for better representation from the visual clue combination instead of single image enhancement for encoding each individual sentence or word."}, {"text_id": "rJgabTX4sr", "sid": 6, "sentence": "According to Distributional Hypothesis (Harris et al., 1954) which states that \u201cwords that occur in similar contexts tend to have similar meanings\u201d, we are inspired to extend the concept in multimodal world, \u201cthe sentences with similar meanings would be likely to pair with similar even the same images\u201d, where the consistent images (with similar topic) could play the role of topic or type clues for similar sentence modeling."}, {"text_id": "rJgabTX4sr", "sid": 7, "sentence": "For your example, the topic words are {private, courts, table}, which can be paired with relevant images and other sentences with the same (similar) topics will be paired with the same (similar) group of images."}, {"text_id": "rJgabTX4sr", "sid": 8, "sentence": "This is also very similar to the idea of word embedding by taking each image as a \u201cword\u201d."}, {"text_id": "rJgabTX4sr", "sid": 9, "sentence": "Because we use the average pooled output of ResNet, each image is represented as 2400d vector."}, {"text_id": "rJgabTX4sr", "sid": 10, "sentence": "For all the 29,000 images, we have an embedding layer with size (29000, 2400)."}, {"text_id": "rJgabTX4sr", "sid": 11, "sentence": "The \u201ccontent\u201d of the image is just like the embedding initialization."}, {"text_id": "rJgabTX4sr", "sid": 12, "sentence": "It indeed makes effects, but the capacity of the neural network is not up to it."}, {"text_id": "rJgabTX4sr", "sid": 13, "sentence": "In contrast, the mapping from text word to the index in the word embedding is critical."}, {"text_id": "rJgabTX4sr", "sid": 14, "sentence": "Similarly, the mapping of sentence to image in image embedding would be essential, i.e., the similar sentences (with the same topic words) tend to map the similar images."}, {"text_id": "rJgabTX4sr", "sid": 15, "sentence": "To verify the hypothesis, we shuffle the image embeddings but keep the lookup table, to only exchange the features of each image but maintain the sentence-image mapping."}, {"text_id": "rJgabTX4sr", "sid": 16, "sentence": "Unsurprisingly, the BLEU score (EN-RO) is 33.53, which is very close to the reported one (33.78)."}, {"text_id": "rJgabTX4sr", "sid": 17, "sentence": "In addition, we randomly initialize the image embedding instead of ResNet, the result is 33.28."}, {"text_id": "rJgabTX4sr", "sid": 18, "sentence": "In comparison, if we randomly retrieve unrelated images to break the lookup, the result is 32.14."}, {"text_id": "rJgabTX4sr", "sid": 19, "sentence": "These results verify the necessity of the lookup table."}, {"text_id": "rJgabTX4sr", "sid": 20, "sentence": "We have added a detailed discussion in the paper (please see Analysis 6.1)."}, {"text_id": "rJgabTX4sr", "sid": 21, "sentence": "We believe this finding would be suggestive for the future research since most previous work focused on the content of the image itself."}, {"text_id": "rJgabTX4sr", "sid": 22, "sentence": "As a different research line, we highlight the consistency among the mono-modality to bridge the gap of language and image modeling."}, {"text_id": "rJgabTX4sr", "sid": 23, "sentence": "2. Why stop words are ignored."}, {"text_id": "rJgabTX4sr", "sid": 24, "sentence": "According to the explanation above, we think the spatial relations or grammatical nuances would not be so important in this task if we take the images as topic guidance."}, {"text_id": "rJgabTX4sr", "sid": 25, "sentence": "Ignoring the stopwords can help us get rid of the disturbance of unnecessary high-frequency words (such as function words) being the topic, as the standard practice for TF-IDF topic extraction."}, {"text_id": "rJgabTX4sr", "sid": 26, "sentence": "3. Comparison of different feature extractors."}, {"text_id": "rJgabTX4sr", "sid": 27, "sentence": "Yes. We compared with ResNet101 and ResNet152 on EN-RO."}, {"text_id": "rJgabTX4sr", "sid": 28, "sentence": "The BLEU scores are 33.63 and 33.87."}, {"text_id": "rJgabTX4sr", "sid": 29, "sentence": "It seems deeper ResNet indeed gives better results but the difference is not very significant."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 0}, {"labels": {"alignments": [8], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 1}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 2}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 3}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 4}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 5}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 6}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 7}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 8}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 9}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 10}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 11}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 12}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 13}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 14}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 15}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 16}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 17}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 18}, {"labels": {"alignments": [8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 19}, {"labels": {"alignments": [8], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJgabTX4sr", "sid": 20}, {"labels": {"alignments": [8], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "rJgabTX4sr", "sid": 21}, {"labels": {"alignments": [8], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "rJgabTX4sr", "sid": 22}, {"labels": {"alignments": [9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 23}, {"labels": {"alignments": [9], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 24}, {"labels": {"alignments": [9], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 25}, {"labels": {"alignments": [11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgabTX4sr", "sid": 26}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgabTX4sr", "sid": 27}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgabTX4sr", "sid": 28}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgabTX4sr", "sid": 29}], "metadata": {"anno": "anno10", "review": "rJe9E2ITYr", "rebuttal": "rJgabTX4sr", "conference": "ICLR2020", "title": "Neural Machine Translation with Universal Visual Representation", "reviewer": "AnonReviewer1", "forum_id": "Byl8hhNYPS", "rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area."}}