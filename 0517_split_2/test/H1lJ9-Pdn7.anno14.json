{"review": [{"text_id": "H1lJ9-Pdn7", "sid": 0, "sentence": "This article presents experiments on medium- and large-scale language modeling when the ideas of adaptive softmax (Grave et al., 2017) are extended to input representations."}, {"text_id": "H1lJ9-Pdn7", "sid": 1, "sentence": "The article is well written and I find the contribution simple, but interesting."}, {"text_id": "H1lJ9-Pdn7", "sid": 2, "sentence": "It is a reasonable and well supported increment from adaptive softmax of Grave et al. (2017)."}, {"text_id": "H1lJ9-Pdn7", "sid": 3, "sentence": "My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space."}, {"text_id": "H1lJ9-Pdn7", "sid": 4, "sentence": "I understand that for two matrices A and B we have rank(AB) <= min(rank(A), rank(B)), and we are not making the small-sized embeddings richer when backprojecting to R^d, but"}, {"text_id": "H1lJ9-Pdn7", "sid": 5, "sentence": "have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?"}, {"text_id": "H1lJ9-Pdn7", "sid": 6, "sentence": "References"}, {"text_id": "H1lJ9-Pdn7", "sid": 7, "sentence": "Joulin, A., Ciss\u00e9, M., Grangier, D. and J\u00e9gou, H., 2017, July. Efficient softmax approximation for GPUs. In International Conference on Machine Learning (pp. 1302-1310)."}], "reviewlabels": [{"text_id": "H1lJ9-Pdn7", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lJ9-Pdn7", "sid": 1, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lJ9-Pdn7", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lJ9-Pdn7", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lJ9-Pdn7", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lJ9-Pdn7", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lJ9-Pdn7", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1lJ9-Pdn7", "sid": 7, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "SJgnkDkMCm", "sid": 0, "sentence": "The primary goal of the projections is to project all embeddings into the model dimension d so that we can have variable sized embeddings."}, {"text_id": "SJgnkDkMCm", "sid": 1, "sentence": "Our goal was not to make the model model expressive."}, {"text_id": "SJgnkDkMCm", "sid": 2, "sentence": "Compared to the rest of the model, these projections add very little overhead compared to the rest of the model."}, {"text_id": "SJgnkDkMCm", "sid": 3, "sentence": "Doing without them is an interesting future direction though!"}], "rebuttallabels": [{"labels": {"alignments": [3], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "SJgnkDkMCm", "sid": 0}, {"labels": {"alignments": [3], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "SJgnkDkMCm", "sid": 1}, {"labels": {"alignments": [3], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "SJgnkDkMCm", "sid": 2}, {"labels": {"alignments": [5], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "SJgnkDkMCm", "sid": 3}], "metadata": {"anno": "anno14", "review": "H1lJ9-Pdn7", "rebuttal": "SJgnkDkMCm", "conference": "ICLR2019", "title": "Adaptive Input Representations for Neural Language Modeling", "reviewer": "AnonReviewer3", "forum_id": "ByxZX20qFQ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}