{"review": [{"text_id": "rylsXynatr", "sid": 0, "sentence": "Overview: This work is an interesting work to understand the generalization capabilities of a two layered neural network in a high dimensional setting (samples, features and neurons tend to infinity)."}, {"text_id": "rylsXynatr", "sid": 1, "sentence": "It studies the conditions under which the \"double descent phenomenon\" may be observed."}, {"text_id": "rylsXynatr", "sid": 2, "sentence": "Summary: The work shows"}, {"text_id": "rylsXynatr", "sid": 3, "sentence": "that in two layered neural networks with non-linearity"}, {"text_id": "rylsXynatr", "sid": 4, "sentence": "1) the double descent phenomenon of the bias-variance decomposition may be observed when the second layer weights are optimized assuming that the first layer weights are constant."}, {"text_id": "rylsXynatr", "sid": 5, "sentence": "2) the bias-variance decomposition does not exhibit double descent when optimizing only the first layer with both vanishing and non-vanishing initialization of weights."}, {"text_id": "rylsXynatr", "sid": 6, "sentence": "3) For vanishing initalization of weights for the first layer with non-linear activation , the gradient flow solution is asymptotically close to a two layered linear network."}, {"text_id": "rylsXynatr", "sid": 7, "sentence": "It is independent of overparametrization."}, {"text_id": "rylsXynatr", "sid": 8, "sentence": "However, the condition for this is smooth activation and the result does not hold for ReLU activation."}, {"text_id": "rylsXynatr", "sid": 9, "sentence": "4) For non-vanishing initilization of the weights for the first layer with non-linear activation, the gradient flow solution is well approximated by a kernel model."}, {"text_id": "rylsXynatr", "sid": 10, "sentence": "However, the risk is independent of overparametrization."}, {"text_id": "rylsXynatr", "sid": 11, "sentence": "I believe this is an interesting work that needs to be accepted."}], "reviewlabels": [{"text_id": "rylsXynatr", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylsXynatr", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylsXynatr", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "rylsXynatr", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "rylsXynatr", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylsXynatr", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylsXynatr", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylsXynatr", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylsXynatr", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylsXynatr", "sid": 9, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylsXynatr", "sid": 10, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rylsXynatr", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "Hkee-HRoiH", "sid": 0, "sentence": "Thank you for the comments and suggestions."}, {"text_id": "Hkee-HRoiH", "sid": 1, "sentence": "As you pointed out, our current result in Section 5 does not apply to non-smooth activations -- understanding the generalization of ReLU networks would be interesting future work."}, {"text_id": "Hkee-HRoiH", "sid": 2, "sentence": "We have updated the manuscript with a few minor modifications: 1) Figure on the population risk of sigmoid network (first layer optimized) in addition to SoftPlus; 2) additional remarks on the population risk of network in the kernel regime in Section 5.2; 3) corrected typos."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Hkee-HRoiH", "sid": 0}, {"labels": {"alignments": [6, 7, 8], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "Hkee-HRoiH", "sid": 1}, {"labels": {"alignments": [], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "Hkee-HRoiH", "sid": 2}], "metadata": {"anno": "anno10", "review": "rylsXynatr", "rebuttal": "Hkee-HRoiH", "conference": "ICLR2020", "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "reviewer": "AnonReviewer1", "forum_id": "H1gBsgBYwH", "rating": "8: Accept", "experience_assessment": "I have read many papers in this area."}}