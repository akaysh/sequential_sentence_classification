{"review": [{"text_id": "B1gFWGts5S", "sid": 0, "sentence": "The paper proposed a new pipelined training strategy to fully utilize the memory and computational power to speed up the training process."}, {"text_id": "B1gFWGts5S", "sid": 1, "sentence": "In order to overcome the generalization degradation of the proposed method, the authors further introduced the so-called hybrid method to combine their proposed pipelined method and normal training."}, {"text_id": "B1gFWGts5S", "sid": 2, "sentence": "The pipelined method is interesting."}, {"text_id": "B1gFWGts5S", "sid": 3, "sentence": "For the pipelined process itself, it is similar to model parallelization."}, {"text_id": "B1gFWGts5S", "sid": 4, "sentence": "For the method proposed by the paper,  it is like the async-SGD method."}, {"text_id": "B1gFWGts5S", "sid": 5, "sentence": "The paper merged these two ideas together but did not solve the problem from async-SGD, i.e. with a large number of processes, the generalization performance degrades (in the paper, it is so-called \"stages\")."}, {"text_id": "B1gFWGts5S", "sid": 6, "sentence": "Even with the hybrid method, the accuracy still drops."}, {"text_id": "B1gFWGts5S", "sid": 7, "sentence": "Also, the sentence, \"We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy.\", is confusing. If I use data parallelization, the gain should be also around 2."}, {"text_id": "B1gFWGts5S", "sid": 8, "sentence": "The ResNet on Cifar-10 results are not convincing."}, {"text_id": "B1gFWGts5S", "sid": 9, "sentence": "The normal accuracy of ResNet20 on Cifar-10 is around 92 but the paper reported 91.1%."}, {"text_id": "B1gFWGts5S", "sid": 10, "sentence": "Based on this, I think the paper has some room for improvement."}], "reviewlabels": [{"text_id": "B1gFWGts5S", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1gFWGts5S", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1gFWGts5S", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1gFWGts5S", "sid": 3, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1gFWGts5S", "sid": 4, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1gFWGts5S", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1gFWGts5S", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1gFWGts5S", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1gFWGts5S", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "B1gFWGts5S", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "B1gFWGts5S", "sid": 10, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "H1xO7NHvjr", "sid": 0, "sentence": "Pipelined backpropagation is similar to model parallelism but it addresses the resource underutilization issue in model parallelism."}, {"text_id": "H1xO7NHvjr", "sid": 1, "sentence": "Our pipelined method might look like async-SGD on surface."}, {"text_id": "H1xO7NHvjr", "sid": 2, "sentence": "However, async-SGD (e.g. Dean et al., pointed out by Reviewer 4) utilizes data parallelism (as indicated in Dean el al.) and a parameter server to keep track of model parameters (weights)."}, {"text_id": "H1xO7NHvjr", "sid": 3, "sentence": "In contrast, our pipelined method does not use any parameter server."}, {"text_id": "H1xO7NHvjr", "sid": 4, "sentence": "Furthermore, each accelerator obtains a replica of a full model in asycn-SGD training while each accelerator contains only a part of the model in our pipelined method, on the assumption that the full model does not fit into the memory of a single accelerator."}, {"text_id": "H1xO7NHvjr", "sid": 5, "sentence": "The accuracy drops for some models in a pure pipelined training."}, {"text_id": "H1xO7NHvjr", "sid": 6, "sentence": "However, hybrid training is able to bring the accuracy of most networks studied in our paper up to a comparable level of the non-pipelined baseline as shown in the evaluation section of our paper."}, {"text_id": "H1xO7NHvjr", "sid": 7, "sentence": "Our pipelined method is different from data parallelism in the following way (for a 2-GPU example)."}, {"text_id": "H1xO7NHvjr", "sid": 8, "sentence": "For data parallelism, a model is duplicated and placed onto 2 GPUs, each GPU containing a full copy of the model."}, {"text_id": "H1xO7NHvjr", "sid": 9, "sentence": "On the other hand, for pipelined parallelism, a model is divided into two partitions (on the assumption that it cannot fit in a single device): one is mapped onto GPU 0 while the other is mapped onto GPU 1, each GPU obtaining only a part of the model."}, {"text_id": "H1xO7NHvjr", "sid": 10, "sentence": "Communication between these two partitions is necessary to enable activation and gradient transfers."}, {"text_id": "H1xO7NHvjr", "sid": 11, "sentence": "Regardless of the parallelization techniques, the maximum speedup of a 2-GPU system is 2X compared to a 1-GPU system."}, {"text_id": "H1xO7NHvjr", "sid": 12, "sentence": "To obtain a close to perfect speedup of 2X, the communication overhead must be almost non-existent and the workload needs to be perfectly balanced between the 2 GPUs."}, {"text_id": "H1xO7NHvjr", "sid": 13, "sentence": "In our implementation, we obtained a speedup of 1.81X for ResNet-362, which is equivalent to 90% utilization of each GPU."}, {"text_id": "H1xO7NHvjr", "sid": 14, "sentence": "Thus, our sentence the reviewer refers to."}, {"text_id": "H1xO7NHvjr", "sid": 15, "sentence": "Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1)."}, {"text_id": "H1xO7NHvjr", "sid": 16, "sentence": "Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study."}, {"text_id": "H1xO7NHvjr", "sid": 17, "sentence": "It is the trend of the decline in inference accuracy with pipelining is what we study."}, {"text_id": "H1xO7NHvjr", "sid": 18, "sentence": "This trend exists with both our hyperparameters and those at, for example, https://github.com/akamaster/pytorch_resnet_cifar10."}, {"text_id": "H1xO7NHvjr", "sid": 19, "sentence": "The use of these set of hyperparameters, obtains an inference accuracy of 91.65% (better than the accuracy stated in the original ResNet paper) for ResNet-20 non-pipelined baseline and 91.21% for pipelined version."}, {"text_id": "H1xO7NHvjr", "sid": 20, "sentence": "We are not aware of any reports of an accuracy of ResNet-20 at 92% (perhaps this is approximate)."}, {"text_id": "H1xO7NHvjr", "sid": 21, "sentence": "Please kindly let us know a pointer."}, {"text_id": "H1xO7NHvjr", "sid": 22, "sentence": "It is relatively easy to update our results in the paper with new hyperparameters."}], "rebuttallabels": [{"labels": {"alignments": [3], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1xO7NHvjr", "sid": 0}, {"labels": {"alignments": [4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1xO7NHvjr", "sid": 1}, {"labels": {"alignments": [4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1xO7NHvjr", "sid": 2}, {"labels": {"alignments": [5, 6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1xO7NHvjr", "sid": 3}, {"labels": {"alignments": [3, 4, 5, 6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1xO7NHvjr", "sid": 4}, {"labels": {"alignments": [3, 4, 5, 6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 5}, {"labels": {"alignments": [3, 4, 5, 6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 6}, {"labels": {"alignments": [3, 4, 5, 6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 7}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 8}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 9}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 10}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 11}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 12}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 13}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 14}, {"labels": {"alignments": [8, 9], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "H1xO7NHvjr", "sid": 15}, {"labels": {"alignments": [8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 16}, {"labels": {"alignments": [8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 17}, {"labels": {"alignments": [8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 18}, {"labels": {"alignments": [8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xO7NHvjr", "sid": 19}, {"labels": {"alignments": [8, 9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1xO7NHvjr", "sid": 20}, {"labels": {"alignments": [8, 9], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "H1xO7NHvjr", "sid": 21}, {"labels": {"alignments": [8, 9], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "H1xO7NHvjr", "sid": 22}], "metadata": {"anno": "anno3", "review": "B1gFWGts5S", "rebuttal": "H1xO7NHvjr", "conference": "ICLR2020", "title": "Pipelined Training with Stale Weights of Deep Convolutional Neural Networks", "reviewer": "AnonReviewer2", "forum_id": "SkgTR3VFvH", "rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area."}}