{"review": [{"text_id": "BJlCnxODhX", "sid": 0, "sentence": "This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function."}, {"text_id": "BJlCnxODhX", "sid": 1, "sentence": "They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost."}, {"text_id": "BJlCnxODhX", "sid": 2, "sentence": "Their experimental results showed competitive performance to SGD/Adam on the same network architectures."}, {"text_id": "BJlCnxODhX", "sid": 3, "sentence": "1."}, {"text_id": "BJlCnxODhX", "sid": 4, "sentence": "Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function."}, {"text_id": "BJlCnxODhX", "sid": 5, "sentence": "While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case."}, {"text_id": "BJlCnxODhX", "sid": 6, "sentence": "An appendix with more numerical comparisons on other loss functions might also be insightful."}, {"text_id": "BJlCnxODhX", "sid": 7, "sentence": "2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2)."}, {"text_id": "BJlCnxODhX", "sid": 8, "sentence": "In Table 2 I saw some optimizers end up with much lower test accuracy."}, {"text_id": "BJlCnxODhX", "sid": 9, "sentence": "Can the authors show the convergence plots of these methods (similar to Figure 2)?"}], "reviewlabels": [{"text_id": "BJlCnxODhX", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlCnxODhX", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlCnxODhX", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlCnxODhX", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlCnxODhX", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlCnxODhX", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlCnxODhX", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlCnxODhX", "sid": 7, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlCnxODhX", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJlCnxODhX", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "SJebCbtF67", "sid": 0, "sentence": "We thank the reviewer for their comments and suggestions."}, {"text_id": "SJebCbtF67", "sid": 1, "sentence": "We answer below:"}, {"text_id": "SJebCbtF67", "sid": 2, "sentence": "1. As the reviewer accurately points out, we choose to always employ the hinge loss for DFW in this paper because it gives an optimal step-size."}, {"text_id": "SJebCbtF67", "sid": 3, "sentence": "In the new version of the paper, we have included additional baselines on the SNLI data set."}, {"text_id": "SJebCbtF67", "sid": 4, "sentence": "This provides more empirical comparisons between the performance of CE and SVM for different optimizers."}, {"text_id": "SJebCbtF67", "sid": 5, "sentence": "2. In appendix B.2 of the paper, we have added the convergence plot for all methods on the CIFAR data sets."}, {"text_id": "SJebCbtF67", "sid": 6, "sentence": "In some cases the training performance can show some oscillations."}, {"text_id": "SJebCbtF67", "sid": 7, "sentence": "We emphasize that this is the result of cross-validating the initial learning rate based on the validation set performance: sometimes a better behaved convergence would be obtained on the training set with a lower learning rate."}, {"text_id": "SJebCbtF67", "sid": 8, "sentence": "However this lower learning rate is not selected because it does not provide the best validation performance (this is consistent with our discussion on the step size in section 6)."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SJebCbtF67", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SJebCbtF67", "sid": 1}, {"labels": {"alignments": [4, 5, 6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJebCbtF67", "sid": 2}, {"labels": {"alignments": [4, 5, 6], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SJebCbtF67", "sid": 3}, {"labels": {"alignments": [4, 5, 6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJebCbtF67", "sid": 4}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJebCbtF67", "sid": 5}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJebCbtF67", "sid": 6}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJebCbtF67", "sid": 7}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJebCbtF67", "sid": 8}], "metadata": {"anno": "anno0", "review": "BJlCnxODhX", "rebuttal": "SJebCbtF67", "conference": "ICLR2019", "title": "Deep Frank-Wolfe For Neural Network Optimization", "reviewer": "AnonReviewer3", "forum_id": "SyVU6s05K7", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}