{"review": [{"text_id": "rklaWryJoH", "sid": 0, "sentence": "The suggested method proposes a technique to compress neural networks bases on PQ quantization."}, {"text_id": "rklaWryJoH", "sid": 1, "sentence": "The algorithm quantizes matrices of linear operations, and, by generalization, also works on convolutional networks."}, {"text_id": "rklaWryJoH", "sid": 2, "sentence": "Rather than trying to compress weights (i.e. to minimize distance between original and quantized weights), the algorithm considers a distribution of unlabeled inputs and looks for such quantization which would affect output activations as little as possible over that distribution of data."}, {"text_id": "rklaWryJoH", "sid": 3, "sentence": "The algorithm works by splitting each column of W_ij into m equal subvectors, learning a codebook for those subvectors, and encoding each of those subvectors as one of the words from the codebook."}, {"text_id": "rklaWryJoH", "sid": 4, "sentence": "The method provides impressive compression ratios (in the order of x20-30) but at the cost of a lower performance."}, {"text_id": "rklaWryJoH", "sid": 5, "sentence": "Whether this is a valuable trade-off is highly application dependent."}, {"text_id": "rklaWryJoH", "sid": 6, "sentence": "Overall I find the paper interesting and enjoyable."}, {"text_id": "rklaWryJoH", "sid": 7, "sentence": "However, as I am not an expert in the research area, I can not assess how state of the art the suggested method is."}, {"text_id": "rklaWryJoH", "sid": 8, "sentence": "There are a few other questions that I think would be nice to answer. I will try to describe them below:"}, {"text_id": "rklaWryJoH", "sid": 9, "sentence": "Suppose we have a matric W_{ij} with dimensions NxM where changing i for a given j defines a column."}, {"text_id": "rklaWryJoH", "sid": 10, "sentence": "By definition, linear operation is defined"}, {"text_id": "rklaWryJoH", "sid": 11, "sentence": "y_i = sum_j W_ij x_j ."}, {"text_id": "rklaWryJoH", "sid": 12, "sentence": "Now say each column of matrix W is quantized into m subvectors."}, {"text_id": "rklaWryJoH", "sid": 13, "sentence": "We can express W_ij in the following way:"}, {"text_id": "rklaWryJoH", "sid": 14, "sentence": "W_ij = (V^1_ij + V^2_ij + ... V^m_ij)x_j where V^m_ij is zero everywhere except for the rows covering a given quantized vector."}, {"text_id": "rklaWryJoH", "sid": 15, "sentence": "For example, if W had dimensions of 8x16 and m=4,"}, {"text_id": "rklaWryJoH", "sid": 16, "sentence": "V^2_{3,j}=0, for all j"}, {"text_id": "rklaWryJoH", "sid": 17, "sentence": ", V^2_{4,j}=non_zero, V^2_{7,j}=non_zero, V^2_{8,j}=0, V^2_{i=4:8,j}=one_of_the_quantized_vectors."}, {"text_id": "rklaWryJoH", "sid": 18, "sentence": "y_i = sum_j W_ij x_j = sum_k sum_j (V^k_ij) x_j =def= sum_k z^k_i where z^k are partial products: z^k_i=0 for i<k*N/m and i>(k+1)N/m"}, {"text_id": "rklaWryJoH", "sid": 19, "sentence": "Thus, the suggested solution effectively splits the output vector y_i into m sections, defines sparse matrices V^k_{ij} 1<=k<=m, and performs column-wise vector quantization for these matrices separately."}, {"text_id": "rklaWryJoH", "sid": 20, "sentence": "Generally, it is not ovious or given that the current method would be able to compress general matrices well, as it implicitly assumes that weight W_{ij} has a high \"correlation\" with weights W_{i+kN/m,j} (which I call \"vertical\" correlation), W_{i,k+some_number} (which I call \"horizontal\" correlation) and W_{i+kN/m,k+some_number} (which I call \"other\" correlation)."}, {"text_id": "rklaWryJoH", "sid": 21, "sentence": "It is not given that those kind of redundancies would exist in arbitrary weight matrices."}, {"text_id": "rklaWryJoH", "sid": 22, "sentence": "Naturally, the method will work well when weight matrices have a lot of structure and then quantized vectors can be reused."}, {"text_id": "rklaWryJoH", "sid": 23, "sentence": "Matrices can have either \"horizontal\" or \"vertical\" redundancy (or \"other\" or neither)."}, {"text_id": "rklaWryJoH", "sid": 24, "sentence": "It would be very interesting to see which kind of redundancy their method managed to caprture."}, {"text_id": "rklaWryJoH", "sid": 25, "sentence": "In the 'horizontal' case, it should work well when inputs have a lot of redundancy (say x_j' and x_j'' are highly correlated making it possible to reuse code-words horizontally within any given V^k: V^k_ij'=V^k_ij'')."}, {"text_id": "rklaWryJoH", "sid": 26, "sentence": "However, if thise was the case, it would make more sense to simply remove redundancy by prunning input vector x_j by removing either x_j' or x_j'' from it."}, {"text_id": "rklaWryJoH", "sid": 27, "sentence": "This can be dome by removing one of the outputs from the previous layer."}, {"text_id": "rklaWryJoH", "sid": 28, "sentence": "This can be a symptom of a redundant input."}, {"text_id": "rklaWryJoH", "sid": 29, "sentence": "Another option is exploiting \"vertical\" redundancy: this happens when output y_i' is correlated with output y_{i'+N/m}. This allows the same code-word to be reused vertically."}, {"text_id": "rklaWryJoH", "sid": 30, "sentence": "This can be a symptom of a redundant output."}, {"text_id": "rklaWryJoH", "sid": 31, "sentence": "It could also be the case that compressibility could be further subtantially improved by trying different matrix row permutations."}, {"text_id": "rklaWryJoH", "sid": 32, "sentence": "Also, if one notices that y_i' ir correlated with y_i'', it might make sense to permute matrix rows in such a way that both rows would end up a multiple N/m apart."}, {"text_id": "rklaWryJoH", "sid": 33, "sentence": "It would be interesting to see how this would affect compressibility."}, {"text_id": "rklaWryJoH", "sid": 34, "sentence": "The third case is when code words are reused in arbitrary cases."}, {"text_id": "rklaWryJoH", "sid": 35, "sentence": "Generally, I think that answering the following questions would be interesting and could guide further research:"}, {"text_id": "rklaWryJoH", "sid": 36, "sentence": "1. It would be very interesting to know what kind of code-word reusa patterns the algorithm was able to capture, as this may guide further research."}, {"text_id": "rklaWryJoH", "sid": 37, "sentence": "2. How invariance copressibility is under random permutations of matrix rows (thus also output vectors)?"}], "reviewlabels": [{"text_id": "rklaWryJoH", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 7, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 8, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 17, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 18, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 19, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 20, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 21, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 22, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 23, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 24, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 25, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 26, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 27, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 28, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 29, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 30, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 31, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 32, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 33, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 34, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Replicability", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 35, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 36, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rklaWryJoH", "sid": 37, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rkxdLt17jr", "sid": 0, "sentence": "We thank Reviewer 1 for their insightful questions and suggestions."}, {"text_id": "rkxdLt17jr", "sid": 1, "sentence": "We agree that Product Quantization (PQ) is key to get \u201cimpressive compression ratio\u201d while maintaining competitive accuracy, provided that there is some special structure and redundancy in the weights and the way we quantize them."}, {"text_id": "rkxdLt17jr", "sid": 2, "sentence": "Which kind of redundancy does our method capture?"}, {"text_id": "rkxdLt17jr", "sid": 3, "sentence": "As rightfully stated by Reviewer 1, choosing which elementary blocks to quantize in the weight matrices is crucial for the success of the method (what the Reviewer calls \u201chorizontal/vertical/other\u201d correlation)"}, {"text_id": "rkxdLt17jr", "sid": 4, "sentence": "."}, {"text_id": "rkxdLt17jr", "sid": 5, "sentence": "In what follows, let us focus on the case of convolutional weights (of size C_out x C_in x K x K)."}, {"text_id": "rkxdLt17jr", "sid": 6, "sentence": "As we state in our paper: \u201cThere are many ways to split a 4D matrix in a set of vectors and we are aiming for one that maximizes the correlation between the vectors since vector quantization-based methods work the best when the vectors are highly correlated"}, {"text_id": "rkxdLt17jr", "sid": 7, "sentence": "\u201d"}, {"text_id": "rkxdLt17jr", "sid": 8, "sentence": "."}, {"text_id": "rkxdLt17jr", "sid": 9, "sentence": "We build on previous work that have documented the *spatial redundancy* in the convolutional filters [1], hence we use blocks of size K x K. Therefore, we rely on the particular nature of convolutional filters to exploit their spatial redundancy."}, {"text_id": "rkxdLt17jr", "sid": 10, "sentence": "We have tried other ways to split the 4D weights into a set of vectors to in preliminary experiments, but none was on par with the proposed choice."}, {"text_id": "rkxdLt17jr", "sid": 11, "sentence": "We agree with Reviewer 1 that the method would probably not yield as good a performance for arbitrary matrices."}, {"text_id": "rkxdLt17jr", "sid": 12, "sentence": "Using row permutations to improve the compressibility?"}, {"text_id": "rkxdLt17jr", "sid": 13, "sentence": "This is a very good remark."}, {"text_id": "rkxdLt17jr", "sid": 14, "sentence": "Indeed, redundancy can be artificially created by finding the *right* permutation of rows (when we quantize using column blocks for a 2D matrix)."}, {"text_id": "rkxdLt17jr", "sid": 15, "sentence": "Yet in our preliminary experiments, we observed that PQ performs systematically worse both in terms of reconstruction error and accuracy of the network that when applying a random permutation to a convolutional filter."}, {"text_id": "rkxdLt17jr", "sid": 16, "sentence": "This confirms that our method captures the spatial redundancy of the convolutional filters as stated in the first point."}, {"text_id": "rkxdLt17jr", "sid": 17, "sentence": "[1] Exploiting linear structure within convolutional networks for efficient evaluation, Denton et al."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rkxdLt17jr", "sid": 0}, {"labels": {"alignments": [0, 1, 2, 3], "responsetype": "accept-praise", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 1}, {"labels": {"alignments": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 2}, {"labels": {"alignments": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 3}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "rkxdLt17jr", "sid": 4}, {"labels": {"alignments": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 5}, {"labels": {"alignments": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 6}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "rkxdLt17jr", "sid": 7}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "rkxdLt17jr", "sid": 8}, {"labels": {"alignments": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 9}, {"labels": {"alignments": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 10}, {"labels": {"alignments": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 11}, {"labels": {"alignments": [29, 30, 31, 32, 33], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 12}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rkxdLt17jr", "sid": 13}, {"labels": {"alignments": [29, 30, 31, 32, 33], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 14}, {"labels": {"alignments": [29, 30, 31, 32, 33], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 15}, {"labels": {"alignments": [29, 30, 31, 32, 33], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rkxdLt17jr", "sid": 16}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rkxdLt17jr", "sid": 17}], "metadata": {"anno": "anno12", "review": "rklaWryJoH", "rebuttal": "rkxdLt17jr", "conference": "ICLR2020", "title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks", "reviewer": "AnonReviewer1", "forum_id": "rJehVyrKwH", "rating": "8: Accept", "experience_assessment": "I do not know much about this area."}}