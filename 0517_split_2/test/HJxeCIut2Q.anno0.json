{"review": [{"text_id": "HJxeCIut2Q", "sid": 0, "sentence": "This paper introduced a new architecture for input embeddings of neural language models: adaptive input representation (ADP)."}, {"text_id": "HJxeCIut2Q", "sid": 1, "sentence": "ADP allowed a model builder to define a set of bands of input words with different frequency where frequent words have larger embedding size than the others."}, {"text_id": "HJxeCIut2Q", "sid": 2, "sentence": "The embeddings of each band are then projected into the same size."}, {"text_id": "HJxeCIut2Q", "sid": 3, "sentence": "This resulted in lowering the number of parameters."}, {"text_id": "HJxeCIut2Q", "sid": 4, "sentence": "Extensive experiments with the Transformer LM on WikiText-103 and Billion Word corpus showed that ADP achieved competitive perplexities."}, {"text_id": "HJxeCIut2Q", "sid": 5, "sentence": "While tying weight with the output did not benefit the perplexity, it lowered the runtime significantly on Billion Word corpus."}, {"text_id": "HJxeCIut2Q", "sid": 6, "sentence": "Further analyses showed that ADP gained performance across all word frequency ranges."}, {"text_id": "HJxeCIut2Q", "sid": 7, "sentence": "Overall, the paper was well-written and the experiments supported the claim."}, {"text_id": "HJxeCIut2Q", "sid": 8, "sentence": "The paper was very clear on its contribution."}, {"text_id": "HJxeCIut2Q", "sid": 9, "sentence": "The variable-size input of this paper was novel as far as I know."}, {"text_id": "HJxeCIut2Q", "sid": 10, "sentence": "However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax."}, {"text_id": "HJxeCIut2Q", "sid": 11, "sentence": "The weight sharing was also needed further investigation and experimental data on sharing different parts."}, {"text_id": "HJxeCIut2Q", "sid": 12, "sentence": "The experiments compared several models with different input levels (characters, BPE, and words)."}, {"text_id": "HJxeCIut2Q", "sid": 13, "sentence": "The perplexities of the proposed approach were competitive with the character model with an advantage on the training time."}, {"text_id": "HJxeCIut2Q", "sid": 14, "sentence": "However, the runtimes were a bit strange."}, {"text_id": "HJxeCIut2Q", "sid": 15, "sentence": "For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4)."}, {"text_id": "HJxeCIut2Q", "sid": 16, "sentence": "The runtime of ADP seemed to lose in term of scaling as well to BPE."}, {"text_id": "HJxeCIut2Q", "sid": 17, "sentence": "Perhaps, the training time was an artifact of multi-GPU training."}, {"text_id": "HJxeCIut2Q", "sid": 18, "sentence": "Questions:"}, {"text_id": "HJxeCIut2Q", "sid": 19, "sentence": "1. I am curious about what would you get if you use ADP on BPE vocab set?"}, {"text_id": "HJxeCIut2Q", "sid": 20, "sentence": "2. How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?"}], "reviewlabels": [{"text_id": "HJxeCIut2Q", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [{"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 12, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 17, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 18, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 19, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxeCIut2Q", "sid": 20, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "ryloqL1G0Q", "sid": 0, "sentence": "We thank the reviewer for the comments!"}, {"text_id": "ryloqL1G0Q", "sid": 1, "sentence": "Q: \u201cADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4)\u201d"}, {"text_id": "ryloqL1G0Q", "sid": 2, "sentence": "The differences in training time are due to the size of the models: Weight tying saves a lot more parameters for the Billion Word model due to the larger vocab compared to the WikiText-103 models which have a smaller vocab."}, {"text_id": "ryloqL1G0Q", "sid": 3, "sentence": "On WikiText-103, tying saves 15% of parameters (Table 3, ADP vs ADP-T, 291M vs 247M) and training time is reduced by about 13%."}, {"text_id": "ryloqL1G0Q", "sid": 4, "sentence": "On Billion Word, tying saves 27% of parameters (Table 4) and training time is reduced by about 34%."}, {"text_id": "ryloqL1G0Q", "sid": 5, "sentence": "The slight discrepancy may be due to multi-machine training for Billion Word compared to the single machine setup for WikiText-103."}, {"text_id": "ryloqL1G0Q", "sid": 6, "sentence": "Q1: \"I am curious about what would you get if you use ADP on BPE vocab set?\""}, {"text_id": "ryloqL1G0Q", "sid": 7, "sentence": "We tried adaptive input embeddings with BPE but the results were worse than softmax."}, {"text_id": "ryloqL1G0Q", "sid": 8, "sentence": "This is likely because 'rare' BPE units are in some sense not rare enough compared to a word vocabulary."}, {"text_id": "ryloqL1G0Q", "sid": 9, "sentence": "In that case, the regularization effect of assigning less capacity to 'rare' BPE tokens through adaptive input embeddings is actually harmful."}, {"text_id": "ryloqL1G0Q", "sid": 10, "sentence": "Q2: \"How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?\""}, {"text_id": "ryloqL1G0Q", "sid": 11, "sentence": "For WikiText-103 (Table 3) we measured 24.92 on test with a full softmax model (a 5.2 PPL improvement over the previous SOTA)."}, {"text_id": "ryloqL1G0Q", "sid": 12, "sentence": "This corresponds to a Transformer model including our tuned optimization scheme."}, {"text_id": "ryloqL1G0Q", "sid": 13, "sentence": "Adding tied adaptive input embeddings (ADP-T) to this configuration reduces this perplexity to 20.51, which is another reduction of 4.4 PPL."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "ryloqL1G0Q", "sid": 0}, {"labels": {"alignments": [15], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryloqL1G0Q", "sid": 1}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 2}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 3}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 4}, {"labels": {"alignments": [15], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 5}, {"labels": {"alignments": [19], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryloqL1G0Q", "sid": 6}, {"labels": {"alignments": [19], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 7}, {"labels": {"alignments": [19], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 8}, {"labels": {"alignments": [19], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 9}, {"labels": {"alignments": [20], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryloqL1G0Q", "sid": 10}, {"labels": {"alignments": [20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 11}, {"labels": {"alignments": [20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 12}, {"labels": {"alignments": [20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "ryloqL1G0Q", "sid": 13}], "metadata": {"anno": "anno0", "review": "HJxeCIut2Q", "rebuttal": "ryloqL1G0Q", "conference": "ICLR2019", "title": "Adaptive Input Representations for Neural Language Modeling", "reviewer": "AnonReviewer2", "forum_id": "ByxZX20qFQ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}