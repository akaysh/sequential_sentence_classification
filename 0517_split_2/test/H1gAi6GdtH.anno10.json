{"review": [{"text_id": "H1gAi6GdtH", "sid": 0, "sentence": "This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors."}, {"text_id": "H1gAi6GdtH", "sid": 1, "sentence": "The authors aim to reduce the distortion of each layer rather than the weight distortion."}, {"text_id": "H1gAi6GdtH", "sid": 2, "sentence": "The proposed algorithm first selects the candidate codeword vectors using k-means clustering and fine-tune them via knowledge distillation."}, {"text_id": "H1gAi6GdtH", "sid": 3, "sentence": "The authors verify the proposed algorithm by comparing it with existing algorithms for ResNet-18 and ResNet-50."}, {"text_id": "H1gAi6GdtH", "sid": 4, "sentence": "Overall, I think that the proposed algorithm is easy to apply and the draft is relatively well written."}, {"text_id": "H1gAi6GdtH", "sid": 5, "sentence": "Some questions and doubts are listed below."}, {"text_id": "H1gAi6GdtH", "sid": 6, "sentence": "-In k-means clustering (E-step and M-step), is it correct to multiply \\tilde x to (c-v)?"}, {"text_id": "H1gAi6GdtH", "sid": 7, "sentence": "I think that the error arising from quantizing v into c is only affected by a subset of rows of \\tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, \u2026 rows of \\tilde x affect to the error."}, {"text_id": "H1gAi6GdtH", "sid": 8, "sentence": "-Does minimizing reconstruction error minimizes the training loss (before any further fine-tuning) compared to na\u00efve PQ? If not,"}, {"text_id": "H1gAi6GdtH", "sid": 9, "sentence": "-Is there any guideline for choosing the optimal number of centroids and the optimal block size given a target compression rate?"}, {"text_id": "H1gAi6GdtH", "sid": 10, "sentence": "-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)"}], "reviewlabels": [{"text_id": "H1gAi6GdtH", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "P-Positive"}, "secondarylabels": [{"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 6, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "H1gAi6GdtH", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "B1ezIjyQiH", "sid": 0, "sentence": "We thank Reviewer 3 for raising important questions."}, {"text_id": "B1ezIjyQiH", "sid": 1, "sentence": "We answer them below."}, {"text_id": "B1ezIjyQiH", "sid": 2, "sentence": "Using \\tilde x in the E- and M-steps."}, {"text_id": "B1ezIjyQiH", "sid": 3, "sentence": "We agree with Reviewer 3 that \u201cthe error arising from quantizing v into c is only affected by a subset of rows of \\tilde x\u201d."}, {"text_id": "B1ezIjyQiH", "sid": 4, "sentence": "However, we solve Equation (2) with this proxy algorithm for two reasons."}, {"text_id": "B1ezIjyQiH", "sid": 5, "sentence": "First, using the full \\tilde x matrix allows to factor the computation of the pseudo-inverse of \\tilde x and thus allows for a much faster algorithm, see answer to Reviewer 2 and the details of the M-step in the paper (as well as footnote 2)."}, {"text_id": "B1ezIjyQiH", "sid": 6, "sentence": "Second, early (and slow) experiments suggested that the gains were not significant when using the right subsets of \\tilde x in this particular context."}, {"text_id": "B1ezIjyQiH", "sid": 7, "sentence": "Minimizing the reconstruction error"}, {"text_id": "B1ezIjyQiH", "sid": 8, "sentence": "Our method results in both better reconstruction error and better training loss than na\u00efve PQ *before* any finetuning."}, {"text_id": "B1ezIjyQiH", "sid": 9, "sentence": "As we state in the paper, applying naive PQ without any finetuning to a ResNet-18 leads to accuracies below 18% for all operating points, whereas our method (without any finetuning) gives accuracy around 50% (not reported in the paper, we will add it in the next version of our paper)."}, {"text_id": "B1ezIjyQiH", "sid": 10, "sentence": "Choosing the optimal number of centroids/blocks size"}, {"text_id": "B1ezIjyQiH", "sid": 11, "sentence": "There is some rationale for the block size, related to the way the information is structured and redundant in the weight matrices (see in particular point 1 of answer to Reviewer 1)."}, {"text_id": "B1ezIjyQiH", "sid": 12, "sentence": "For instance, for convolutional weight filters with a kernel size of 3x3, the natural block size is 9, as we wish to exploit the spatial redundancy in the convolutional filters."}, {"text_id": "B1ezIjyQiH", "sid": 13, "sentence": "For the fully-connected classifier matrices and 1x1 convolutions however, the only constraint on the block size if to be a divisor of the column size."}, {"text_id": "B1ezIjyQiH", "sid": 14, "sentence": "Early experiments when trying to quantize such matrices in the row or column direction gave similar results."}, {"text_id": "B1ezIjyQiH", "sid": 15, "sentence": "Regarding the number of centroids, we expect byte-aligned schemes (256 centroids indexed over 1 byte) to be more friendly for an efficient implementation of the forward in the compressed domain."}, {"text_id": "B1ezIjyQiH", "sid": 16, "sentence": "Otherwise, as can be seen in Figure 3, doubling the number of centroids results in better performance, even if the curve tends to saturate around k=2048 centroids."}, {"text_id": "B1ezIjyQiH", "sid": 17, "sentence": "As a side note, there exists some strategies that automatically adjust for those two parameters (see HAQ for example)."}, {"text_id": "B1ezIjyQiH", "sid": 18, "sentence": "Comparison with pruning and low-rank approximation"}, {"text_id": "B1ezIjyQiH", "sid": 19, "sentence": "We argue that both pruning and low-rank approximation are orthogonal and complementary approaches to our method, akin to what happens in image compression where the transform stage (e.g., DCT or wavelet) is complementary with quantization. See \u201cDeep neural network compression by in-parallel pruning-quantization\u201d, Tung and Mori for some works investigating this direction."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "B1ezIjyQiH", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1ezIjyQiH", "sid": 1}, {"labels": {"alignments": [6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1ezIjyQiH", "sid": 2}, {"labels": {"alignments": [7], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "B1ezIjyQiH", "sid": 3}, {"labels": {"alignments": [7], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "B1ezIjyQiH", "sid": 4}, {"labels": {"alignments": [7], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "B1ezIjyQiH", "sid": 5}, {"labels": {"alignments": [7], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "B1ezIjyQiH", "sid": 6}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1ezIjyQiH", "sid": 7}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 8}, {"labels": {"alignments": [8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 9}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1ezIjyQiH", "sid": 10}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 11}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 12}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 13}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 14}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 15}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 16}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 17}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "B1ezIjyQiH", "sid": 18}, {"labels": {"alignments": [10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "B1ezIjyQiH", "sid": 19}], "metadata": {"anno": "anno10", "review": "H1gAi6GdtH", "rebuttal": "B1ezIjyQiH", "conference": "ICLR2020", "title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks", "reviewer": "AnonReviewer3", "forum_id": "rJehVyrKwH", "rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area."}}