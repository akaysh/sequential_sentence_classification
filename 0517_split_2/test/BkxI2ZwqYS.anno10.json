{"review": [{"text_id": "BkxI2ZwqYS", "sid": 0, "sentence": "This paper investigates an SGD variant (PowerSGD) where the stochastic gradient is raised to a power of $\\gamma \\in [0,1]$.  The authors introduce PowerSGD and PowerSGD with momentum (PowerSGDM)."}, {"text_id": "BkxI2ZwqYS", "sid": 1, "sentence": "The theoretical proof of  the convergence is given and experimental results show that the proposed algorithm converges faster than some of the existing popular adaptive SGD techniques."}, {"text_id": "BkxI2ZwqYS", "sid": 2, "sentence": "Intuitively, the proposed PowerSGD can boost the gradient (since $\\gamma \\in [0,1]$) so it may be helpful for the gradient of the lower layers of a deep network which may be hit by the vanishing gradient issue."}, {"text_id": "BkxI2ZwqYS", "sid": 3, "sentence": "This may give rise to a faster convergence."}, {"text_id": "BkxI2ZwqYS", "sid": 4, "sentence": "So overall the idea makes sense but I have the following concerns."}, {"text_id": "BkxI2ZwqYS", "sid": 5, "sentence": "1."}, {"text_id": "BkxI2ZwqYS", "sid": 6, "sentence": "The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD."}, {"text_id": "BkxI2ZwqYS", "sid": 7, "sentence": "At the first glance, it is $O(\\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\\frac{1}{\\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$."}, {"text_id": "BkxI2ZwqYS", "sid": 8, "sentence": "In other words, when the number of iterations is large, the batch size will be large too."}, {"text_id": "BkxI2ZwqYS", "sid": 9, "sentence": "I consider this assumption unrealistic."}, {"text_id": "BkxI2ZwqYS", "sid": 10, "sentence": "Given that $T$ is typically very large (it is iterations, not epochs),  it will require a huge batch size, probably close to the whole training set."}, {"text_id": "BkxI2ZwqYS", "sid": 11, "sentence": "In this case, it is basically a GD, not SGD any more."}, {"text_id": "BkxI2ZwqYS", "sid": 12, "sentence": "That's why the rate is $O(\\frac{1}{T})$, which is the convergence rate of GD."}, {"text_id": "BkxI2ZwqYS", "sid": 13, "sentence": "I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume."}, {"text_id": "BkxI2ZwqYS", "sid": 14, "sentence": "Actually in the experiments the authors never use an increasing batch size."}, {"text_id": "BkxI2ZwqYS", "sid": 15, "sentence": "Instead, a constant batch size 128 is used."}, {"text_id": "BkxI2ZwqYS", "sid": 16, "sentence": "Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2."}, {"text_id": "BkxI2ZwqYS", "sid": 17, "sentence": "2. There are numerous inaccuracies in the proof given the supplementary material."}, {"text_id": "BkxI2ZwqYS", "sid": 18, "sentence": "For instance, in Eq.7,"}, {"text_id": "BkxI2ZwqYS", "sid": 19, "sentence": "$\\nabla f(x) \\sigma(\\nabla f(x))$"}, {"text_id": "BkxI2ZwqYS", "sid": 20, "sentence": "should be $\\nabla f(x)^{T} \\sigma(\\nabla f(x))$   The random variable $\\xi_{t}$ should be a scalar on training samples, not a vector, etc..  The authors should clean it up."}, {"text_id": "BkxI2ZwqYS", "sid": 21, "sentence": "3. It would be helpful to show the $\\gamma$ value on each experiment with different tasks."}, {"text_id": "BkxI2ZwqYS", "sid": 22, "sentence": "It would be good to know how $\\gamma$ varies across tasks."}, {"text_id": "BkxI2ZwqYS", "sid": 23, "sentence": "4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm."}, {"text_id": "BkxI2ZwqYS", "sid": 24, "sentence": "5. The term \"PowerSGD\" seems to have been used by other papers."}], "reviewlabels": [{"text_id": "BkxI2ZwqYS", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 10, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 12, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 15, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 17, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 18, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 19, "labels": {"coarse": "Structuring", "fine": "Structuring.Quote", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 20, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 21, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 22, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 23, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BkxI2ZwqYS", "sid": 24, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Other", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rke7afxSjr", "sid": 0, "sentence": "We thank you for your comments and hope that the following response will address your concerns."}, {"text_id": "rke7afxSjr", "sid": 1, "sentence": "1. We did stated both in the Theorem statements and Remark 3.4 that the a large batch size $B_t=T$ is used for the convergence proof."}, {"text_id": "rke7afxSjr", "sid": 2, "sentence": "This means the effective rate of convergence is $O(1/\\sqrt{T})$ as pointed out by the reviewer."}, {"text_id": "rke7afxSjr", "sid": 3, "sentence": "This rate matches the currently best known rate of convergence for SGD (see, e.g. Ge et al., COLT'15)."}, {"text_id": "rke7afxSjr", "sid": 4, "sentence": "We have now made this very clear in both Remarks 3.3 and 3.5."}, {"text_id": "rke7afxSjr", "sid": 5, "sentence": "Please see changes highlighted in blue and also our response to Reviewer 2 on novelty of the convergence analysis."}, {"text_id": "rke7afxSjr", "sid": 6, "sentence": "If our response addresses your main concern, we sincerely hope you that you can reconsider your score."}, {"text_id": "rke7afxSjr", "sid": 7, "sentence": "For your other points, we have made the following changes in the paper."}, {"text_id": "rke7afxSjr", "sid": 8, "sentence": "2. We have checked and fixed a few typos in the paper."}, {"text_id": "rke7afxSjr", "sid": 9, "sentence": "Please note that we wrote  $\\nabla f(x)\\cdot \\sigma (\\nabla f(x))$ in eq. (7) as a dot product."}, {"text_id": "rke7afxSjr", "sid": 10, "sentence": "So it is the same as $\\nabla f(x)^{T} \\sigma(\\nabla f(x))$. This notation was explained in the notation section."}, {"text_id": "rke7afxSjr", "sid": 11, "sentence": "If you have any remaining concerns, please let us know."}, {"text_id": "rke7afxSjr", "sid": 12, "sentence": "3. We have added the values for chosen $\\gamma$ in the updated version (see caption of Figure 1)."}, {"text_id": "rke7afxSjr", "sid": 13, "sentence": "4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments."}, {"text_id": "rke7afxSjr", "sid": 14, "sentence": "We promise to do so in the final version."}, {"text_id": "rke7afxSjr", "sid": 15, "sentence": "5. We were not aware of this at the time of submission."}, {"text_id": "rke7afxSjr", "sid": 16, "sentence": "We have changed this to PoweredSGD. If you have any alternative suggestions, please let us know."}, {"text_id": "rke7afxSjr", "sid": 17, "sentence": "We summarize the main contributions of the paper as follows:"}, {"text_id": "rke7afxSjr", "sid": 18, "sentence": "- In the theoretical part, we provided more concise convergence rate analysis for stochastic momentum methods in the non-convex setting."}, {"text_id": "rke7afxSjr", "sid": 19, "sentence": "This was made possible by a sharp estimate of the accumulated momentum terms (Lemma B1)."}, {"text_id": "rke7afxSjr", "sid": 20, "sentence": "We believe this is an important but under-explored topic (Yan et al., 2018)."}, {"text_id": "rke7afxSjr", "sid": 21, "sentence": "- In the experimental part, we empirically showed that the proposed optimisation algorithms have potential to solve realistic problems."}, {"text_id": "rke7afxSjr", "sid": 22, "sentence": "We are not claiming these variants will outperform all other methods in all training cases, but we sincerely believe that the results are promising."}, {"text_id": "rke7afxSjr", "sid": 23, "sentence": "In particular, we have demonstrated their potential benefits of mitigating gradient vanishing and combining other techniques for accelerating optimization."}, {"text_id": "rke7afxSjr", "sid": 24, "sentence": "We do admit the gap between our theoretical analysis and experiments in the sense that the analysis does not account for the initial acceleration observed in many experiments."}, {"text_id": "rke7afxSjr", "sid": 25, "sentence": "We think this is a very interesting question for future research and hope that this paper can motivate further research in this area."}, {"text_id": "rke7afxSjr", "sid": 26, "sentence": "We agree with your intuition that this may have something to do with $\\gamma\\in (0,1)$ boosting the gradients."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 0}, {"labels": {"alignments": [6, 7], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rke7afxSjr", "sid": 1}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11, 12], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rke7afxSjr", "sid": 2}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11, 12], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rke7afxSjr", "sid": 3}, {"labels": {"alignments": [6, 7, 8, 9, 10, 11, 12], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rke7afxSjr", "sid": 4}, {"labels": {"alignments": [], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rke7afxSjr", "sid": 5}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 6}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 7}, {"labels": {"alignments": [17, 18, 19, 20], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rke7afxSjr", "sid": 8}, {"labels": {"alignments": [17, 18, 19, 20], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rke7afxSjr", "sid": 9}, {"labels": {"alignments": [17, 18, 19, 20], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rke7afxSjr", "sid": 10}, {"labels": {"alignments": [17, 18, 19, 20], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 11}, {"labels": {"alignments": [21, 22], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rke7afxSjr", "sid": 12}, {"labels": {"alignments": [23], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "rke7afxSjr", "sid": 13}, {"labels": {"alignments": [23], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "rke7afxSjr", "sid": 14}, {"labels": {"alignments": [24], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "rke7afxSjr", "sid": 15}, {"labels": {"alignments": [24], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rke7afxSjr", "sid": 16}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 17}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 18}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 19}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 20}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 21}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 22}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 23}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 24}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 25}, {"labels": {"alignments": [0], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "rke7afxSjr", "sid": 26}], "metadata": {"anno": "anno10", "review": "BkxI2ZwqYS", "rebuttal": "rke7afxSjr", "conference": "ICLR2020", "title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "reviewer": "AnonReviewer1", "forum_id": "rJlqoTEtDB", "rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area."}}