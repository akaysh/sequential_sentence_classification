{"review": [{"text_id": "S1e82d0HqB", "sid": 0, "sentence": "This paper proposes to use codes and codebooks to compress the weights."}, {"text_id": "S1e82d0HqB", "sid": 1, "sentence": "The authors also try minimizing the layer reconstruction error instead of weight approximation error for better quantization results."}, {"text_id": "S1e82d0HqB", "sid": 2, "sentence": "Distillation loss is also used for fine-tuning the quantized weight."}, {"text_id": "S1e82d0HqB", "sid": 3, "sentence": "Empirical results on resnets show that the proposed method has a good compression ratio while maintaining competitive accuracy."}, {"text_id": "S1e82d0HqB", "sid": 4, "sentence": "This paper is overall easy to follow."}, {"text_id": "S1e82d0HqB", "sid": 5, "sentence": "My main concern comes from the novelty of this paper."}, {"text_id": "S1e82d0HqB", "sid": 6, "sentence": "The two main contributions of the paper:"}, {"text_id": "S1e82d0HqB", "sid": 7, "sentence": "(1) using codes and codebooks to compress weights; and"}, {"text_id": "S1e82d0HqB", "sid": 8, "sentence": "(2) minimizing layer reconstruction error instead of weight approximation error"}, {"text_id": "S1e82d0HqB", "sid": 9, "sentence": "are both not new."}, {"text_id": "S1e82d0HqB", "sid": 10, "sentence": "For instance, using codes and codebooks to compress the weights has already been used in [1,2]."}, {"text_id": "S1e82d0HqB", "sid": 11, "sentence": "A weighted k-means solver is also used in [2], though the \"weighted\" in [2] comes from second-order information instead of minimizing reconstruction error."}, {"text_id": "S1e82d0HqB", "sid": 12, "sentence": "In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4]."}, {"text_id": "S1e82d0HqB", "sid": 13, "sentence": "Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method."}, {"text_id": "S1e82d0HqB", "sid": 14, "sentence": "It is not clear how the compression ratio in table 1 is obtained."}, {"text_id": "S1e82d0HqB", "sid": 15, "sentence": "Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong)."}, {"text_id": "S1e82d0HqB", "sid": 16, "sentence": "Can the authors provide an example to explain how to compute the compression ratio?"}, {"text_id": "S1e82d0HqB", "sid": 17, "sentence": "[1]."}, {"text_id": "S1e82d0HqB", "sid": 18, "sentence": "Model compression as constrained optimization, with application to neural nets."}, {"text_id": "S1e82d0HqB", "sid": 19, "sentence": "part ii: quantization."}, {"text_id": "S1e82d0HqB", "sid": 20, "sentence": "[2]. Towards the limit of network quantization."}, {"text_id": "S1e82d0HqB", "sid": 21, "sentence": "[3]. Efficient and Accurate Approximations of Nonlinear Convolutional Networks."}, {"text_id": "S1e82d0HqB", "sid": 22, "sentence": "[4]. ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression."}], "reviewlabels": [{"text_id": "S1e82d0HqB", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 14, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 15, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 16, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 17, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 18, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 19, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 20, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 21, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1e82d0HqB", "sid": 22, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [{"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}], "merge-with-prior": false}], "rebuttal": [{"text_id": "rJlnnKJQiH", "sid": 0, "sentence": "We thank Reviewer 4 for stating that \u201cthe proposed method has a good compression ratio while maintaining competitive accuracy\u201d."}, {"text_id": "rJlnnKJQiH", "sid": 1, "sentence": "We provide clarification for the two main questions of the Reviewer below."}, {"text_id": "rJlnnKJQiH", "sid": 2, "sentence": "Novelty of the paper"}, {"text_id": "rJlnnKJQiH", "sid": 3, "sentence": "As we state in our introduction, using codebooks to compress networks is not new, as well as using a weighted k-means technique."}, {"text_id": "rJlnnKJQiH", "sid": 4, "sentence": "However, as we state in the paper: \u201cThe closest work we are aware of is the one by Choi et al. (2016), but the authors use a different objective (their weighted term is derived from second-order information) along with a different quantization technique (scalar quantization)."}, {"text_id": "rJlnnKJQiH", "sid": 5, "sentence": "Our method targets a better in-domain reconstruction, as depicted by Figure 1\u201d."}, {"text_id": "rJlnnKJQiH", "sid": 6, "sentence": "Note that we already cite two of the suggested references by Reviewer 4, namely \u201cTowards the limit of network quantization\u201d and \u201cThiNet: A filter level pruning method for deep neural network compression\u201d in our work."}, {"text_id": "rJlnnKJQiH", "sid": 7, "sentence": "We will further clarify our positioning in an updated version of the paper."}, {"text_id": "rJlnnKJQiH", "sid": 8, "sentence": "Compression ratio"}, {"text_id": "rJlnnKJQiH", "sid": 9, "sentence": "We provide an example of the computation of compression ratio in Section 4.1, paragraph \u201cMetrics\u201d."}, {"text_id": "rJlnnKJQiH", "sid": 10, "sentence": "Let us detail it further here."}, {"text_id": "rJlnnKJQiH", "sid": 11, "sentence": "The memory footprint of a compressed layer is split between the indexing cost (one index per block indicating the centroid used to encode the block) and the cost of storing the centroids."}, {"text_id": "rJlnnKJQiH", "sid": 12, "sentence": "Say we quantize a layer of size 128 \u00d7 128 \u00d7 3 \u00d7 3 with 256 centroids and a block size of 9."}, {"text_id": "rJlnnKJQiH", "sid": 13, "sentence": "Then, each block of size 9 is indexed by an integer between 0 and 255: such integer can be stored using 8 bits or 1 byte (as 2^8 = 256)."}, {"text_id": "rJlnnKJQiH", "sid": 14, "sentence": "Thus, as we have 128 x 128 blocks, the indexing cost is 128 x 128 x 1 byte = 16,384 bytes = 16 kB."}, {"text_id": "rJlnnKJQiH", "sid": 15, "sentence": "Finally, we have to store 256 centroids of dimension 9 in fp16, which represents 256 x 9 floats (fp16) = 256 x 9 x 2 = 4,608 bits = 4.5 kB."}, {"text_id": "rJlnnKJQiH", "sid": 16, "sentence": "The size of the compressed model is the sum of the sizes of the compressed layers."}, {"text_id": "rJlnnKJQiH", "sid": 17, "sentence": "Finally, we deduce the overall compression ratio which is the size of the compressed model divided by the size of the non-compressed model."}], "rebuttallabels": [{"labels": {"alignments": [3], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rJlnnKJQiH", "sid": 0}, {"labels": {"alignments": [5, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 1}, {"labels": {"alignments": [5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 2}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 3}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 4}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 5}, {"labels": {"alignments": [7, 8, 9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 6}, {"labels": {"alignments": [7, 8, 9, 10, 11, 12], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 7}, {"labels": {"alignments": [13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 8}, {"labels": {"alignments": [14, 15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 9}, {"labels": {"alignments": [14, 15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 10}, {"labels": {"alignments": [14, 15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 11}, {"labels": {"alignments": [14, 15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 12}, {"labels": {"alignments": [14, 15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 13}, {"labels": {"alignments": [14, 15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 14}, {"labels": {"alignments": [14, 15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 15}, {"labels": {"alignments": [14, 15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 16}, {"labels": {"alignments": [14, 15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlnnKJQiH", "sid": 17}], "metadata": {"anno": "anno12", "review": "S1e82d0HqB", "rebuttal": "rJlnnKJQiH", "conference": "ICLR2020", "title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks", "reviewer": "AnonReviewer4", "forum_id": "rJehVyrKwH", "rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area."}}