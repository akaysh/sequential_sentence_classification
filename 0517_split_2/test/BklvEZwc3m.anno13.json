{"review": [{"text_id": "BklvEZwc3m", "sid": 0, "sentence": "This paper presents some experiments using random projections instead of embeddings from a 1-of-V encoding."}, {"text_id": "BklvEZwc3m", "sid": 1, "sentence": "Experiments on the Penn TreeBank benchmark data set show that in a feed-forward language modeling architecture similar to that of (Bengio, 2003), the random projections substantially reduce the number of parameters of the model while not harming perplexity too much."}, {"text_id": "BklvEZwc3m", "sid": 2, "sentence": "The paper would need to be improved substantially in order to appear at a conference like ICLR."}, {"text_id": "BklvEZwc3m", "sid": 3, "sentence": "First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture."}, {"text_id": "BklvEZwc3m", "sid": 4, "sentence": "Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques."}, {"text_id": "BklvEZwc3m", "sid": 5, "sentence": "First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings."}, {"text_id": "BklvEZwc3m", "sid": 6, "sentence": "Second, the paper needs to use more state-of-the-art architectures."}, {"text_id": "BklvEZwc3m", "sid": 7, "sentence": "Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models."}, {"text_id": "BklvEZwc3m", "sid": 8, "sentence": "Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques."}, {"text_id": "BklvEZwc3m", "sid": 9, "sentence": "Changing the number/sizes of the network layers or using sparse weight matrices (perhaps with sparsity-inducing regularization) would be natural ways to reduce the parameter space."}, {"text_id": "BklvEZwc3m", "sid": 10, "sentence": "In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction."}, {"text_id": "BklvEZwc3m", "sid": 11, "sentence": "Minor"}, {"text_id": "BklvEZwc3m", "sid": 12, "sentence": "In the start of Section 3, it is not clear why having the projection be sparse is desired."}, {"text_id": "BklvEZwc3m", "sid": 13, "sentence": "Later, space (and time) efficiency is revealed as the motivation for the sparsity, but it would be helpful if the paper said this earlier."}, {"text_id": "BklvEZwc3m", "sid": 14, "sentence": "Equation 6 seems to have an error, the probability should be P(w_t | w_t-1...) instead of P(w_t , w_t-1...) if this is to represent the standard LM objective (the probability of the corpus)."}, {"text_id": "BklvEZwc3m", "sid": 15, "sentence": "Sec 3.3: \"all models sare\""}], "reviewlabels": [{"text_id": "BklvEZwc3m", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 4, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 6, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "BklvEZwc3m", "sid": 7, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "BklvEZwc3m", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Other", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 11, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 14, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BklvEZwc3m", "sid": 15, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "ryx8fMZiC7", "sid": 0, "sentence": "* models that get scores in the ~80 ppl range for Penn Treebank are important."}, {"text_id": "ryx8fMZiC7", "sid": 1, "sentence": "we agree with the advice but not with the justification."}, {"text_id": "ryx8fMZiC7", "sid": 2, "sentence": "We explain why in the general response: our goal is not to get good language models, but to use language modelling as a setting to test a property of a mechanism that is proposed."}, {"text_id": "ryx8fMZiC7", "sid": 3, "sentence": "The perplexity becomes a way to observer the effect of a mechanism and not the goal itself."}, {"text_id": "ryx8fMZiC7", "sid": 4, "sentence": "Moreover, (not in this case but) the architectures used to achieve better scores on given datasets are so over-parametrized that it's hardly reasonable to assume that the improvement justifies the cost of accommodating huge models overfitted to a particular dataset (and sometimes to a particular dataset configuration)"}, {"text_id": "ryx8fMZiC7", "sid": 5, "sentence": "That said, we agree that using different architectures would strengthen our point and make the paper more convincing."}, {"text_id": "ryx8fMZiC7", "sid": 6, "sentence": "Also, using different datasets would help us demonstrate that the effect of the proposed mechanism is data-independent."}, {"text_id": "ryx8fMZiC7", "sid": 7, "sentence": "We are also considering it's application to a different set of tasks in the future."}, {"text_id": "ryx8fMZiC7", "sid": 8, "sentence": "We did follow reviewer recommendations and performed experiments with LSTMs and QRNN (slightly faster) along with WikiText (which is larger but not intractable), unfortunately we couldn't accommodate all the analysis and changes in time."}, {"text_id": "ryx8fMZiC7", "sid": 9, "sentence": "* its parameter-reduction approaches against other compression and hyperparameter optimization techniques."}, {"text_id": "ryx8fMZiC7", "sid": 10, "sentence": "We recognize that the focus on parameter reduction was perhaps counter productive to making the goal or this work clear."}, {"text_id": "ryx8fMZiC7", "sid": 11, "sentence": "It is a byproduct of the technique, but modelling discrete distributions without prior knowledge of how many classes one might encounter is the main issue we are trying to solve. We could do that by using character-level or sub-word tokens, but again, the goal is not --solely-- language modelling as a task."}, {"text_id": "ryx8fMZiC7", "sid": 12, "sentence": "The mechanism is applicable to settings where the number of possible input patterns is too large to instantiate as a parameter table (embeddings), but where the number of patterns that actually occur could actually more \"reasonable\". Meaning that as long as the \"world\" is not random uniform, we can make predictions."}], "rebuttallabels": [{"labels": {"alignments": [7], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryx8fMZiC7", "sid": 0}, {"labels": {"alignments": [7], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryx8fMZiC7", "sid": 1}, {"labels": {"alignments": [7], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryx8fMZiC7", "sid": 2}, {"labels": {"alignments": [7], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryx8fMZiC7", "sid": 3}, {"labels": {"alignments": [7], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "ryx8fMZiC7", "sid": 4}, {"labels": {"alignments": [7], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "ryx8fMZiC7", "sid": 5}, {"labels": {"alignments": [7], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "ryx8fMZiC7", "sid": 6}, {"labels": {"alignments": [7], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "ryx8fMZiC7", "sid": 7}, {"labels": {"alignments": [5, 7], "responsetype": "reject-request_scope_No", "coarseresponse": "dispute"}, "text_id": "ryx8fMZiC7", "sid": 8}, {"labels": {"alignments": [8], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "ryx8fMZiC7", "sid": 9}, {"labels": {"alignments": [8], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "ryx8fMZiC7", "sid": 10}, {"labels": {"alignments": [8], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryx8fMZiC7", "sid": 11}, {"labels": {"alignments": [8], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "ryx8fMZiC7", "sid": 12}], "metadata": {"anno": "anno13", "review": "BklvEZwc3m", "rebuttal": "ryx8fMZiC7", "conference": "ICLR2019", "title": "Neural Random Projections for Language Modelling", "reviewer": "AnonReviewer1", "forum_id": "BJlMcjC5K7", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}