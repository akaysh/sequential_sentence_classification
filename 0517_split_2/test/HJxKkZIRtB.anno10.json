{"review": [{"text_id": "HJxKkZIRtB", "sid": 0, "sentence": "The paper presents an end-to-end methods for jointly training named entity recognition (NER) and relation extraction (RE)."}, {"text_id": "HJxKkZIRtB", "sid": 1, "sentence": "The model leverage pre-trained BERT language models, making it very fast to train."}, {"text_id": "HJxKkZIRtB", "sid": 2, "sentence": "The methods is evaluated on 5 standard NER+RE datasets with good performances."}, {"text_id": "HJxKkZIRtB", "sid": 3, "sentence": "Pros:"}, {"text_id": "HJxKkZIRtB", "sid": 4, "sentence": "- the paper is well written and very clear"}, {"text_id": "HJxKkZIRtB", "sid": 5, "sentence": "- the proposed model has two main advantages: (1) it is very fast to train due to the use of pre-trained BERT representations and (2) it does not depends on any external NLP tool (such as dependency parser)"}, {"text_id": "HJxKkZIRtB", "sid": 6, "sentence": "Cons:"}, {"text_id": "HJxKkZIRtB", "sid": 7, "sentence": "- I think the main source of improvement comes from the BERT representations used as input."}, {"text_id": "HJxKkZIRtB", "sid": 8, "sentence": "As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE."}, {"text_id": "HJxKkZIRtB", "sid": 9, "sentence": "- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train. This is not really surprising..."}], "reviewlabels": [{"text_id": "HJxKkZIRtB", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxKkZIRtB", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxKkZIRtB", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxKkZIRtB", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxKkZIRtB", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxKkZIRtB", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxKkZIRtB", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxKkZIRtB", "sid": 7, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxKkZIRtB", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJxKkZIRtB", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "Bkg8JkNJsB", "sid": 0, "sentence": "Hello,"}, {"text_id": "Bkg8JkNJsB", "sid": 1, "sentence": "We would like to thank you for reviewing our paper."}, {"text_id": "Bkg8JkNJsB", "sid": 2, "sentence": "Also, thank you for your comment about the clarity of the writing, we spent a lot of effort ensuring the paper was easy to read."}, {"text_id": "Bkg8JkNJsB", "sid": 3, "sentence": "Regarding the suggested ablation,"}, {"text_id": "Bkg8JkNJsB", "sid": 4, "sentence": "This comment was also made in the official blind review #2."}, {"text_id": "Bkg8JkNJsB", "sid": 5, "sentence": "We also responded to this suggestion in the public comment. For your convenience, we have copied our response here:"}, {"text_id": "Bkg8JkNJsB", "sid": 6, "sentence": "In our model, BERT is more than a source of contextual word embeddings as we fine-tune all of its ~110M parameters during training."}, {"text_id": "Bkg8JkNJsB", "sid": 7, "sentence": "Simply replacing BERT with distributed embeddings and a character-CNN or LSTM wouldn\u2019t allow us to determine the effect of contextualized embeddings because we would simultaneously be removing the majority of our model\u2019s trainable parameters."}, {"text_id": "Bkg8JkNJsB", "sid": 8, "sentence": "Nevertheless, we performed the suggested ablation by swapping BERT for GloVe embeddings (300 dimensional) and found that NER performance dropped from 89.46% to 40.33% and RE performance fell from 66.83% to 14.44% on the test set of the ConLL04 corpus (note that we had to increase the learning rate by 10X to get the model to converge)."}, {"text_id": "Bkg8JkNJsB", "sid": 9, "sentence": "If you were to somehow control for this drop in model capacity, say by adding in an LSTM network, the ablated model would closely match this paper [1], whom we outperform by ~3% overall on the CoNLL04 corpus."}, {"text_id": "Bkg8JkNJsB", "sid": 10, "sentence": "This paper is not cited in Table 1 as they report macro-averaged F1 scores, while most other papers (including the current state-of-the-art [2]) report micro-averaged F1 scores, as we did."}, {"text_id": "Bkg8JkNJsB", "sid": 11, "sentence": "Finally, it is well known that contextual embeddings outperform distributed embeddings on a wide range of NLP tasks, including NER [3]."}, {"text_id": "Bkg8JkNJsB", "sid": 12, "sentence": "The aim of our study wasn\u2019t to compare contextual vs. distributed embeddings but on how to successfully integrate BERT into a state-of-the-art joint NER and RE architecture."}, {"text_id": "Bkg8JkNJsB", "sid": 13, "sentence": "Regarding your comments:"}, {"text_id": "Bkg8JkNJsB", "sid": 14, "sentence": "\u201cI think the main source of improvement comes from the BERT representations used as input.\u201d"}, {"text_id": "Bkg8JkNJsB", "sid": 15, "sentence": "\u201c[...] the contributions of this paper are to show that using BERT representations as input [\u2026]\u201d"}, {"text_id": "Bkg8JkNJsB", "sid": 16, "sentence": "We would like to clarify that we are not simply using BERT representations as input."}, {"text_id": "Bkg8JkNJsB", "sid": 17, "sentence": "We are integrating BERT as part of our model architecture and fine-tuning it along with the task-specific parameters (as stated in the second to last paragraph of the introduction)."}, {"text_id": "Bkg8JkNJsB", "sid": 18, "sentence": "For the particular problem of joint NER and RE, we found this to be critical."}, {"text_id": "Bkg8JkNJsB", "sid": 19, "sentence": "For example, early on in our experiments we tested using BERT as a feature extractor vs. fine-tuning the entire architecture and found that performance dropped to ~42.82% (from 78.15%) on the CoNLL04 corpus."}, {"text_id": "Bkg8JkNJsB", "sid": 20, "sentence": "Integrating BERT as part of our model (as opposed to simply using its embeddings as inputs) allowed us to swap recurrent architectures common in joint NER and RE models in favour of simple and shallow task-specific architectures composed of feed forward neural networks."}, {"text_id": "Bkg8JkNJsB", "sid": 21, "sentence": "This reduced training times while improving performance (see our response to official review #1 for more details)."}, {"text_id": "Bkg8JkNJsB", "sid": 22, "sentence": "Again, thanks for your constructive comments!"}, {"text_id": "Bkg8JkNJsB", "sid": 23, "sentence": "[1] https://link.springer.com/chapter/10.1007/978-3-030-15712-8_47"}, {"text_id": "Bkg8JkNJsB", "sid": 24, "sentence": "[2] https://arxiv.org/abs/1905.05529"}, {"text_id": "Bkg8JkNJsB", "sid": 25, "sentence": "[3] https://arxiv.org/abs/1802.05365"}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 0}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 1}, {"labels": {"alignments": [4], "responsetype": "accept-praise", "coarseresponse": "concur"}, "text_id": "Bkg8JkNJsB", "sid": 2}, {"labels": {"alignments": [7, 8], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 3}, {"labels": {"alignments": [7, 8], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 4}, {"labels": {"alignments": [7, 8], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 5}, {"labels": {"alignments": [7, 8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 6}, {"labels": {"alignments": [7, 8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 7}, {"labels": {"alignments": [7, 8], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "Bkg8JkNJsB", "sid": 8}, {"labels": {"alignments": [7, 8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 9}, {"labels": {"alignments": [7, 8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 10}, {"labels": {"alignments": [7, 8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 11}, {"labels": {"alignments": [7, 8], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 12}, {"labels": {"alignments": [7, 8, 9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 13}, {"labels": {"alignments": [7], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 14}, {"labels": {"alignments": [9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 15}, {"labels": {"alignments": [7, 9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Bkg8JkNJsB", "sid": 16}, {"labels": {"alignments": [7, 9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Bkg8JkNJsB", "sid": 17}, {"labels": {"alignments": [7, 9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Bkg8JkNJsB", "sid": 18}, {"labels": {"alignments": [7, 9], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 19}, {"labels": {"alignments": [7, 9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Bkg8JkNJsB", "sid": 20}, {"labels": {"alignments": [7, 9], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Bkg8JkNJsB", "sid": 21}, {"labels": {"alignments": [7, 9], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 22}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 23}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 24}, {"labels": {"alignments": [], "responsetype": "other", "coarseresponse": "nonarg"}, "text_id": "Bkg8JkNJsB", "sid": 25}], "metadata": {"anno": "anno10", "review": "HJxKkZIRtB", "rebuttal": "Bkg8JkNJsB", "conference": "ICLR2020", "title": "End-to-end named entity recognition and relation extraction using pre-trained language models", "reviewer": "AnonReviewer3", "forum_id": "rkgqm0VKwB", "rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years."}}