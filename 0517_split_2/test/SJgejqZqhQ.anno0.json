{"review": [{"text_id": "SJgejqZqhQ", "sid": 0, "sentence": "The authors consider the use of tensor approximations to more accurately capture syntactical aspects of compositionality for word embeddings."}, {"text_id": "SJgejqZqhQ", "sid": 1, "sentence": "Given two words a and b, when your goal is to find a word whose meaning is roughly that of the phrase (a,b)"}, {"text_id": "SJgejqZqhQ", "sid": 2, "sentence": ", a standard approach to to find the word whose embedding is close to the sum of the embeddings, a + b. The authors point out that others have observed that this form of compositionality does not leverage any information on the syntax of the pair (a,b), and the propose using a tensor contraction to model an additional multiplicative interaction between a and b, so they propose finding the word whose embedding is closest to a + b + T*a*b, where T is a tensor, and T*a*b denotes the vector obtained by contracting a and b with T. They test this idea specifically on the use-case where (a,b) is an adjective,noun pair, and show that their form of compositionality outperforms weighted versions of additive compositionality in terms of spearman and pearson correlation with human judgements."}, {"text_id": "SJgejqZqhQ", "sid": 3, "sentence": "In their model, the word embeddings are learned separately, then the tensor T is learned by minimizing an objective whose goal is to minimize the error in predicting observed trigram statistics."}, {"text_id": "SJgejqZqhQ", "sid": 4, "sentence": "The specific objective comes from a nontrivial tensorial extension of the original matricial RAND-WALK model for learning word embeddings."}, {"text_id": "SJgejqZqhQ", "sid": 5, "sentence": "The topic is fitting with ICLR, and some attendees will find the results interesting."}, {"text_id": "SJgejqZqhQ", "sid": 6, "sentence": "As in the original RAND-WALK paper, the theory is interesting, but not the main attraction, as it relies on strong generative modeling assumptions that essentially bake in the desired results."}, {"text_id": "SJgejqZqhQ", "sid": 7, "sentence": "The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown)."}, {"text_id": "SJgejqZqhQ", "sid": 8, "sentence": "Pros:"}, {"text_id": "SJgejqZqhQ", "sid": 9, "sentence": "- theoretical justification is given for their assumption that the higher-order interactions can be modeled by a tensor"}, {"text_id": "SJgejqZqhQ", "sid": 10, "sentence": "- the tensor model does deliver some improvement over linear composition on noun-adjective pairs when measured against human judgement"}, {"text_id": "SJgejqZqhQ", "sid": 11, "sentence": "Cons:"}, {"text_id": "SJgejqZqhQ", "sid": 12, "sentence": "- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks."}, {"text_id": "SJgejqZqhQ", "sid": 13, "sentence": "- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?"}, {"text_id": "SJgejqZqhQ", "sid": 14, "sentence": "- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper"}, {"text_id": "SJgejqZqhQ", "sid": 15, "sentence": "Some additional citations:"}, {"text_id": "SJgejqZqhQ", "sid": 16, "sentence": "- the above-mentioned ICLR paper provides a performant alternative to unweighted linear composition"}, {"text_id": "SJgejqZqhQ", "sid": 17, "sentence": "- the 2017 Gittens, Achlioptas, Drineas ACL paper provides theory on the linear composition of some word embeddings"}], "reviewlabels": [{"text_id": "SJgejqZqhQ", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "SJgejqZqhQ", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "SJgejqZqhQ", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 11, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 15, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 16, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "SJgejqZqhQ", "sid": 17, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "HyeFqDog0X", "sid": 0, "sentence": "We are grateful to the reviewer for their time and effort in reading our paper and providing feedback."}, {"text_id": "HyeFqDog0X", "sid": 1, "sentence": "Generative model assumptions: our model is an expansion of the original RAND-WALK model of Arora et. al., with the purpose of accounting for syntactic dependencies."}, {"text_id": "HyeFqDog0X", "sid": 2, "sentence": "The additional assumptions we include and the concentration phenomena we prove theoretically are verified empirically in section 5, so our results do hold up on real data."}, {"text_id": "HyeFqDog0X", "sid": 3, "sentence": "Use on downstream tasks: we believe that capturing syntactic relationships using a tensor can be useful for some downstream tasks, since our results in the paper suggest that it captures additional information above and beyond the standard additive composition."}, {"text_id": "HyeFqDog0X", "sid": 4, "sentence": "However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work."}, {"text_id": "HyeFqDog0X", "sid": 5, "sentence": "Interaction between arbitrary word pairs: our model introduces the tensor in order to capture syntactic relationships between pairs of words, such as adjective-noun and verb-object pairs."}, {"text_id": "HyeFqDog0X", "sid": 6, "sentence": "While it might be interesting to try to capture interactions between all pairs of words, that is not justified by our model and we didn't explore it."}, {"text_id": "HyeFqDog0X", "sid": 7, "sentence": "However, we also trained our model using verb-object pairs, and we have updated section 5 as well as the appendix to include these additional results."}, {"text_id": "HyeFqDog0X", "sid": 8, "sentence": "Comparison to Arora, Liang, Ma ICLR 2017: we appreciate the suggestion to include a comparison with the SIF embedding method of Arora et. al., as this method is also obtained from a variant of the original RAND-WALK paper."}, {"text_id": "HyeFqDog0X", "sid": 9, "sentence": "We have updated Table 2 and the discussion in section 5 to include these additional results."}, {"text_id": "HyeFqDog0X", "sid": 10, "sentence": "As reported in their paper, the SIF embeddings yield a strong baseline for sentence embedding tasks, and we find the same to be true in the phrase similarity task for adjective-noun phrases (not so for verb-object phrases)."}, {"text_id": "HyeFqDog0X", "sid": 11, "sentence": "However, we find that we can improve upon the SIF performance by addition of the tensor component from our model. (We note that we have just used the tensors trained in our original model; it is possible that combining the model in SIF and syntactic RAND-WALK more carefully could give even better results.)"}, {"text_id": "HyeFqDog0X", "sid": 12, "sentence": "Additional citations: we have updated the paper to include both additional citations."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HyeFqDog0X", "sid": 0}, {"labels": {"alignments": [6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HyeFqDog0X", "sid": 1}, {"labels": {"alignments": [6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HyeFqDog0X", "sid": 2}, {"labels": {"alignments": [7, 12], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "HyeFqDog0X", "sid": 3}, {"labels": {"alignments": [7, 12], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "HyeFqDog0X", "sid": 4}, {"labels": {"alignments": [13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HyeFqDog0X", "sid": 5}, {"labels": {"alignments": [13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HyeFqDog0X", "sid": 6}, {"labels": {"alignments": [13], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "HyeFqDog0X", "sid": 7}, {"labels": {"alignments": [13], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "HyeFqDog0X", "sid": 8}, {"labels": {"alignments": [13], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "HyeFqDog0X", "sid": 9}, {"labels": {"alignments": [13], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "HyeFqDog0X", "sid": 10}, {"labels": {"alignments": [13], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "HyeFqDog0X", "sid": 11}, {"labels": {"alignments": [], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "HyeFqDog0X", "sid": 12}], "metadata": {"anno": "anno0", "review": "SJgejqZqhQ", "rebuttal": "HyeFqDog0X", "conference": "ICLR2019", "title": "Understanding Composition of Word Embeddings via Tensor Decomposition", "reviewer": "AnonReviewer1", "forum_id": "H1eqjiCctX", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}