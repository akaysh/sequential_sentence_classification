{"review": [{"text_id": "ryegpfeaYS", "sid": 0, "sentence": "This paper suggests a quantization approach for neural networks, based on the Product Quantization (PQ) algorithm which has been successful in quantization for similarity search."}, {"text_id": "ryegpfeaYS", "sid": 1, "sentence": "The basic idea is to quantize the weights of a neuron/single layer with a variant of PQ, which is modified to optimize the quantization error of inner products of sample inputs with the weights, rather than the weights themselves."}, {"text_id": "ryegpfeaYS", "sid": 2, "sentence": "This is cast as a weighted variant of k-means."}, {"text_id": "ryegpfeaYS", "sid": 3, "sentence": "The inner product is more directly related to the network output (though still does not account for non-linear neuron activations) and thus is expected to yield better downstream performance, and only requires introducing unlabeled input samples into the quantization process."}, {"text_id": "ryegpfeaYS", "sid": 4, "sentence": "This approach is built into a pipeline that gradually quantizes the entire network."}, {"text_id": "ryegpfeaYS", "sid": 5, "sentence": "Overall, I support the paper and recommend acceptance."}, {"text_id": "ryegpfeaYS", "sid": 6, "sentence": "PQ is known to be successful for quantization in other contexts, and the specialization suggested here for neural networks is natural and well-motivated."}, {"text_id": "ryegpfeaYS", "sid": 7, "sentence": "The method can be expected to perform well empirically, which the experiments verify, and to have potential impact."}, {"text_id": "ryegpfeaYS", "sid": 8, "sentence": "Questions:"}, {"text_id": "ryegpfeaYS", "sid": 9, "sentence": "1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead."}, {"text_id": "ryegpfeaYS", "sid": 10, "sentence": "Does it pose a difficulty? How does it compare to other methods?"}, {"text_id": "ryegpfeaYS", "sid": 11, "sentence": "2. Can you elaborate on the issue of non-linearity? It is mentioned only briefly in the conclusion. What is the difficulty in incorporating it? Is it in solving equation (4)? And perhaps, how do you expect it to effect the results?"}], "reviewlabels": [{"text_id": "ryegpfeaYS", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryegpfeaYS", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "SJlzQjkXjB", "sid": 0, "sentence": "We thank Reviewer 2 for their support and questions."}, {"text_id": "SJlzQjkXjB", "sid": 1, "sentence": "We answer them below."}, {"text_id": "SJlzQjkXjB", "sid": 2, "sentence": "Quantization time"}, {"text_id": "SJlzQjkXjB", "sid": 3, "sentence": "As we state in our paper, quantizing a ResNet-50 (quantization + finetuning steps) takes about one day on one Volta V100 GPU."}, {"text_id": "SJlzQjkXjB", "sid": 4, "sentence": "The time of quantization is around 1 to 2 hours, the rest being dedicated to finetuning."}, {"text_id": "SJlzQjkXjB", "sid": 5, "sentence": "Thus, the time dedicated to quantization is relatively short, especially compared with the fine-tuning and even more with the initial network training."}, {"text_id": "SJlzQjkXjB", "sid": 6, "sentence": "This is because we optimized our EM implementation in at least two ways as detailed below."}, {"text_id": "SJlzQjkXjB", "sid": 7, "sentence": "-\tThe E-step is performed on the GPU (see file src/quantization/distance.py, lines 61-75) with automatic chunking."}, {"text_id": "SJlzQjkXjB", "sid": 8, "sentence": "This means that the code chunks the centroids and the weight matrices into blocks, performs the distance computation on those blocks and aggregates the results."}, {"text_id": "SJlzQjkXjB", "sid": 9, "sentence": "This falls within the map/reduce paradigm."}, {"text_id": "SJlzQjkXjB", "sid": 10, "sentence": "Note that the blocks are automatically calculated to be the largest that fit into the GPU, such that the utilization of the GPU is maximized, so as to minimize the compute time."}, {"text_id": "SJlzQjkXjB", "sid": 11, "sentence": "-\tThe M-step involves calculating a solution of a least squares problem (see footnote 2 in our paper)."}, {"text_id": "SJlzQjkXjB", "sid": 12, "sentence": "The bottleneck for this is to calculate the pseudo-inverse of the activations x. However, we fix x when iterating our EM algorithm, therefore we can factor the computation of the pseudo inverse of x before alternating between the E and the M steps (see file src/quantization/solver.py and in particular the docstring)."}, {"text_id": "SJlzQjkXjB", "sid": 13, "sentence": "We provided pointers to the files in the code anonymously shared on OpenReview."}, {"text_id": "SJlzQjkXjB", "sid": 14, "sentence": "To our knowledge, these implementation strategies are novel in this context and were key in the development of our method to be able to iterate rapidly."}, {"text_id": "SJlzQjkXjB", "sid": 15, "sentence": "Both strategies are documented in the code so that they can benefit to the community."}, {"text_id": "SJlzQjkXjB", "sid": 16, "sentence": "Incorporating the non-linearity"}, {"text_id": "SJlzQjkXjB", "sid": 17, "sentence": "As the Reviewer rightfully stated, optimally we should take the non-linearity in Equation (4) into account."}, {"text_id": "SJlzQjkXjB", "sid": 18, "sentence": "One could hope for a higher compression ratio."}, {"text_id": "SJlzQjkXjB", "sid": 19, "sentence": "Indeed, the approximation constraint on the positive outputs would stay the same (they have to be close to the original outputs)."}, {"text_id": "SJlzQjkXjB", "sid": 20, "sentence": "On the other hand, the only constraint lying on the negative outputs is that they should remain negative (with a possible margin), but not necessarily close to the original negative outputs."}, {"text_id": "SJlzQjkXjB", "sid": 21, "sentence": "However, our early experiments with this method resulted in a rather unstable EM algorithm."}, {"text_id": "SJlzQjkXjB", "sid": 22, "sentence": "This direction may deserve further investigation."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SJlzQjkXjB", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SJlzQjkXjB", "sid": 1}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SJlzQjkXjB", "sid": 2}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 3}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 4}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 5}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 6}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 7}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 8}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 9}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 10}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 11}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 12}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 13}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 14}, {"labels": {"alignments": [9, 10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 15}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SJlzQjkXjB", "sid": 16}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 17}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 18}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 19}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 20}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 21}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SJlzQjkXjB", "sid": 22}], "metadata": {"anno": "anno10", "review": "ryegpfeaYS", "rebuttal": "SJlzQjkXjB", "conference": "ICLR2020", "title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks", "reviewer": "AnonReviewer2", "forum_id": "rJehVyrKwH", "rating": "8: Accept", "experience_assessment": "I have read many papers in this area."}}