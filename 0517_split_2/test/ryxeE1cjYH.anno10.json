{"review": [{"text_id": "ryxeE1cjYH", "sid": 0, "sentence": "This paper provides exact bounds on the risk when training a two-layer neural network in an asymptotic regime."}, {"text_id": "ryxeE1cjYH", "sid": 1, "sentence": "Namely, the paper considers training under the square-loss objective, a two-layer neural network with $h$ hidden units on inputs of dimension $d$ and training on $n$ samples."}, {"text_id": "ryxeE1cjYH", "sid": 2, "sentence": "The asymptotic regime is considered by making all of $d$, $h$, $n$ go to $\\infty$, in a way that the ratio $d/n$ approaches $\\gamma_1$ and the ratio $h/n$ approaches $\\gamma_2$."}, {"text_id": "ryxeE1cjYH", "sid": 3, "sentence": "This paper considers the following scenarios of training described below, where the data is generated from a linear model on Gaussian inputs and with a zero-mean noise."}, {"text_id": "ryxeE1cjYH", "sid": 4, "sentence": "The emphasis of the results is on understanding when a \"double descent\" type phenomenon occurs (\"Double descent\" is a recently coined phenomenon in literature where the risk, as a function of the \"complexity of the model\", initially has a classical U-shape behavior, but eventually decreases again once the complexity of the model exceeds the number of training points.)"}, {"text_id": "ryxeE1cjYH", "sid": 5, "sentence": "1. Training only the second layer: The risk is first decomposed into a bias and a variance term."}, {"text_id": "ryxeE1cjYH", "sid": 6, "sentence": "An exact bound on the variance term of the risk is obtained."}, {"text_id": "ryxeE1cjYH", "sid": 7, "sentence": "While the exact nature of the bound is rather complex to parse, the takeaway is that a double descent phenomenon is observed in terms of $\\gamma_2$, namely, the risk blows up when $h \\approx n$, but decreases as $h$ is increased beyond $n$."}, {"text_id": "ryxeE1cjYH", "sid": 8, "sentence": "2. Training only the first layer: Two different regimes are considered here, depending on the scale of initialization, called \"vanishing\" and \"non-vanishing\" initializations."}, {"text_id": "ryxeE1cjYH", "sid": 9, "sentence": "In both regimes, the risk is independent of $\\gamma_2$, that is, the risk does not depend on number of hidden units (although the risk bounds are different and there is an additional assumption in the case of non-vanishing initialization to ensure that the initialized network computes the zero function)."}, {"text_id": "ryxeE1cjYH", "sid": 10, "sentence": "In other words, a \"double descent\" phenomenon is not observed in this setting."}, {"text_id": "ryxeE1cjYH", "sid": 11, "sentence": "Recommendation:"}, {"text_id": "ryxeE1cjYH", "sid": 12, "sentence": "I recommend \"weak acceptance\"."}, {"text_id": "ryxeE1cjYH", "sid": 13, "sentence": "The paper extends prior works that obtain asymptotic risk bounds on linear models to the setting of two-layer neural networks (where only one layer is trained)."}, {"text_id": "ryxeE1cjYH", "sid": 14, "sentence": "However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory."}, {"text_id": "ryxeE1cjYH", "sid": 15, "sentence": "Technical Comments:"}, {"text_id": "ryxeE1cjYH", "sid": 16, "sentence": "- I felt that while it is valuable to have exact bounds on the risk, the form of the bounds are quite complex and hard to parse (especially in Thm 4, case of training only the second layer)."}, {"text_id": "ryxeE1cjYH", "sid": 17, "sentence": "Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically."}, {"text_id": "ryxeE1cjYH", "sid": 18, "sentence": "So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network."}, {"text_id": "ryxeE1cjYH", "sid": 19, "sentence": "- The parameter count of a 2-layer network with $h$ hidden units and input dimension $d$ is $O(dh)$. So perhaps it makes sense to study an asymptotic regime where $dh/n$ approaches $\\gamma$, instead of both d and h growing linearly in n. While this issue is hinted at in the discussion section, I don't understand the statement \"the mechanism that provably gives rise to double descent from previous works Hastie et al. (2019); Belkin et al. (2019) might not translate to optimizing two-layer neural networks.\""}, {"text_id": "ryxeE1cjYH", "sid": 20, "sentence": "- Another future direction that could be included in discussions is the setting where both layers are trained simultaneously."}], "reviewlabels": [{"text_id": "ryxeE1cjYH", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 9, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 10, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 11, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 12, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 13, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 15, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 17, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 18, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 19, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxeE1cjYH", "sid": 20, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "Bklc6FAsoH", "sid": 0, "sentence": "Thank you for the comments and suggestions."}, {"text_id": "Bklc6FAsoH", "sid": 1, "sentence": "The technical comments are addressed below:"}, {"text_id": "Bklc6FAsoH", "sid": 2, "sentence": "Extending result to other target functions:"}, {"text_id": "Bklc6FAsoH", "sid": 3, "sentence": "We agree that the problem might be significantly more difficult for different target functions, and would like to make the following remarks:"}, {"text_id": "Bklc6FAsoH", "sid": 4, "sentence": "1."}, {"text_id": "Bklc6FAsoH", "sid": 5, "sentence": "Note that in our bias-variance decomposition, only the bias term depends on the target function."}, {"text_id": "Bklc6FAsoH", "sid": 6, "sentence": "In other words, our result on the variance (including Theorem 4) would still be valid for other targets, such as two-layer neural network."}, {"text_id": "Bklc6FAsoH", "sid": 7, "sentence": "One caveat is that for general target function, the output needs to be properly scaled since our current analysis in Section 5 relies on linearizing the network."}, {"text_id": "Bklc6FAsoH", "sid": 8, "sentence": "2. When the target function is a multiple-neuron neural network, deriving the bias term can be challenging."}, {"text_id": "Bklc6FAsoH", "sid": 9, "sentence": "However, we note that under the same setup, the bias may be obtained when the teacher is a slightly more general single-index model, i.e. $y=\\psi(\\beta^\\top x)$ with Lipschitz link function $\\psi$, equivalent to a single-neuron network."}, {"text_id": "Bklc6FAsoH", "sid": 10, "sentence": "For instance, the bias under vanishing initialization is the same as that of least squares regression on the input, which can be solved under isotropic prior on $\\beta$ via decomposing the activation function similar to Appendix C.5."}, {"text_id": "Bklc6FAsoH", "sid": 11, "sentence": "Parameter count:"}, {"text_id": "Bklc6FAsoH", "sid": 12, "sentence": "To clarify our statement in the discussion section, our current result requires $n,d,h$ to grow at the same rate, and thus $n = O(dh)$ is beyond the regime we consider."}, {"text_id": "Bklc6FAsoH", "sid": 13, "sentence": "This is also true for previous works on double-descent in random feature model [Hastie et al. (2019)][Mei and Montanari (2019)]."}, {"text_id": "Bklc6FAsoH", "sid": 14, "sentence": "When $h \\ll n$, it is not clear if the same analysis still applies (for instance approximating the network with a kernel model), and thus the instability of the inverse may not be the complete explanation of double-descent (if it appears)."}, {"text_id": "Bklc6FAsoH", "sid": 15, "sentence": "Characterizing the generalization in this regime would be an interesting direction."}, {"text_id": "Bklc6FAsoH", "sid": 16, "sentence": "Training both layers:"}, {"text_id": "Bklc6FAsoH", "sid": 17, "sentence": "Thank you for the suggestion; we have included training both layers simultaneously as a future direction."}, {"text_id": "Bklc6FAsoH", "sid": 18, "sentence": "We would like to briefly mention that under certain model parameterization and initialization, gradient flow on both layers may reduce to one of the three models we analyzed (see [Williams et al. (2019)])."}, {"text_id": "Bklc6FAsoH", "sid": 19, "sentence": "More generally, our current result may be extended to cases where the dynamics of training both layers can be linearized (for instance initialization in the \"kernel regime\"), for which the learned model can be written down in closed-form."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 1}, {"labels": {"alignments": [13, 14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 2}, {"labels": {"alignments": [13, 14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 3}, {"labels": {"alignments": [13, 14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 4}, {"labels": {"alignments": [13, 14], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 5}, {"labels": {"alignments": [13, 14], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 6}, {"labels": {"alignments": [13, 14], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 7}, {"labels": {"alignments": [13, 14], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 8}, {"labels": {"alignments": [13, 14], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 9}, {"labels": {"alignments": [13, 14], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 10}, {"labels": {"alignments": [19], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 11}, {"labels": {"alignments": [19], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bklc6FAsoH", "sid": 12}, {"labels": {"alignments": [19], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bklc6FAsoH", "sid": 13}, {"labels": {"alignments": [19], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Bklc6FAsoH", "sid": 14}, {"labels": {"alignments": [19], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "Bklc6FAsoH", "sid": 15}, {"labels": {"alignments": [20], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 16}, {"labels": {"alignments": [20], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "Bklc6FAsoH", "sid": 17}, {"labels": {"alignments": [20], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 18}, {"labels": {"alignments": [20], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "Bklc6FAsoH", "sid": 19}], "metadata": {"anno": "anno10", "review": "ryxeE1cjYH", "rebuttal": "Bklc6FAsoH", "conference": "ICLR2020", "title": "Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint", "reviewer": "AnonReviewer3", "forum_id": "H1gBsgBYwH", "rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area."}}