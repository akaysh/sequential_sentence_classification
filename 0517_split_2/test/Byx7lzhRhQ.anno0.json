{"review": [{"text_id": "Byx7lzhRhQ", "sid": 0, "sentence": "This paper used the concept based on channel deficiency to derive a variational bound similar to variational information bottleneck."}, {"text_id": "Byx7lzhRhQ", "sid": 1, "sentence": "Theoretical analysis shows that this bound is an lower bound on the VIB objective."}, {"text_id": "Byx7lzhRhQ", "sid": 2, "sentence": "The empirical analysis shows it outperforms VIB in some sense."}, {"text_id": "Byx7lzhRhQ", "sid": 3, "sentence": "I think this paper's contribution is rather theoretical than practical."}, {"text_id": "Byx7lzhRhQ", "sid": 4, "sentence": "The experiments section can be improved in the following aspect:"}, {"text_id": "Byx7lzhRhQ", "sid": 5, "sentence": "-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines"}, {"text_id": "Byx7lzhRhQ", "sid": 6, "sentence": "- I(Z;Y) vs I(Z;X) graph is typically used in a VIB setting."}, {"text_id": "Byx7lzhRhQ", "sid": 7, "sentence": "In the paper's variational deficiency setting, although plotting I(Z;Y) vs I(Z;X) is necessary, it would be also helpful for the authors' to plot Deficiency vs I(Z;X), because this is what new objective is trading-off."}, {"text_id": "Byx7lzhRhQ", "sid": 8, "sentence": "- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings."}, {"text_id": "Byx7lzhRhQ", "sid": 9, "sentence": "- How do the paper estimate I(Z;Y) and I(Z;X) for plotting these figures? Does the paper use lower bound or some estimators? It should be made clear in the paper since these are non-trivial estimations."}, {"text_id": "Byx7lzhRhQ", "sid": 10, "sentence": "Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:"}, {"text_id": "Byx7lzhRhQ", "sid": 11, "sentence": "- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016"}, {"text_id": "Byx7lzhRhQ", "sid": 12, "sentence": "It was kind of surprising that the authors did not cite this paper given the results are pretty much the same."}, {"text_id": "Byx7lzhRhQ", "sid": 13, "sentence": "It would also be helpful for the authors to do a comparison or connection section with this paper."}, {"text_id": "Byx7lzhRhQ", "sid": 14, "sentence": "I like the paper in general, but given it still has some space for improvement, I would keep my decision as boarder line for now."}], "reviewlabels": [{"text_id": "Byx7lzhRhQ", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 6, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 7, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byx7lzhRhQ", "sid": 14, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "SkeI70MiAm", "sid": 0, "sentence": "Thank you for your comments!"}, {"text_id": "SkeI70MiAm", "sid": 1, "sentence": "* We included a table showing accuracy numbers for different values of beta and M (see p. 6, Table 1) for the latent bottleneck sizes K=256 (Figure 2) and K=2 (Figure 3)."}, {"text_id": "SkeI70MiAm", "sid": 2, "sentence": "*"}, {"text_id": "SkeI70MiAm", "sid": 3, "sentence": "In relation to the figures, we have improved these in the revision."}, {"text_id": "SkeI70MiAm", "sid": 4, "sentence": "We are added a figure tracing the mutual information between representation and output I(Z;Y) vs. the minimality term I(Z;X) for different values of beta (see Figure 2, lower right panel), when training with our loss function."}, {"text_id": "SkeI70MiAm", "sid": 5, "sentence": "This is the usual information bottleneck curve."}, {"text_id": "SkeI70MiAm", "sid": 6, "sentence": "This contrasts with the deficiency bottleneck curve (Figure 2, upper right panel) which traces the corresponding sufficiency term J(Z;Y) (which is just the entropy of the labels minus our loss) vs. I(Z;X) for different values of beta."}, {"text_id": "SkeI70MiAm", "sid": 7, "sentence": "Note that for M=1, J(Z;Y) = I(Z;Y)."}, {"text_id": "SkeI70MiAm", "sid": 8, "sentence": "We apologize for the confusion."}, {"text_id": "SkeI70MiAm", "sid": 9, "sentence": "The text now makes this more explicit (see p.7, first paragraph)."}, {"text_id": "SkeI70MiAm", "sid": 10, "sentence": "*"}, {"text_id": "SkeI70MiAm", "sid": 11, "sentence": "In response to your question about how we estimate the mutual information"}, {"text_id": "SkeI70MiAm", "sid": 12, "sentence": "."}, {"text_id": "SkeI70MiAm", "sid": 13, "sentence": "Yes, we minimize an upper bound on both the deficiency and the rate term (see p.3, equation 3 and discussion leading up to the VDB objective in equation 4)."}, {"text_id": "SkeI70MiAm", "sid": 14, "sentence": "The estimation of this upper bound is simplified by our choice of the prior and the encoding distribution which are diagonal Gaussians."}, {"text_id": "SkeI70MiAm", "sid": 15, "sentence": "The KL term can be computed and differentiated without estimation."}, {"text_id": "SkeI70MiAm", "sid": 16, "sentence": "We estimate the expected loss term using Monte Carlo sampling."}, {"text_id": "SkeI70MiAm", "sid": 17, "sentence": "We draw samples from the encoder using the reparameterization trick and leverage automatic differentiation (in Tensorflow) to compute the gradients."}, {"text_id": "SkeI70MiAm", "sid": 18, "sentence": "Since the expectation is inside the log, gradient updates may have higher variance for larger values of M."}, {"text_id": "SkeI70MiAm", "sid": 19, "sentence": "Our model is a classifier and our loss term is a tighter bound on the misclassification error (bias) than the usual cross-entropy loss as in the VIB (see p. 12, equation 13)."}, {"text_id": "SkeI70MiAm", "sid": 20, "sentence": "Trading bias for variance has been investigated in some recent works (see, e.g., Bamler, Robert, et al. \"Perturbative black box variational inference.\" NIPS 2017)."}, {"text_id": "SkeI70MiAm", "sid": 21, "sentence": "See last paragraph in p. 18 for the related discussion in the unsupervised setting."}, {"text_id": "SkeI70MiAm", "sid": 22, "sentence": "* In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1."}, {"text_id": "SkeI70MiAm", "sid": 23, "sentence": "The method is different from ours, except in the limiting case where M = 1 and beta =1, in which case it coincides with the beta-VAE and also with our method."}, {"text_id": "SkeI70MiAm", "sid": 24, "sentence": "After taking a close look, we make the following observations:"}, {"text_id": "SkeI70MiAm", "sid": 25, "sentence": "For M > 1, the IWAE bound does not admit a decomposition like the standard ELBO (see equation 29 and 36) into a reconstruction loss term and a regularization term."}, {"text_id": "SkeI70MiAm", "sid": 26, "sentence": "In particular, this implies we cannot trade-off reconstruction fidelity for learning more meaningful representations by incorporating bottleneck constraints."}, {"text_id": "SkeI70MiAm", "sid": 27, "sentence": "See ensuing discussion in p.18 following equation 36."}, {"text_id": "SkeI70MiAm", "sid": 28, "sentence": "In contrast, our method has a tuning parameter beta."}, {"text_id": "SkeI70MiAm", "sid": 29, "sentence": "The IWAE bound is known to be equivalent to the ELBO in expectation with a more complex approximate posterior qIW (see p.17, equation 34 and 35 and references therein in Appendix E.1)."}, {"text_id": "SkeI70MiAm", "sid": 30, "sentence": "For beta values other than 1, a naive trick would be to plant qIW in liue of qphi in equation 37 (p. 18) to get a beta-IWAE of sorts."}, {"text_id": "SkeI70MiAm", "sid": 31, "sentence": "It is not entirely clear however, why we would want to do so when modulating beta already suffices to tune the VAE towards autoencoding (low beta) or autodecoding behavior (high beta) depending on the requirement at hand."}, {"text_id": "SkeI70MiAm", "sid": 32, "sentence": "A similar argument goes in the direction of an \"Importance weighted Variational Information Bottleneck\"."}, {"text_id": "SkeI70MiAm", "sid": 33, "sentence": "We have not explored if and how using more expressive posteriors such as the qIW (p. 17, equation 35) can help the supervised bottleneck formulations in VDB or VIB."}, {"text_id": "SkeI70MiAm", "sid": 34, "sentence": "This remains a scope for future study."}, {"text_id": "SkeI70MiAm", "sid": 35, "sentence": "We are now also citing the paper Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SkeI70MiAm", "sid": 0}, {"labels": {"alignments": [5], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 1}, {"labels": {"alignments": [5], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 2}, {"labels": {"alignments": [5], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 3}, {"labels": {"alignments": [6, 7], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 4}, {"labels": {"alignments": [6, 7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 5}, {"labels": {"alignments": [6, 7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 6}, {"labels": {"alignments": [6, 7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 7}, {"labels": {"alignments": [6, 7], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 8}, {"labels": {"alignments": [6, 7], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 9}, {"labels": {"alignments": [6, 7], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 10}, {"labels": {"alignments": [9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkeI70MiAm", "sid": 11}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 12}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 13}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 14}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 15}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 16}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 17}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 18}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 19}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "SkeI70MiAm", "sid": 20}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "SkeI70MiAm", "sid": 21}, {"labels": {"alignments": [10, 11], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 22}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 23}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 24}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 25}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 26}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 27}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "SkeI70MiAm", "sid": 28}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 29}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 30}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 31}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 32}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 33}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 34}, {"labels": {"alignments": [10, 11, 12, 13], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SkeI70MiAm", "sid": 35}], "metadata": {"anno": "anno0", "review": "Byx7lzhRhQ", "rebuttal": "SkeI70MiAm", "conference": "ICLR2019", "title": "The Variational Deficiency Bottleneck", "reviewer": "AnonReviewer2", "forum_id": "rygjN3C9F7", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}