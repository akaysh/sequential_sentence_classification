{"review": [{"text_id": "S1g74Sfq9H", "sid": 0, "sentence": "Summary"}, {"text_id": "S1g74Sfq9H", "sid": 1, "sentence": "The authors introduce kaleidoscope matrices (K-matrices) and propose to use them as a substitute for structured matrices arising in ML applications (e.g. circulant matrix used for the convolution operation)."}, {"text_id": "S1g74Sfq9H", "sid": 2, "sentence": "The authors prove that K-matrices are expressive enough to capture any structured matrix with near-optimal space and matvec time complexity."}, {"text_id": "S1g74Sfq9H", "sid": 3, "sentence": "The authors demonstrate that learnable K-matrices achieve similar metrics compared to hand-crafted features on speech processing and computer vision tasks, can learn from permuted images, achieve performance close to a CNN trained on unpermuted images and demonstrate the improvement of inference speed of a transformer-based architecture for a machine translation task."}, {"text_id": "S1g74Sfq9H", "sid": 4, "sentence": "Review"}, {"text_id": "S1g74Sfq9H", "sid": 5, "sentence": "The overall quality of the paper is high."}, {"text_id": "S1g74Sfq9H", "sid": 6, "sentence": "The main contribution of the paper is the introduction of a family of matrices called kaleidoscope matrices (or K-matrices) which can be represented as a product of block-diagonal matrices of a special structure."}, {"text_id": "S1g74Sfq9H", "sid": 7, "sentence": "Because of the special structure, the family allows near-optimal time matvec operations with near-optimal space complexity for structured matrices which are commonly used in deep architectures."}, {"text_id": "S1g74Sfq9H", "sid": 8, "sentence": "The proposed approach is novel."}, {"text_id": "S1g74Sfq9H", "sid": 9, "sentence": "It gives a new characterization of sparse matrices with optimal space complexity up to a logarithmic term."}, {"text_id": "S1g74Sfq9H", "sid": 10, "sentence": "Moreover, the proposed characterization is able to learn any structured matrix and matvec time complexity of the K-matrix representation is near-optimal matvec time complexity of the structured matrix."}, {"text_id": "S1g74Sfq9H", "sid": 11, "sentence": "Even though in the worst-case complexity is not optimal, the authors argue that for matrices that are commonly used in machine learning architectures (e.g. circulant matrix in a convolution layer) the characterization is optimal."}, {"text_id": "S1g74Sfq9H", "sid": 12, "sentence": "This results in a new differentiable layer based on a K-matrix that can be trained with the rest of an architecture using standard stochastic gradient methods."}, {"text_id": "S1g74Sfq9H", "sid": 13, "sentence": "However, it is worth noting that the reviewer is not an expert in the field, and it is hard for him to compare the proposed approach with previous work."}, {"text_id": "S1g74Sfq9H", "sid": 14, "sentence": "The paper is generally easy to follow."}, {"text_id": "S1g74Sfq9H", "sid": 15, "sentence": "Even though the introduction of K-matrices requires a lot of definitions, they are presented clearly and Figure 1 helps to understand the concept of K-matrices."}, {"text_id": "S1g74Sfq9H", "sid": 16, "sentence": "The experimental pipeline is also clear."}, {"text_id": "S1g74Sfq9H", "sid": 17, "sentence": "Given the special structure of the family, the reviewer might guess that having K-matrices can slow down the training, i.e. it might require more epochs to achieve the reported results compared to baselines."}, {"text_id": "S1g74Sfq9H", "sid": 18, "sentence": "Providing training plots might increase the quality of the paper."}, {"text_id": "S1g74Sfq9H", "sid": 19, "sentence": "The experimental results are convincing."}, {"text_id": "S1g74Sfq9H", "sid": 20, "sentence": "First, the authors show that K-matrices can be used instead of a handcrafted MFSC featurization in an LSTM-based architecture on the TIMIT speech recognition benchmark with only a 0.4% loss of phoneme error rate."}, {"text_id": "S1g74Sfq9H", "sid": 21, "sentence": "Then, the authors evaluate K-matrices on ImageNet dataset."}, {"text_id": "S1g74Sfq9H", "sid": 22, "sentence": "In order to do so, they compare a lightweight ShuffleNet architecture which uses a handcrafted permutation layer to the same architecture but with a learnable K-matrix instead of the permutation layer."}, {"text_id": "S1g74Sfq9H", "sid": 23, "sentence": "The authors demonstrate the 5% improvement of accuracy over the ShuffleNet with 0.46M parameters with only 0.05M additional parameters of the K-matrix and the 1.2% improvement of accuracy over the ShuffleNet with 2.5M parameters with only 0.2M additional parameters of the K-matrix."}, {"text_id": "S1g74Sfq9H", "sid": 24, "sentence": "Next, the authors show that K-matrices can be used to train permutations in image classification domains."}, {"text_id": "S1g74Sfq9H", "sid": 25, "sentence": "In order to demonstrate so, they take the Permuted CIFAR-10 dataset and ResNet-18 architecture, insert a trainable K-matrix at the beginning of the architecture and compare against ResNet-18 with an inserted FC-layer (attempting to learn the permutation as well) and ResNet-18 trained on the original, unpermuted CIFAR-10 dataset."}, {"text_id": "S1g74Sfq9H", "sid": 26, "sentence": "With K-matrix, the authors achieve a 7.9% accuracy improvement over FC+ResNet-18 and only a 2.4% accuracy drop compared to ResNet-18 trained on the original CIFAR-10."}, {"text_id": "S1g74Sfq9H", "sid": 27, "sentence": "Finally, the authors demonstrate that K-matrices can be used instead of the decoder\u2019s linear layers in a Transformer-based architecture on the IWSLT-14 German-English translation benchmark which allows obtaining 30% speedup of the inference using a model with 25% fewer parameters with 1.0 drop of BLEU score."}, {"text_id": "S1g74Sfq9H", "sid": 28, "sentence": "Overall, the analysis and the empirical evaluations suggest that K-matrices can be a practical tool in modern deep architectures with a variety of potential benefits and tradeoffs between a number of parameters, inference speed and accuracy, and ability to learn complex structures (e.g. permutations)."}, {"text_id": "S1g74Sfq9H", "sid": 29, "sentence": "Improvements"}, {"text_id": "S1g74Sfq9H", "sid": 30, "sentence": "1. Even though K-matrices are aimed at structured matrices, it would be curious either to empirically compare K-matrices to linear transformations in fully-connected networks (i.e. dense matrices) or to provide some theoretical analysis."}, {"text_id": "S1g74Sfq9H", "sid": 31, "sentence": "2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper."}], "reviewlabels": [{"text_id": "S1g74Sfq9H", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "S1g74Sfq9H", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "S1g74Sfq9H", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "S1g74Sfq9H", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "S1g74Sfq9H", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 13, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 17, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 18, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 19, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 20, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 21, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 22, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 23, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 24, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 25, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 26, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 27, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 28, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 29, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 30, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "S1g74Sfq9H", "sid": 31, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rJl51UQ_iH", "sid": 0, "sentence": "We appreciate the reviewer\u2019s positive comments about our work."}, {"text_id": "rJl51UQ_iH", "sid": 1, "sentence": "Regarding the convergence and speed of training, we would like to stress that all hyperparameters for training were kept the same as those for training the default model architecture, other than those we explicitly mentioned as being tuned (e.g. learning rate for the speech experiment)."}, {"text_id": "rJl51UQ_iH", "sid": 2, "sentence": "In particular, for all experiments, the number of epochs is the same for both the baseline approach and the K-matrix approach."}, {"text_id": "rJl51UQ_iH", "sid": 3, "sentence": "Additionally, for the speech preprocessing and ShuffleNet experiments, we compare the total wall-clock training time of our K-matrix approach to that of the baseline approach, in both cases finding that the training time required by our approach is at most 20% longer than that of the baseline approach."}, {"text_id": "rJl51UQ_iH", "sid": 4, "sentence": "In our updated revision, we also include the training time comparison for the DynamicConv model in Appendix B.4.2 (in this case, the modified model with K-matrices actually trains slightly faster than the baseline)."}, {"text_id": "rJl51UQ_iH", "sid": 5, "sentence": "We agree with the reviewer that a training plot can help provide a better understanding of how our proposed approach performs, and therefore have included an example plot (for the ShuffleNet experiment) in our updated revision (in Appendix B.2.3)."}, {"text_id": "rJl51UQ_iH", "sid": 6, "sentence": "Regarding empirical comparisons to dense matrices, in Table 5 (Appendix B.1.2), we compare the use of K-matrices in the raw-features speech model with several other classes of matrices, including dense matrices."}, {"text_id": "rJl51UQ_iH", "sid": 7, "sentence": "We find that, while using a trainable dense matrix slightly outperforms just using the fixed FFT (0.3% drop in test phoneme error rate), using a K-matrix instead of a dense matrix yields a further improvement of 0.8% in the phoneme error rate."}, {"text_id": "rJl51UQ_iH", "sid": 8, "sentence": "Another empirical comparison of K-matrices and dense matrices is in Section 3.3, in which we replace the linear layers in the decoder of a DynamicConv model with K-matrices; these linear layers are by default dense (fully-connected) matrices."}, {"text_id": "rJl51UQ_iH", "sid": 9, "sentence": "Theoretically, in Lemma E.3 we show that arbitrary dense matrices are contained in the BB* hierarchy \u2013 in particular, that any n x n matrix is in (BB*)^{2n-2}, which implies that its K-matrix representation requires at most (4n log n)*(2n-2) = O(n^2 log n) parameters and thus is tight up to a logarithmic factor in n."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rJl51UQ_iH", "sid": 0}, {"labels": {"alignments": [31], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJl51UQ_iH", "sid": 1}, {"labels": {"alignments": [31], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rJl51UQ_iH", "sid": 2}, {"labels": {"alignments": [31], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJl51UQ_iH", "sid": 3}, {"labels": {"alignments": [31], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJl51UQ_iH", "sid": 4}, {"labels": {"alignments": [17, 18], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJl51UQ_iH", "sid": 5}, {"labels": {"alignments": [30], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJl51UQ_iH", "sid": 6}, {"labels": {"alignments": [30], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJl51UQ_iH", "sid": 7}, {"labels": {"alignments": [30], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJl51UQ_iH", "sid": 8}, {"labels": {"alignments": [30], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJl51UQ_iH", "sid": 9}], "metadata": {"anno": "anno3", "review": "S1g74Sfq9H", "rebuttal": "rJl51UQ_iH", "conference": "ICLR2020", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps", "reviewer": "AnonReviewer4", "forum_id": "BkgrBgSYDS", "rating": "8: Accept", "experience_assessment": "I do not know much about this area."}}