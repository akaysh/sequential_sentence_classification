{"review": [{"text_id": "r1g2QHW0h7", "sid": 0, "sentence": "Dual Block-Coordinate Frank-Wolfe (Dual-BCFW) has been widely used in the literature of non-smooth and strongly-convex stochastic optimization problems, such as (structural) Support Vector Machine."}, {"text_id": "r1g2QHW0h7", "sid": 1, "sentence": "To my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network, which employs a proximal-point method that linearizes not the whole loss function but only the DNN (up to the logits) to form a convex subproblem and then deal with the loss part in the dual."}, {"text_id": "r1g2QHW0h7", "sid": 2, "sentence": "The attempt is not perfect (actually with a couple of issues detailed below), but the proposed approach is inspiring and I personally would love it published to encourage more development along this thread."}, {"text_id": "r1g2QHW0h7", "sid": 3, "sentence": "The following points out a couple of items that could probably help further improve the paper."}, {"text_id": "r1g2QHW0h7", "sid": 4, "sentence": "*FW vs BCFW*"}, {"text_id": "r1g2QHW0h7", "sid": 5, "sentence": "The algorithm employed in the paper is actually not Frank-Wolfe (FW) but Block-Coordinate Frank-Wolfe (BCFW), as it minimizes w.r.t. a block of dual variables belonging to the min-batch of samples."}, {"text_id": "r1g2QHW0h7", "sid": 6, "sentence": "*Batch Size*"}, {"text_id": "r1g2QHW0h7", "sid": 7, "sentence": "Though the algorithm can be easily extended to the min-batch case, the author should discuss more how the batch size is interpreted in this case (i.e. minimizing w.r.t. a larger block of dual variables belonging to the batch of samples) and the algorithmic block (Algorithm 1) should be presented in a way reflecting the batch size since this is the way people use an algorithm in practice (to improve the utilization rate of a GPU)."}, {"text_id": "r1g2QHW0h7", "sid": 8, "sentence": "*Convex-Conjugate Loss*"}, {"text_id": "r1g2QHW0h7", "sid": 9, "sentence": "The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss)."}, {"text_id": "r1g2QHW0h7", "sid": 10, "sentence": "All convex loss function can derive a dual formulation based on its convex-conjugate."}, {"text_id": "r1g2QHW0h7", "sid": 11, "sentence": "See [1,2] for examples."}, {"text_id": "r1g2QHW0h7", "sid": 12, "sentence": "It would be more insightful to compare SGD vs dual-BCFW when both of them are optimizing the same loss functions (either hinge loss or cross-entropy loss) in the experimental comparison."}, {"text_id": "r1g2QHW0h7", "sid": 13, "sentence": "[1] Shalev-Shwartz, Shai, and Tong Zhang. \"Stochastic dual coordinate ascent methods for regularized loss minimization.\" JMLR (2013)"}, {"text_id": "r1g2QHW0h7", "sid": 14, "sentence": "[2] Tomioka, Ryota, Taiji Suzuki, and Masashi Sugiyama. \"Super-linear convergence of dual augmented Lagrangian algorithm for sparsity regularized estimation.\" JMLR (2011)."}, {"text_id": "r1g2QHW0h7", "sid": 15, "sentence": "*BCFW vs BCD*"}, {"text_id": "r1g2QHW0h7", "sid": 16, "sentence": "Actually, (Lacoste-Julien, S. et al., 2013) proposes Dual-BCFW to optimize structural SVM because the problem contains exponentially many number of dual variables."}, {"text_id": "r1g2QHW0h7", "sid": 17, "sentence": "For typical multiclass hinge loss problem the Dual Block-Coordinate Descent that minimizes w.r.t. all dual variables of a sample in a closed-form update converges faster without extra computational cost."}, {"text_id": "r1g2QHW0h7", "sid": 18, "sentence": "See the details in, for example, [3, appendix for the multiclass hinge loss case]."}, {"text_id": "r1g2QHW0h7", "sid": 19, "sentence": "[3] Fan, Rong-En, et al. \"LIBLINEAR: A library for large linear classification.\" JMLR (2008)."}, {"text_id": "r1g2QHW0h7", "sid": 20, "sentence": "*Hyper-Parameter*"}, {"text_id": "r1g2QHW0h7", "sid": 21, "sentence": "The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD."}], "reviewlabels": [{"text_id": "r1g2QHW0h7", "sid": 0, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 1, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 7, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 10, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 11, "labels": {"coarse": "Structuring", "fine": "Structuring.Quote", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 13, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 14, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 15, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 16, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 17, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 18, "labels": {"coarse": "Structuring", "fine": "Structuring.Quote", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 19, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 20, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "r1g2QHW0h7", "sid": 21, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "Syei-gKKTm", "sid": 0, "sentence": "We thank the reviewer for their detailed review and for their suggestions."}, {"text_id": "Syei-gKKTm", "sid": 1, "sentence": "We answer point by point:"}, {"text_id": "Syei-gKKTm", "sid": 2, "sentence": "*FW vs BCFW*"}, {"text_id": "Syei-gKKTm", "sid": 3, "sentence": "The (primal) proximal problem is created for a mini-batch of samples, and not for the entire data set (details in section 3.2)."}, {"text_id": "Syei-gKKTm", "sid": 4, "sentence": "In other words, the primal problem consists of the proximal term which encourages proximity to the current iterate, the linearized regularization, and the average over the mini-batch of the losses applied to the linearized model."}, {"text_id": "Syei-gKKTm", "sid": 5, "sentence": "As a result, we can compute the Frank-Wolfe update for all dual coordinates simultaneously, and we do not need to operate in a block-coordinate fashion."}, {"text_id": "Syei-gKKTm", "sid": 6, "sentence": "We have included this clarification in the new version of the paper."}, {"text_id": "Syei-gKKTm", "sid": 7, "sentence": "*"}, {"text_id": "Syei-gKKTm", "sid": 8, "sentence": "Batch-Size*"}, {"text_id": "Syei-gKKTm", "sid": 9, "sentence": "We thank the reviewer for this suggestion."}, {"text_id": "Syei-gKKTm", "sid": 10, "sentence": "We have adapted the description of Algorithm 1 accordingly."}, {"text_id": "Syei-gKKTm", "sid": 11, "sentence": "*Convex-Conjugate Loss*"}, {"text_id": "Syei-gKKTm", "sid": 12, "sentence": "In order to compare the DFW algorithm to the strongest possible baselines, we choose the baselines to use the CE loss in the CIFAR experiments."}, {"text_id": "Syei-gKKTm", "sid": 13, "sentence": "Indeed we have generally found CE to help the baselines in this setting."}, {"text_id": "Syei-gKKTm", "sid": 14, "sentence": "In addition, the hand-designed learning rate schedule of SGD and the l2 regularization were originally tuned for CE."}, {"text_id": "Syei-gKKTm", "sid": 15, "sentence": "In the case of the SNLI data set, we allow the baseline to use either CE or SVM because using the hinge loss can increase their performance."}, {"text_id": "Syei-gKKTm", "sid": 16, "sentence": "Finally, we choose to always employ the multi-class hinge loss for DFW because it gives an optimal step-size in closed form for the dual, which is a key strength of the formulation."}, {"text_id": "Syei-gKKTm", "sid": 17, "sentence": "*BCFW vs BCD*"}, {"text_id": "Syei-gKKTm", "sid": 18, "sentence": "We thank the reviewer for this recommendation."}, {"text_id": "Syei-gKKTm", "sid": 19, "sentence": "It would be interesting indeed to explore how to exploit such updates in the context of the composite minimization framework for deep neural networks."}, {"text_id": "Syei-gKKTm", "sid": 20, "sentence": "In our case, we emphasize that for speed reasons, it is crucial to process the samples within a mini-batch in parallel, and this does not look straightforward with the algorithm in [3, E.3]."}, {"text_id": "Syei-gKKTm", "sid": 21, "sentence": "Therefore we believe that for this setting the FW algorithm permits faster updates thanks to an easy parallelization over the mini-batch on GPU."}, {"text_id": "Syei-gKKTm", "sid": 22, "sentence": "*Hyper-parameter*"}, {"text_id": "Syei-gKKTm", "sid": 23, "sentence": "Counting a single hyper-parameter for SGD implicitly assumes that SGD can employ a constant step-size."}, {"text_id": "Syei-gKKTm", "sid": 24, "sentence": "Using such a constant step-size for SGD would incur a significant loss of performance (e.g. at least a few percents on the CIFAR data set)."}, {"text_id": "Syei-gKKTm", "sid": 25, "sentence": "Therefore in order to obtain good performance, SGD requires a manual schedule of the learning rate, which involves many hyper-parameters to tune in practice."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 0}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 1}, {"labels": {"alignments": [4], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 2}, {"labels": {"alignments": [5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Syei-gKKTm", "sid": 3}, {"labels": {"alignments": [5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Syei-gKKTm", "sid": 4}, {"labels": {"alignments": [5], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Syei-gKKTm", "sid": 5}, {"labels": {"alignments": [5], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 6}, {"labels": {"alignments": [6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 7}, {"labels": {"alignments": [6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 8}, {"labels": {"alignments": [6], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 9}, {"labels": {"alignments": [6], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 10}, {"labels": {"alignments": [8], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 11}, {"labels": {"alignments": [9, 10, 11, 12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 12}, {"labels": {"alignments": [9, 10, 11, 12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 13}, {"labels": {"alignments": [9, 10, 11, 12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 14}, {"labels": {"alignments": [9, 10, 11, 12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 15}, {"labels": {"alignments": [9, 10, 11, 12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 16}, {"labels": {"alignments": [15], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 17}, {"labels": {"alignments": [16, 17, 18], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 18}, {"labels": {"alignments": [16, 17, 18], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 19}, {"labels": {"alignments": [16, 17, 18], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 20}, {"labels": {"alignments": [16, 17, 18], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Syei-gKKTm", "sid": 21}, {"labels": {"alignments": [20], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Syei-gKKTm", "sid": 22}, {"labels": {"alignments": [21], "responsetype": "contradict-assertion", "coarseresponse": "dispute"}, "text_id": "Syei-gKKTm", "sid": 23}, {"labels": {"alignments": [21], "responsetype": "contradict-assertion", "coarseresponse": "dispute"}, "text_id": "Syei-gKKTm", "sid": 24}, {"labels": {"alignments": [21], "responsetype": "contradict-assertion", "coarseresponse": "dispute"}, "text_id": "Syei-gKKTm", "sid": 25}], "metadata": {"anno": "anno0", "review": "r1g2QHW0h7", "rebuttal": "Syei-gKKTm", "conference": "ICLR2019", "title": "Deep Frank-Wolfe For Neural Network Optimization", "reviewer": "AnonReviewer1", "forum_id": "SyVU6s05K7", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}