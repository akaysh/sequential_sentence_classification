{"review": [{"text_id": "BJxVfiNqhm", "sid": 0, "sentence": "(As a disclamer I want to point out I'm not an expert in GANs and have only a basic understanding of the sub-field, but arguably this would make me target audience of this paper)."}, {"text_id": "BJxVfiNqhm", "sid": 1, "sentence": "The authors presents a large scale study comparing a large number of GAN experiments, in this study they compare various choices of architechtures, losses and hyperparameters."}, {"text_id": "BJxVfiNqhm", "sid": 2, "sentence": "The first part of the paper describes the various losses, architectures, regularization and normalization schemes; and the second part describes the results of the comparison experiments."}, {"text_id": "BJxVfiNqhm", "sid": 3, "sentence": "While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs."}, {"text_id": "BJxVfiNqhm", "sid": 4, "sentence": "As far I can see the most important take home message of the paper can be summarized in \"one should consider non-saturating GAN loss and spectral normalization as default choices [...] Given additional computational budget, we suggest adding"}, {"text_id": "BJxVfiNqhm", "sid": 5, "sentence": "the"}, {"text_id": "BJxVfiNqhm", "sid": 6, "sentence": "gradient penalty [...] and train the model until convergence\"."}, {"text_id": "BJxVfiNqhm", "sid": 7, "sentence": "Pros:"}, {"text_id": "BJxVfiNqhm", "sid": 8, "sentence": "- available source code"}, {"text_id": "BJxVfiNqhm", "sid": 9, "sentence": "- large number of experiments"}, {"text_id": "BJxVfiNqhm", "sid": 10, "sentence": "Cons:"}, {"text_id": "BJxVfiNqhm", "sid": 11, "sentence": "- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show"}, {"text_id": "BJxVfiNqhm", "sid": 12, "sentence": "- not clear what the target audience of the first part (section 2) is, it is too technical for a survey intended for outsiders, and discusses subtle points that are not easy to understand without more knowledge, but at the same time seems unlikely to give additional insight to an insider"}, {"text_id": "BJxVfiNqhm", "sid": 13, "sentence": "- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type"}, {"text_id": "BJxVfiNqhm", "sid": 14, "sentence": "Some suggestions that I think could make the paper stronger"}, {"text_id": "BJxVfiNqhm", "sid": 15, "sentence": "- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot."}, {"text_id": "BJxVfiNqhm", "sid": 16, "sentence": "I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it."}, {"text_id": "BJxVfiNqhm", "sid": 17, "sentence": "I would leave out some of the details, shortening the whole sections, and focus more on making a few of the concepts more understandable, and potentially leaving more space for a clearer description of the results"}, {"text_id": "BJxVfiNqhm", "sid": 18, "sentence": "- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?"}, {"text_id": "BJxVfiNqhm", "sid": 19, "sentence": "- \"the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once\"? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily."}, {"text_id": "BJxVfiNqhm", "sid": 20, "sentence": "It should probably be rephrased"}, {"text_id": "BJxVfiNqhm", "sid": 21, "sentence": "- at the start of section 3: what is an \"experiment\"?"}, {"text_id": "BJxVfiNqhm", "sid": 22, "sentence": "- in 3.1 towards the end of the first paragraph, what is a \"study\", is that the same as experiment or something different?"}, {"text_id": "BJxVfiNqhm", "sid": 23, "sentence": "- (minor) stating that lower is better in the graphs might be useful"}, {"text_id": "BJxVfiNqhm", "sid": 24, "sentence": "- (minor) typo in page 5 \"We use a fixed the number\""}], "reviewlabels": [{"text_id": "BJxVfiNqhm", "sid": 0, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [{"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "BJxVfiNqhm", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "BJxVfiNqhm", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "BJxVfiNqhm", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 10, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 14, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "BJxVfiNqhm", "sid": 17, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "BJxVfiNqhm", "sid": 18, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 19, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 20, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 21, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 22, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 23, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "BJxVfiNqhm", "sid": 24, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "H1e5hKOdTm", "sid": 0, "sentence": "Thank you for the comments, please find our responses to specific points below."}, {"text_id": "H1e5hKOdTm", "sid": 1, "sentence": "[Q] \u201cAs far as I can see the most important take home message of the paper can be summarized in \"one should consider non-saturating GAN loss and spectral normalization as default choices [...] Given additional computational budget, we suggest adding the gradient penalty [...] and train the model until convergence.\""}, {"text_id": "H1e5hKOdTm", "sid": 2, "sentence": "[A] While we want this study to be approachable by non-experts, some level of formalism is required as our main audience are researchers working on or interested in GANs."}, {"text_id": "H1e5hKOdTm", "sid": 3, "sentence": "The summary you provided is indeed correct -- coupled with our open-sourced code, it allows a non-expert to train a GAN with state-of-the-art methods without needing to understand the details."}, {"text_id": "H1e5hKOdTm", "sid": 4, "sentence": "On the other hand, for more experienced researchers, we provide more details on which design choices generalize to new settings and identify the biggest obstacles towards fair and unbiased quantitative evaluation of generative models."}, {"text_id": "H1e5hKOdTm", "sid": 5, "sentence": "[Q] Limited amount of new insight."}, {"text_id": "H1e5hKOdTm", "sid": 6, "sentence": "[A] Our paper presents many useful insights, namely: NS-GAN performs well, spectral norm is a good default normalization technique, gradient penalty should also be considered, even in combination with spectral norm but will cost substantially more in terms of computational resources, popular metrics such as KID and FID result in the same relative ordering of the models so there is no point in computing both, most Resnet tricks do not matter, etc."}, {"text_id": "H1e5hKOdTm", "sid": 7, "sentence": "All of these insights are supported by a fair and unbiased rigorous experimental process."}, {"text_id": "H1e5hKOdTm", "sid": 8, "sentence": "On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models."}, {"text_id": "H1e5hKOdTm", "sid": 9, "sentence": "[Q] Clarification and exposition of plots."}, {"text_id": "H1e5hKOdTm", "sid": 10, "sentence": "[A] Say that you had access to a GPU and had to train a model (loss+penalty+architecture)."}, {"text_id": "H1e5hKOdTm", "sid": 11, "sentence": "How many hyperparameter settings would you need to consider to achieve a certain quality?"}, {"text_id": "H1e5hKOdTm", "sid": 12, "sentence": "The FID from the plot is the estimate of the min FID computed by bootstrap estimation and the line-plots show this relationship."}, {"text_id": "H1e5hKOdTm", "sid": 13, "sentence": "In other words, given a computing budget, which model should you pick?"}, {"text_id": "H1e5hKOdTm", "sid": 14, "sentence": "We will provide additional details in the caption of the plot."}, {"text_id": "H1e5hKOdTm", "sid": 15, "sentence": "[Q] Bayesian optimization and variance."}, {"text_id": "H1e5hKOdTm", "sid": 16, "sentence": "[A] We agree and will provide more details."}, {"text_id": "H1e5hKOdTm", "sid": 17, "sentence": "When the sequential Bayesian optimization chooses the next set of hyperparameter combinations to test we run the model once (per hyperparameter combination) and report the scores to the optimizer."}, {"text_id": "H1e5hKOdTm", "sid": 18, "sentence": "Then, the optimization algorithm takes these scores into account when selecting the next set of hyperparameters."}, {"text_id": "H1e5hKOdTm", "sid": 19, "sentence": "The algorithm itself trades-off exploration and exploitation and it can explore hyperparameters \"close\" to the existing ones if they seem promising."}, {"text_id": "H1e5hKOdTm", "sid": 20, "sentence": "Hence, the averaging happens implicitly during the search."}, {"text_id": "H1e5hKOdTm", "sid": 21, "sentence": "[Q]: Studies and experiments. Stating that lower is better in the plots."}, {"text_id": "H1e5hKOdTm", "sid": 22, "sentence": "[A]: Study is a set of experiments (say a study on the impact of the loss)."}, {"text_id": "H1e5hKOdTm", "sid": 23, "sentence": "Experiment is a concrete run with certain hyperparameters."}, {"text_id": "H1e5hKOdTm", "sid": 24, "sentence": "Stating lower is better is a good idea, we will add this to the captions."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1e5hKOdTm", "sid": 0}, {"labels": {"alignments": [4, 5, 6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1e5hKOdTm", "sid": 1}, {"labels": {"alignments": [4, 5, 6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 2}, {"labels": {"alignments": [4, 5, 6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1e5hKOdTm", "sid": 3}, {"labels": {"alignments": [4, 5, 6], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1e5hKOdTm", "sid": 4}, {"labels": {"alignments": [3, 13], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1e5hKOdTm", "sid": 5}, {"labels": {"alignments": [3, 13], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1e5hKOdTm", "sid": 6}, {"labels": {"alignments": [3, 13], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1e5hKOdTm", "sid": 7}, {"labels": {"alignments": [3, 13], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "H1e5hKOdTm", "sid": 8}, {"labels": {"alignments": [11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1e5hKOdTm", "sid": 9}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 10}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 11}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 12}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 13}, {"labels": {"alignments": [11], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 14}, {"labels": {"alignments": [19, 20], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1e5hKOdTm", "sid": 15}, {"labels": {"alignments": [19, 20], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 16}, {"labels": {"alignments": [19, 20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 17}, {"labels": {"alignments": [19, 20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 18}, {"labels": {"alignments": [19, 20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 19}, {"labels": {"alignments": [19, 20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 20}, {"labels": {"alignments": [23], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1e5hKOdTm", "sid": 21}, {"labels": {"alignments": [23], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 22}, {"labels": {"alignments": [23], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 23}, {"labels": {"alignments": [23], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "H1e5hKOdTm", "sid": 24}], "metadata": {"anno": "anno3", "review": "BJxVfiNqhm", "rebuttal": "H1e5hKOdTm", "conference": "ICLR2019", "title": "The GAN Landscape: Losses, Architectures, Regularization, and Normalization", "reviewer": "AnonReviewer1", "forum_id": "rkGG6s0qKQ", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}