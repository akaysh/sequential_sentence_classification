{"review": [{"text_id": "HkeGURdb6X", "sid": 0, "sentence": "Summary: This paper introduces a new Neural Network training procedure, designed for tabular data, that seeks to leverage feature clusters extracted from GBDTs."}, {"text_id": "HkeGURdb6X", "sid": 1, "sentence": "Strengths: The idea of leveraging feature groups in a neural network structure; the novelty of the RESE model;"}, {"text_id": "HkeGURdb6X", "sid": 2, "sentence": "Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT."}, {"text_id": "HkeGURdb6X", "sid": 3, "sentence": "Minor typos:"}, {"text_id": "HkeGURdb6X", "sid": 4, "sentence": "(abstract)"}, {"text_id": "HkeGURdb6X", "sid": 5, "sentence": "- \"NN has achieved\" => \"Neural Networks have achieved\""}, {"text_id": "HkeGURdb6X", "sid": 6, "sentence": "- \"performances\" => performance"}, {"text_id": "HkeGURdb6X", "sid": 7, "sentence": "- \"explicitly leverages\" => \"explicitly leverage\""}, {"text_id": "HkeGURdb6X", "sid": 8, "sentence": "Questions:"}, {"text_id": "HkeGURdb6X", "sid": 9, "sentence": "- (top of p. 2) What exactly is the difference between \"implicit feature combinations\" and \"explicit (?), expressive feature combinations\""}, {"text_id": "HkeGURdb6X", "sid": 10, "sentence": "- (top of p. 2) \"encourage parameter sharing\" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]"}, {"text_id": "HkeGURdb6X", "sid": 11, "sentence": "- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN."}, {"text_id": "HkeGURdb6X", "sid": 12, "sentence": "Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?"}], "reviewlabels": [{"text_id": "HkeGURdb6X", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 1, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 6, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 7, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 9, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HkeGURdb6X", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "HJxBzN0-Tm", "sid": 0, "sentence": "Thanks for your efforts in reviewing our paper and the valuable comments."}, {"text_id": "HJxBzN0-Tm", "sid": 1, "sentence": "We attempt to address your concerns in the following."}, {"text_id": "HJxBzN0-Tm", "sid": 2, "sentence": "1. Response to the \"Weaknesses\" part and the comparison with GBDT"}, {"text_id": "HJxBzN0-Tm", "sid": 3, "sentence": "As stated in the response to review 1, our goal is not inventing a model to beat GBDT but developing a model to cover the scenarios not suitable for GBDT such as some applications need online updating."}, {"text_id": "HJxBzN0-Tm", "sid": 4, "sentence": "\"The next contender\" model in your comment is the GBDT, which indeed works well for tabular data."}, {"text_id": "HJxBzN0-Tm", "sid": 5, "sentence": "However, GBDT suffers from two shortages, as stated in Section 2 and the responses to reviewer 3."}, {"text_id": "HJxBzN0-Tm", "sid": 6, "sentence": "These 2 shortages make GBDT very hard to be used in many real-world scenarios."}, {"text_id": "HJxBzN0-Tm", "sid": 7, "sentence": "For example, in an online recommender system, we need to update the model frequently to achieve the satisfying real-time performance."}, {"text_id": "HJxBzN0-Tm", "sid": 8, "sentence": "In this case, GBDT will be very inefficient as it needs to be re-trained from scratch."}, {"text_id": "HJxBzN0-Tm", "sid": 9, "sentence": "In contrast, NN can be learned by mini-batch fashion and therefore can learn from streaming data naturally."}, {"text_id": "HJxBzN0-Tm", "sid": 10, "sentence": "The proposed TabNN can overcome these shortages and achieve comparable accuracy with GBDT."}, {"text_id": "HJxBzN0-Tm", "sid": 11, "sentence": "Moreover, compared with previous NN based solutions for tabular data, TabNN outperforms them significantly."}, {"text_id": "HJxBzN0-Tm", "sid": 12, "sentence": "Therefore, TabNN is a better general solution for tabular data as it can cover more scenarios."}, {"text_id": "HJxBzN0-Tm", "sid": 13, "sentence": "2. Difference between \"implicit feature combinations\" and \"explicit feature combinations\""}, {"text_id": "HJxBzN0-Tm", "sid": 14, "sentence": "The main difference lies in whether the feature combination information is explicitly introduced into model structure or not."}, {"text_id": "HJxBzN0-Tm", "sid": 15, "sentence": "For example, in FCNN, as all features are connected to the neurons in the next layer, there are no feature combination information in the model structure."}, {"text_id": "HJxBzN0-Tm", "sid": 16, "sentence": "Although the feature combination information are not explicitly provided, one neuron in FCNN can learn a linear combination of its input features."}, {"text_id": "HJxBzN0-Tm", "sid": 17, "sentence": "Thus, we say there are \"implicit feature combinations\" in FCNN."}, {"text_id": "HJxBzN0-Tm", "sid": 18, "sentence": "In TabNN, we leverage GBDT to find feature combinations and then construct model structure according to them."}, {"text_id": "HJxBzN0-Tm", "sid": 19, "sentence": "Thus, we say there are \"explicit feature combinations\" in TabNN."}, {"text_id": "HJxBzN0-Tm", "sid": 20, "sentence": "\"Implicit feature combinations\" is not efficient as it introduces much more trainable parameters, and has a risk of over-fitting."}, {"text_id": "HJxBzN0-Tm", "sid": 21, "sentence": "In contrast, \"explicit feature combinations\" let model focus on the more important feature combinations and is more efficient."}, {"text_id": "HJxBzN0-Tm", "sid": 22, "sentence": "The successful CNN model also uses \"explicit feature combinations\", as it only combines the local pixels."}, {"text_id": "HJxBzN0-Tm", "sid": 23, "sentence": "3. About \"encourage parameter sharing\"."}, {"text_id": "HJxBzN0-Tm", "sid": 24, "sentence": "Yes, we use parameter sharing in the one cluster of feature groups."}, {"text_id": "HJxBzN0-Tm", "sid": 25, "sentence": "We will clarify this in the paper."}, {"text_id": "HJxBzN0-Tm", "sid": 26, "sentence": "4. Benefits brought by the \"Structural Knowledge\""}, {"text_id": "HJxBzN0-Tm", "sid": 27, "sentence": "We had compared the benefit brought by the 'Structural Knowledge' in the experiment."}, {"text_id": "HJxBzN0-Tm", "sid": 28, "sentence": "The difference between TabNN (S) and TabNN (R), as shown in Table 3, implies that that the structural knowledge from GBDT yields a large contribution to the performance of TabNN."}, {"text_id": "HJxBzN0-Tm", "sid": 29, "sentence": "The \"Structural Knowledge\" is in TabNN by default. We will clarify this in the paper."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HJxBzN0-Tm", "sid": 0}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HJxBzN0-Tm", "sid": 1}, {"labels": {"alignments": [2], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJxBzN0-Tm", "sid": 2}, {"labels": {"alignments": [2], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJxBzN0-Tm", "sid": 3}, {"labels": {"alignments": [2], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJxBzN0-Tm", "sid": 4}, {"labels": {"alignments": [2], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJxBzN0-Tm", "sid": 5}, {"labels": {"alignments": [2], "responsetype": "reject-request_scope_No", "coarseresponse": "dispute"}, "text_id": "HJxBzN0-Tm", "sid": 6}, {"labels": {"alignments": [2], "responsetype": "reject-request_scope_No", "coarseresponse": "dispute"}, "text_id": "HJxBzN0-Tm", "sid": 7}, {"labels": {"alignments": [2], "responsetype": "reject-request_scope_No", "coarseresponse": "dispute"}, "text_id": "HJxBzN0-Tm", "sid": 8}, {"labels": {"alignments": [2], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJxBzN0-Tm", "sid": 9}, {"labels": {"alignments": [2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 10}, {"labels": {"alignments": [2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 11}, {"labels": {"alignments": [2], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 12}, {"labels": {"alignments": [9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJxBzN0-Tm", "sid": 13}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 14}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 15}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 16}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 17}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 18}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 19}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 20}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 21}, {"labels": {"alignments": [9], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 22}, {"labels": {"alignments": [10], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJxBzN0-Tm", "sid": 23}, {"labels": {"alignments": [10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 24}, {"labels": {"alignments": [10], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 25}, {"labels": {"alignments": [11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "HJxBzN0-Tm", "sid": 26}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 27}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 28}, {"labels": {"alignments": [11], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "HJxBzN0-Tm", "sid": 29}], "metadata": {"anno": "anno0", "review": "HkeGURdb6X", "rebuttal": "HJxBzN0-Tm", "conference": "ICLR2019", "title": "TabNN: A Universal Neural Network Solution for Tabular Data", "reviewer": "AnonReviewer2", "forum_id": "r1eJssCqY7", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}