{"review": [{"text_id": "B1lJvAAcnX", "sid": 0, "sentence": "This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution that can be computed in closed form without drawing samples from the prior (but only when it is Gaussian)."}, {"text_id": "B1lJvAAcnX", "sid": 1, "sentence": "The primary contribution is the new CW statistical distance, which is the l2 distance between projected distributions, integrated over all possible projections (although not calculated as so in practice)."}, {"text_id": "B1lJvAAcnX", "sid": 2, "sentence": "Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance."}, {"text_id": "B1lJvAAcnX", "sid": 3, "sentence": "Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score)."}, {"text_id": "B1lJvAAcnX", "sid": 4, "sentence": "Some potential options include:"}, {"text_id": "B1lJvAAcnX", "sid": 5, "sentence": "1) Faster training times."}, {"text_id": "B1lJvAAcnX", "sid": 6, "sentence": "It seems to me one potential advantage of the closed-form distance would be that the stochastic WAE-optimization can converge faster (due to lower-variance gradients)."}, {"text_id": "B1lJvAAcnX", "sid": 7, "sentence": "However, the authors only presented per-batch processing times as opposed to overall training time for these models."}, {"text_id": "B1lJvAAcnX", "sid": 8, "sentence": "2) Stabler training."}, {"text_id": "B1lJvAAcnX", "sid": 9, "sentence": "Perhaps sampling from the prior (as needed to compute statistical distances in the other WAE variants) introduces undesirable extra variance in the training procedure."}, {"text_id": "B1lJvAAcnX", "sid": 10, "sentence": "The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results."}, {"text_id": "B1lJvAAcnX", "sid": 11, "sentence": "3) Usefulness of the CW distance outside of the autoencoder context."}, {"text_id": "B1lJvAAcnX", "sid": 12, "sentence": "Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE)."}, {"text_id": "B1lJvAAcnX", "sid": 13, "sentence": "Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?"}, {"text_id": "B1lJvAAcnX", "sid": 14, "sentence": "Without demonstrating any practical advance, this work becomes simply another one of the multitude of V/W-AE-variants that already exist."}, {"text_id": "B1lJvAAcnX", "sid": 15, "sentence": "Other Comments:"}, {"text_id": "B1lJvAAcnX", "sid": 16, "sentence": "- While I agree that standard WAE-MMD and SWAE require some form of sampling to compute their respective statistical distance, a variant of WAE-MMD could be converted to a closed form statistical distance in the case of a Gaussian prior, by way of Stein's method or other existing goodness-of-fit measures designed specifically for Gaussians."}, {"text_id": "B1lJvAAcnX", "sid": 17, "sentence": "See for example:"}, {"text_id": "B1lJvAAcnX", "sid": 18, "sentence": "Chwialkowski et al: https://arxiv.org/pdf/1602.02964.pdf"}, {"text_id": "B1lJvAAcnX", "sid": 19, "sentence": "which like CW-distance is also a quadratic-time closed-form distance between samples and a target density."}, {"text_id": "B1lJvAAcnX", "sid": 20, "sentence": "Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives."}, {"text_id": "B1lJvAAcnX", "sid": 21, "sentence": "- Silverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?"}, {"text_id": "B1lJvAAcnX", "sid": 22, "sentence": "After reading the revision: I have raised my score by 1 point and recommend acceptance."}], "reviewlabels": [{"text_id": "B1lJvAAcnX", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 2, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 3, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 6, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 9, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 11, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 12, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 15, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 16, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 17, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 18, "labels": {"coarse": "Other", "fine": "Other", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 19, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 20, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 21, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "B1lJvAAcnX", "sid": 22, "labels": {"coarse": "Social", "fine": "Social", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "r1e16X4HAm", "sid": 0, "sentence": "The reviewer observed that \u201cauthors need to highlight at least one practical advance introduced by the CW distance\u201d and suggested the following potential options:"}, {"text_id": "r1e16X4HAm", "sid": 1, "sentence": "1) Faster training times."}, {"text_id": "r1e16X4HAm", "sid": 2, "sentence": "2) Stabler training."}, {"text_id": "r1e16X4HAm", "sid": 3, "sentence": "3) Usefulness of the CW distance outside of the autoencoder context."}, {"text_id": "r1e16X4HAm", "sid": 4, "sentence": "Ad. 1) (faster training) The experiments show, that CWAE model approaches best generalization, measured with the FID score, much more rapidly, than it is the case with WAE or SWAE models."}, {"text_id": "r1e16X4HAm", "sid": 5, "sentence": "E.g., when trained on the CelebA problem, the FID-score in case of CWAE drops below 100 after only about 75 batches, while for the WAE model only near 400 batches, it does so."}, {"text_id": "r1e16X4HAm", "sid": 6, "sentence": "The same applies to SWAE."}, {"text_id": "r1e16X4HAm", "sid": 7, "sentence": "Needless to say, the FID score for CWAE is near a common best value (after about 500 epochs) of about 95 (these are results are for a DeConv encoder-decoder architecture, see. Appendix E for details; for a direct comparison with Tolstikhin at al\u2019s paper results for an identical architecture to theirs are given in Table 1 in the paper) after a much shorter processing time."}, {"text_id": "r1e16X4HAm", "sid": 8, "sentence": "This is both thanks to the quicker convergence, but also due to faster batch processing (as it was shown in the paper)."}, {"text_id": "r1e16X4HAm", "sid": 9, "sentence": "The MMD-like cloud-to-cloud formula for CW-distance (see equation (3) in the paper) is much more cumbersome than the actual one cloud-to-distribution used in the experiments derived in the paper and shown in equation at page 5 of the paper."}, {"text_id": "r1e16X4HAm", "sid": 10, "sentence": "The proposed Cramer-Wold kernel behaves correctly."}, {"text_id": "r1e16X4HAm", "sid": 11, "sentence": "We have added graphs describing this to the paper, exchanging those on page 8 (as the new are much more clearer)."}, {"text_id": "r1e16X4HAm", "sid": 12, "sentence": "Graphs comparing CWAE, WAE and SWAE learning, on both CelebA and CIFAR10 datasets, shall be added to the Appendix."}, {"text_id": "r1e16X4HAm", "sid": 13, "sentence": "Ad. 2) (stable training) We have run repeated experiments with different initializations for all the generative models, as the reviewer has suggested."}, {"text_id": "r1e16X4HAm", "sid": 14, "sentence": "All experiments show that CWAE learning process is stable and repetitive: the standard deviations, for most of the coefficients computed during training are smaller than those of WAE or SWAE models (in particular CWAE minimizes WAE distance faster then WAE-MMD)."}, {"text_id": "r1e16X4HAm", "sid": 15, "sentence": "We have added appropriate graphs to the paper."}, {"text_id": "r1e16X4HAm", "sid": 16, "sentence": "Ad. 3) (CW usefulness) We have verified how the Cramer-Wold metric works as a Gaussian goodness of fit,"}, {"text_id": "r1e16X4HAm", "sid": 17, "sentence": "however, the results were not satisfactory."}, {"text_id": "r1e16X4HAm", "sid": 18, "sentence": "The tests based on Cramer-Wold metric were, in general, in the middle of compared tests (Mardia, Henze-Zirkler and Royston tests)."}, {"text_id": "r1e16X4HAm", "sid": 19, "sentence": "We doubt it can be efficiently applied in this direction."}, {"text_id": "r1e16X4HAm", "sid": 20, "sentence": "However, since Cramer-Wold metric is defined by characteristic kernel, it can be applied in the large field of kernel-based methods in machine learning (where its particular advantage lies in the fact that it can be efficiently computed for the mixture of radial Gaussians)."}, {"text_id": "r1e16X4HAm", "sid": 21, "sentence": "The reviewer noted that \u201cbesides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives."}, {"text_id": "r1e16X4HAm", "sid": 22, "sentence": "In our opinion sliced approach works well for neural networks, as the neural networks see/process data by applying similar one dimensional projections."}, {"text_id": "r1e16X4HAm", "sid": 23, "sentence": "Also the success of neural networks based on the classical activation functions, as compared to RBF networks, supports this."}, {"text_id": "r1e16X4HAm", "sid": 24, "sentence": "Concerning the closed-form, Cramer-Wold kernel is the only known to the authors, which is given by the sliced approach and has a closed form for radial gaussians."}, {"text_id": "r1e16X4HAm", "sid": 25, "sentence": "The reviewer also noted, that \u201cSilverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?\u201d."}, {"text_id": "r1e16X4HAm", "sid": 26, "sentence": "In our opinion the model works well due to the fact that we compare it to the Gaussian N(0,I), where the Silverman\u2019s kernel is optimal."}, {"text_id": "r1e16X4HAm", "sid": 27, "sentence": "However, if the prior in general would not be standard Gaussian, the situation could possibly be different."}], "rebuttallabels": [{"labels": {"alignments": [3], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 0}, {"labels": {"alignments": [5], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 1}, {"labels": {"alignments": [8], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 2}, {"labels": {"alignments": [11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 3}, {"labels": {"alignments": [5, 6, 7], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 4}, {"labels": {"alignments": [5, 6, 7], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 5}, {"labels": {"alignments": [5, 6, 7], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 6}, {"labels": {"alignments": [5, 6, 7], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 7}, {"labels": {"alignments": [5, 6, 7], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 8}, {"labels": {"alignments": [5, 6, 7], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 9}, {"labels": {"alignments": [5, 6, 7], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 10}, {"labels": {"alignments": [5, 6, 7], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 11}, {"labels": {"alignments": [5, 6, 7], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 12}, {"labels": {"alignments": [8, 9, 10], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 13}, {"labels": {"alignments": [8, 9, 10], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 14}, {"labels": {"alignments": [8, 9, 10], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 15}, {"labels": {"alignments": [11, 12, 13], "responsetype": "done_manu_No", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 16}, {"labels": {"alignments": [11, 12, 13], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 17}, {"labels": {"alignments": [11, 12, 13], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 18}, {"labels": {"alignments": [11, 12, 13], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 19}, {"labels": {"alignments": [11, 12, 13], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 20}, {"labels": {"alignments": [20], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 21}, {"labels": {"alignments": [20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 22}, {"labels": {"alignments": [20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 23}, {"labels": {"alignments": [20], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 24}, {"labels": {"alignments": [21], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "r1e16X4HAm", "sid": 25}, {"labels": {"alignments": [21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 26}, {"labels": {"alignments": [21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "r1e16X4HAm", "sid": 27}], "metadata": {"anno": "anno10", "review": "B1lJvAAcnX", "rebuttal": "r1e16X4HAm", "conference": "ICLR2019", "title": "Cramer-Wold AutoEncoder", "reviewer": "AnonReviewer2", "forum_id": "rkgwuiA9F7", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}