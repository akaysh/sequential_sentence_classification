{"review": [{"text_id": "rJggAvorhQ", "sid": 0, "sentence": "This paper attempts to mitigate catastrophic problem in continual learning."}, {"text_id": "rJggAvorhQ", "sid": 1, "sentence": "Different from the previous works where episodic memory is used, this work adopts the generative replay strategy and improve the work in (Serra et al., 2018) by extending the output neurons of generative network when facing the significant domain shift between tasks."}, {"text_id": "rJggAvorhQ", "sid": 2, "sentence": "Here are my detailed comments:"}, {"text_id": "rJggAvorhQ", "sid": 3, "sentence": "Catastrophic problem is the most severe problem in continual learning since when learning more and more new tasks, the classifier will forget what they learned before, which will be no longer an effective continual learning model."}, {"text_id": "rJggAvorhQ", "sid": 4, "sentence": "Considering that episodic memory will cost too much space, this work adopts the generative replay strategy where old representative data are generated by a generative model."}, {"text_id": "rJggAvorhQ", "sid": 5, "sentence": "Thus, at every time step, the model will receive data from every task so that its performance on old tasks will retain."}, {"text_id": "rJggAvorhQ", "sid": 6, "sentence": "However, if the differences between tasks are significant, the generator cannot reserve vacant neurons for new tasks or in other words, the generator will forget the old information from old tasks when overwritten by information from new tasks."}, {"text_id": "rJggAvorhQ", "sid": 7, "sentence": "Therefore, this work tries to tackle this problem by extending the output neurons of the generator to keep vacant neurons to retain receive new information."}, {"text_id": "rJggAvorhQ", "sid": 8, "sentence": "As far as I am concerned, this is the main contribution of this work."}, {"text_id": "rJggAvorhQ", "sid": 9, "sentence": "Nevertheless, I think there are some deficiencies in this work."}, {"text_id": "rJggAvorhQ", "sid": 10, "sentence": "First, this paper is not easy to follow."}, {"text_id": "rJggAvorhQ", "sid": 11, "sentence": "The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."}, {"text_id": "rJggAvorhQ", "sid": 12, "sentence": "For example, in Section 4.1, I am not sure the equation (3), (4), (5), (6) are the contributions of this paper or not since a large number of citations appear."}, {"text_id": "rJggAvorhQ", "sid": 13, "sentence": "Second, the authors mention that to avoid storing previous data, they adopt generative replay and continuously enlarge the generator to tackle the significant domain shift between tasks."}, {"text_id": "rJggAvorhQ", "sid": 14, "sentence": "However, in this way, when more and more tasks come, the generator will become larger and larger."}, {"text_id": "rJggAvorhQ", "sid": 15, "sentence": "The storing problem still exists."}, {"text_id": "rJggAvorhQ", "sid": 16, "sentence": "Generative replay also brings the time complexity problem since it is time consuming to generate previous data."}, {"text_id": "rJggAvorhQ", "sid": 17, "sentence": "Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method."}, {"text_id": "rJggAvorhQ", "sid": 18, "sentence": "Third, the datasets used in this paper are rather limited."}, {"text_id": "rJggAvorhQ", "sid": 19, "sentence": "Three datasets cannot make the experiments convincing."}, {"text_id": "rJggAvorhQ", "sid": 20, "sentence": "In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10."}, {"text_id": "rJggAvorhQ", "sid": 21, "sentence": "I hope the author could explain this phenomenon."}, {"text_id": "rJggAvorhQ", "sid": 22, "sentence": "Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent."}, {"text_id": "rJggAvorhQ", "sid": 23, "sentence": "Fourth, there are some grammar mistakes and typos."}, {"text_id": "rJggAvorhQ", "sid": 24, "sentence": "For example, there are two \"the\" in the end of the third paragraph in Related Work."}, {"text_id": "rJggAvorhQ", "sid": 25, "sentence": "In the last paragraph in Related Work, \"provide\" should be \"provides\"."}, {"text_id": "rJggAvorhQ", "sid": 26, "sentence": "In page 8, the double quotation marks of \"short-term\" are not correct."}, {"text_id": "rJggAvorhQ", "sid": 27, "sentence": "Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios."}, {"text_id": "rJggAvorhQ", "sid": 28, "sentence": "The proposed method is also heuristic and lacks promising guarantee."}], "reviewlabels": [{"text_id": "rJggAvorhQ", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 1, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 3, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 5, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 6, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Quote", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 8, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 9, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 13, "labels": {"coarse": "Structuring", "fine": "Structuring.Quote", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 17, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 18, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 19, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 20, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 21, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Meaningful Comparison", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 22, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 23, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 24, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 25, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 26, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 27, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "rJggAvorhQ", "sid": 28, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rJgfByXdRQ", "sid": 0, "sentence": "1. We first want to point out the main contributions of the paper."}, {"text_id": "rJgfByXdRQ", "sid": 1, "sentence": "First, we address the catastrophic forgetting problem in continual learning."}, {"text_id": "rJgfByXdRQ", "sid": 2, "sentence": "Thereby we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative network endowed with neuronal plasticity through efficient learning of sparse attention mask for layer activations."}, {"text_id": "rJgfByXdRQ", "sid": 3, "sentence": "Hereby we extend the idea of [2] to generative networks."}, {"text_id": "rJgfByXdRQ", "sid": 4, "sentence": "We highlight the differences to DGR [3] in the Sec. 2 of our work."}, {"text_id": "rJgfByXdRQ", "sid": 5, "sentence": "2."}, {"text_id": "rJgfByXdRQ", "sid": 6, "sentence": "Equation (5) and (6) are taken from [2] one to one."}, {"text_id": "rJgfByXdRQ", "sid": 7, "sentence": "Equations (3) and (4) are adopted from [2]: equation (3) describes the annealing of the parameter s, we anneal it globally over the course of epochs, whereas [2] anneal it for each epoch over the number of batches; equation (4) is a simplified version of the one used by [2]."}, {"text_id": "rJgfByXdRQ", "sid": 8, "sentence": "3. To avoid confusion of the proposed method to utilize techniques of DGR[3] in order to prevent forgetting in the G, we kindly ask the reviewer to refer to our response (2) to the Reviewer 1."}, {"text_id": "rJgfByXdRQ", "sid": 9, "sentence": "In the proposed work we adopt the generative replay not in order to avoid storing previous samples, but in order to prevent forgetting in the discriminator (which is used as a final classification model)."}, {"text_id": "rJgfByXdRQ", "sid": 10, "sentence": "Data synthesized by the generator is replayed for to the discriminator during the training of the subsequent tasks."}, {"text_id": "rJgfByXdRQ", "sid": 11, "sentence": "There is no replay applied to the generator network."}, {"text_id": "rJgfByXdRQ", "sid": 12, "sentence": "In order to avoid storing previous data, we utilize parameter level attention mechanism similar to HAT [2]."}, {"text_id": "rJgfByXdRQ", "sid": 13, "sentence": "Concerning the time comparison, there is no reason why our approach should be less time efficient then DGR based approaches [1, 3] as our method does not require retraining the generator from scratch at each time step."}, {"text_id": "rJgfByXdRQ", "sid": 14, "sentence": "4. Why our method does not outperform joint training on SVHN?"}, {"text_id": "rJgfByXdRQ", "sid": 15, "sentence": "Using generated samples accommodates for better performance then joint training is the case of tasks of relatively low complexity such as MNIST."}, {"text_id": "rJgfByXdRQ", "sid": 16, "sentence": "Indeed, such a result has been shown in other works, e.g. [1]."}, {"text_id": "rJgfByXdRQ", "sid": 17, "sentence": "As explained in Sec. 5.2, this can be attributed to a potentially higher diversity with a steady quality of the generated samples."}, {"text_id": "rJgfByXdRQ", "sid": 18, "sentence": "Clearly, the performance of the classifier trained on the generated samples highly depends on the complexity of the task and quality of the generated samples."}, {"text_id": "rJgfByXdRQ", "sid": 19, "sentence": "Thus, this effect can not be observed neither in the SVHN not the CIFAR10 benchmarks."}, {"text_id": "rJgfByXdRQ", "sid": 20, "sentence": "5. Grammar mistakes and typos."}, {"text_id": "rJgfByXdRQ", "sid": 21, "sentence": "This will be fixed in the updated version of the paper."}, {"text_id": "rJgfByXdRQ", "sid": 22, "sentence": "6. No guarantee to work for any task or scenario."}, {"text_id": "rJgfByXdRQ", "sid": 23, "sentence": "As pointed out by the reviewer and is true for many machine learning method, there is no guarantee that the proposed method will work for any task or scenario."}, {"text_id": "rJgfByXdRQ", "sid": 24, "sentence": "[1] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs: learning to generate images from new categories without forgetting. In Advances In Neural Information Processing Systems, 2018."}, {"text_id": "rJgfByXdRQ", "sid": 25, "sentence": "[2] J. Serr\u00e0, D. Sur\u00eds, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018."}, {"text_id": "rJgfByXdRQ", "sid": 26, "sentence": "URL http://arxiv.org/abs/1801.01423."}, {"text_id": "rJgfByXdRQ", "sid": 27, "sentence": "[3] H. Shin, J. K. Lee, J. Kim, and J. Kim."}, {"text_id": "rJgfByXdRQ", "sid": 28, "sentence": "Continual learning with deep generative replay."}, {"text_id": "rJgfByXdRQ", "sid": 29, "sentence": "In"}, {"text_id": "rJgfByXdRQ", "sid": 30, "sentence": "Advances in Neural Information Processing Systems, pages 2990\u20132999, 2017."}], "rebuttallabels": [{"labels": {"alignments": [11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 0}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 1}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 2}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 3}, {"labels": {"alignments": [11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 4}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 5}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 6}, {"labels": {"alignments": [12], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 7}, {"labels": {"alignments": [14, 15], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rJgfByXdRQ", "sid": 8}, {"labels": {"alignments": [14, 15], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rJgfByXdRQ", "sid": 9}, {"labels": {"alignments": [16], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rJgfByXdRQ", "sid": 10}, {"labels": {"alignments": [16], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rJgfByXdRQ", "sid": 11}, {"labels": {"alignments": [16], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "rJgfByXdRQ", "sid": 12}, {"labels": {"alignments": [17], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 13}, {"labels": {"alignments": [20, 21], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 14}, {"labels": {"alignments": [20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 15}, {"labels": {"alignments": [20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 16}, {"labels": {"alignments": [20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 17}, {"labels": {"alignments": [20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 18}, {"labels": {"alignments": [20, 21], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 19}, {"labels": {"alignments": [23], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 20}, {"labels": {"alignments": [23], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJgfByXdRQ", "sid": 21}, {"labels": {"alignments": [28], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 22}, {"labels": {"alignments": [28], "responsetype": "mitigate-criticism", "coarseresponse": "dispute"}, "text_id": "rJgfByXdRQ", "sid": 23}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 24}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 25}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 26}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 27}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 28}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 29}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "rJgfByXdRQ", "sid": 30}], "metadata": {"anno": "anno14", "review": "rJggAvorhQ", "rebuttal": "rJgfByXdRQ", "conference": "ICLR2019", "title": "Learning to remember: Dynamic Generative Memory for Continual Learning", "reviewer": "AnonReviewer1", "forum_id": "H1lIzhC9FX", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}