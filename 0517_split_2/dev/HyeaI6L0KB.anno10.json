{"review": [{"text_id": "HyeaI6L0KB", "sid": 0, "sentence": "This paper makes an interesting theoretical contribution; namely, that SGD with momentum (and with a slight modification to the step-size rule) is guaranteed to quickly converge to a second-order stationary point, implying it quickly escapes saddle points."}, {"text_id": "HyeaI6L0KB", "sid": 1, "sentence": "SGD with momentum is widely used in the practice of deep learning, but a theoretical analysis has remained largely elusive."}, {"text_id": "HyeaI6L0KB", "sid": 2, "sentence": "This paper sheds light theoretical properties justifying its use for deep learning."}, {"text_id": "HyeaI6L0KB", "sid": 3, "sentence": "Although the paper makes assumptions (e.g., twice differentiable, with smooth Hessian) that are not valid for the most widely-used deep learning models, the theoretical contributions of this paper should nonetheless be of interest to researchers in optimization for machine learning."}, {"text_id": "HyeaI6L0KB", "sid": 4, "sentence": "I recommend it be accepted."}, {"text_id": "HyeaI6L0KB", "sid": 5, "sentence": "The experiments reported in the paper, including those used to validate the required properties, are for small toy problems."}, {"text_id": "HyeaI6L0KB", "sid": 6, "sentence": "This is reasonable given that the main contribution of the paper is theoretical."}, {"text_id": "HyeaI6L0KB", "sid": 7, "sentence": "However, I would have given a higher rating if some further exploration of the validity of these properties was carried out for problems closer to those of interest to the broader ICLR community."}, {"text_id": "HyeaI6L0KB", "sid": 8, "sentence": "Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling."}, {"text_id": "HyeaI6L0KB", "sid": 9, "sentence": "This may also help to understand some of the limitations of this analysis."}, {"text_id": "HyeaI6L0KB", "sid": 10, "sentence": "One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?"}], "reviewlabels": [{"text_id": "HyeaI6L0KB", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Other", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HyeaI6L0KB", "sid": 10, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "Sye_nW4IjB", "sid": 0, "sentence": "We thank for your valuable comments and suggestions."}, {"text_id": "Sye_nW4IjB", "sid": 1, "sentence": "=== Regarding to the assumptions, specifically, twice differentiable/smooth Hessian ="}, {"text_id": "Sye_nW4IjB", "sid": 2, "sentence": "="}, {"text_id": "Sye_nW4IjB", "sid": 3, "sentence": "="}, {"text_id": "Sye_nW4IjB", "sid": 4, "sentence": "Twice differentiable/smooth Hessian are only used for analyzing the process of escaping saddle points."}, {"text_id": "Sye_nW4IjB", "sid": 5, "sentence": "So we conjecture that one can relax the assumptions and introduce the notions like ``locally twice differentiable'' and ``locally smooth Hessian'', meaning that the assumptions only need to hold in the region of the saddle points."}, {"text_id": "Sye_nW4IjB", "sid": 6, "sentence": "Since the gradient norm in the region of the saddle points is small, it implies that the Hessian should not change too much and ``locally smooth Hessian'' should make sense."}, {"text_id": "Sye_nW4IjB", "sid": 7, "sentence": "However, we are not aware of any related works of escaping saddle points introducing any measures of ``locally smooth Hessian''."}, {"text_id": "Sye_nW4IjB", "sid": 8, "sentence": "You might actually point out a good research direction."}, {"text_id": "Sye_nW4IjB", "sid": 9, "sentence": "=== Regarding to the empirical results/experiments ==="}, {"text_id": "Sye_nW4IjB", "sid": 10, "sentence": "We appreciate your acknowledgment of our contributions and pointing out that the properties may only need to be satisfied at some critical points during training deep neural nets."}, {"text_id": "Sye_nW4IjB", "sid": 11, "sentence": "We will keep updating the paper and conducting more thorough experiments."}, {"text_id": "Sye_nW4IjB", "sid": 12, "sentence": "=== Regarding to the small step size ==="}, {"text_id": "Sye_nW4IjB", "sid": 13, "sentence": "We think that it is a gap, for which people in the community haven't have any good remedies yet."}, {"text_id": "Sye_nW4IjB", "sid": 14, "sentence": "Almost all of the theoretical works in nonconvex optimization and deep learning require a small step size (e.g. works of natural tangent kernel, works of showing the global convergence for a two layer neural net)."}, {"text_id": "Sye_nW4IjB", "sid": 15, "sentence": "Nevertheless, we want to note that the step size $\\eta = O(\\epsilon^5)$ in our paper is of the same order as the closely related work (Daneshmand et al. 2018) of escaping saddle points."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "Sye_nW4IjB", "sid": 0}, {"labels": {"alignments": [3], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Sye_nW4IjB", "sid": 1}, {"labels": {"alignments": [3], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Sye_nW4IjB", "sid": 2}, {"labels": {"alignments": [3], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Sye_nW4IjB", "sid": 3}, {"labels": {"alignments": [3], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Sye_nW4IjB", "sid": 4}, {"labels": {"alignments": [3], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Sye_nW4IjB", "sid": 5}, {"labels": {"alignments": [3], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Sye_nW4IjB", "sid": 6}, {"labels": {"alignments": [3], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "Sye_nW4IjB", "sid": 7}, {"labels": {"alignments": [3], "responsetype": "future", "coarseresponse": "concur"}, "text_id": "Sye_nW4IjB", "sid": 8}, {"labels": {"alignments": [5, 6, 7, 8, 9], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Sye_nW4IjB", "sid": 9}, {"labels": {"alignments": [5, 6, 7, 8, 9], "responsetype": "concede-criticism", "coarseresponse": "concur"}, "text_id": "Sye_nW4IjB", "sid": 10}, {"labels": {"alignments": [5, 6, 7, 8, 9], "responsetype": "by-cr_manu_Yes", "coarseresponse": "concur"}, "text_id": "Sye_nW4IjB", "sid": 11}, {"labels": {"alignments": [10], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "Sye_nW4IjB", "sid": 12}, {"labels": {"alignments": [10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Sye_nW4IjB", "sid": 13}, {"labels": {"alignments": [10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Sye_nW4IjB", "sid": 14}, {"labels": {"alignments": [10], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "Sye_nW4IjB", "sid": 15}], "metadata": {"anno": "anno10", "review": "HyeaI6L0KB", "rebuttal": "Sye_nW4IjB", "conference": "ICLR2020", "title": "Escaping Saddle Points Faster with Stochastic Momentum", "reviewer": "AnonReviewer2", "forum_id": "rkeNfp4tPr", "rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area."}}