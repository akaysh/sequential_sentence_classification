{"review": [{"text_id": "Skx9YZG-qB", "sid": 0, "sentence": "This paper first gives a concise yet precise summary of maximizing one of variational lower bounds of mutual information, InfoNCE, then it provides an alternative view to explain case by case why word embedding Skip-gram, BERT, XLNet work in practice can be viewed by InfoNCE framework, thus we have a good understand for these methods."}, {"text_id": "Skx9YZG-qB", "sid": 1, "sentence": "Moreover it introduces a self-learning method  that maximizes the mutual information between a global sentence representation and n-grams in the sentence based on deep InfoMax framework instead."}, {"text_id": "Skx9YZG-qB", "sid": 2, "sentence": "Experiments show that it is better then BERT and BERT-NCE. It's known that InfoNCE increases bias but reduce variance, the same is true for deep InfoMax. Do you observe this in your experiments? If so, please provide."}, {"text_id": "Skx9YZG-qB", "sid": 3, "sentence": "The paper is well-written and easy to follow."}, {"text_id": "Skx9YZG-qB", "sid": 4, "sentence": "The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling."}, {"text_id": "Skx9YZG-qB", "sid": 5, "sentence": "In equations 1 and 2, should a, b be written in capital? Since they represent random variables."}], "reviewlabels": [{"text_id": "Skx9YZG-qB", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Skx9YZG-qB", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Skx9YZG-qB", "sid": 2, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Skx9YZG-qB", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Skx9YZG-qB", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Skx9YZG-qB", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "rJlgPc-mjB", "sid": 0, "sentence": "Thank you for your thoughtful review."}, {"text_id": "rJlgPc-mjB", "sid": 1, "sentence": "We have updated notations in Equations 1 and 2."}, {"text_id": "rJlgPc-mjB", "sid": 2, "sentence": "The expectations are now taken over random variables (A and B) and the function takes particular values (a and b) of these random variables."}, {"text_id": "rJlgPc-mjB", "sid": 3, "sentence": "Regarding your comment about increasing bias and reducing variance, we did observe that the quality of the InfoWord representations is relatively stable across different runs in our experiments (as evaluated by performance on downstream tasks). Could you please clarify a bit more whether this is what you are asking?"}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "rJlgPc-mjB", "sid": 0}, {"labels": {"alignments": [5], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "rJlgPc-mjB", "sid": 1}, {"labels": {"alignments": [1], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "rJlgPc-mjB", "sid": 2}, {"labels": {"alignments": [2], "responsetype": "followup", "coarseresponse": "nonarg"}, "text_id": "rJlgPc-mjB", "sid": 3}], "metadata": {"anno": "anno13", "review": "Skx9YZG-qB", "rebuttal": "rJlgPc-mjB", "conference": "ICLR2020", "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "reviewer": "AnonReviewer2", "forum_id": "Syx79eBKwr", "rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area."}}