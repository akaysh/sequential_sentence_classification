{"review": [{"text_id": "HJl_1dRl5r", "sid": 0, "sentence": "This paper explores the relation among the generalization error of neural networks and the model and data scales empirically."}, {"text_id": "HJl_1dRl5r", "sid": 1, "sentence": "The topic is interesting, while I was expecting to learn more from the paper, instead of some well-known conclusions."}, {"text_id": "HJl_1dRl5r", "sid": 2, "sentence": "If the paper could provide some guidance for model and data selection, that would be an interesting paper for the ICLR audience."}, {"text_id": "HJl_1dRl5r", "sid": 3, "sentence": "For instance, how deep should a model be for a classification or regression task?"}, {"text_id": "HJl_1dRl5r", "sid": 4, "sentence": "What is the minimum/maximum layers of a deep model? How much data is sufficient for a model to learn?"}, {"text_id": "HJl_1dRl5r", "sid": 5, "sentence": "What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?"}, {"text_id": "HJl_1dRl5r", "sid": 6, "sentence": "What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?"}, {"text_id": "HJl_1dRl5r", "sid": 7, "sentence": "How about the gain of the task performance?"}], "reviewlabels": [{"text_id": "HJl_1dRl5r", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJl_1dRl5r", "sid": 1, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJl_1dRl5r", "sid": 2, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJl_1dRl5r", "sid": 3, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJl_1dRl5r", "sid": 4, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJl_1dRl5r", "sid": 5, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJl_1dRl5r", "sid": 6, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJl_1dRl5r", "sid": 7, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "SkxT1pNwir", "sid": 0, "sentence": "Thank you for your review."}, {"text_id": "SkxT1pNwir", "sid": 1, "sentence": "We are a bit surprised since the paper provides answers to the exact questions you raised as missing. We are sorry you missed it, and we have cleaned up the presentation so it is hopefully now clear that we do answer these questions and more."}, {"text_id": "SkxT1pNwir", "sid": 2, "sentence": "The answers, as you pointed out, were much desired and not known before."}, {"text_id": "SkxT1pNwir", "sid": 3, "sentence": "Below are answers resultant from eq. 5 to the specific questions the referee raised, with some added definitions to make them concrete."}, {"text_id": "SkxT1pNwir", "sid": 4, "sentence": "1. \u201chow deep should a model be for a classification or regression task? \u201c"}, {"text_id": "SkxT1pNwir", "sid": 5, "sentence": "We show in section 6.1 that the dependency of the classification error on the number of layers is also well approximated by eq. 5 (recall $m$ scales linearly with depth)."}, {"text_id": "SkxT1pNwir", "sid": 6, "sentence": "So, if we consider some target error $\\epsilon_{target}$, we can solve eq. 5 for m or n given the other or for both, attaining the m,n contour for $\\hat{\\epsilon}(m,n) = \\epsilon_{target}$."}, {"text_id": "SkxT1pNwir", "sid": 7, "sentence": "2. \u201cWhat is the minimum/maximum layers of a deep model? \u201c"}, {"text_id": "SkxT1pNwir", "sid": 8, "sentence": "For a fixed dataset size, model scaling eventually contributes marginally to error reduction and becomes negligible when $bm^{-\\beta} \\ll n_{lim}^{-\\alpha}$ (Eq. 5)."}, {"text_id": "SkxT1pNwir", "sid": 9, "sentence": "Define the relative contribution threshold $T$ as satisfying $ T = \\frac{n^{-\\alpha} }{ bm^{-\\beta}}$. (For example, $T=10$.) Then the maximal useful model size meeting threshold $T$ is:"}, {"text_id": "SkxT1pNwir", "sid": 10, "sentence": "$$     m_{max}(T) = \\left(bT\\right)^{1/\\beta} n_{lim}^{\\alpha/\\beta}  $$"}, {"text_id": "SkxT1pNwir", "sid": 11, "sentence": "As for minimal depth, here too let\u2019s consider a definition as a working example: what is the minimum depth that could meet a certain error level $\\epsilon_{target}$ (if data is not a limit)."}, {"text_id": "SkxT1pNwir", "sid": 12, "sentence": "For example, when the target error is small relative to the \u201crandom guess error\u201d $\\epsilon_0$ (equivalently when $ n^{-\\alpha} + bm^{-\\beta} \\ll \\eta$), by solving eq. 5 for $m$ we have:"}, {"text_id": "SkxT1pNwir", "sid": 13, "sentence": "$$ m_{min} = \\left(\\frac{b}{\\frac{\\epsilon_{target}}{\\epsilon_0}\\eta-c_\\infty}\\right)^{1/\\beta} $$"}, {"text_id": "SkxT1pNwir", "sid": 14, "sentence": "3. \u201cHow much data is sufficient for a model to learn? What is the minimum/maximum size of the data set?\u201d"}, {"text_id": "SkxT1pNwir", "sid": 15, "sentence": "Similarly to the above:"}, {"text_id": "SkxT1pNwir", "sid": 16, "sentence": "Minimum data needed for target error (if model size is not a limit):"}, {"text_id": "SkxT1pNwir", "sid": 17, "sentence": "$$ n_{min} = \\left(\\frac{1}{\\frac{\\epsilon_{target}}{\\epsilon_0}\\eta-c_\\infty}\\right)^{1/\\alpha} $$"}, {"text_id": "SkxT1pNwir", "sid": 18, "sentence": "4. Maximum useful data (in the marginal sense $T$ for a limited size model, as above):"}, {"text_id": "SkxT1pNwir", "sid": 19, "sentence": "$$n_{max}(T) = \\left(1/bT\\right)^{1/\\alpha} m_{lim}^{\\beta/\\alpha} $$"}, {"text_id": "SkxT1pNwir", "sid": 20, "sentence": "In particular, note that there is also a minimal amount of data and model size needed for better-than-random-guess error level, characterized by the location of the pole $\\eta$: $n^{-\\alpha}+bm^{-\\beta}< \\eta$"}, {"text_id": "SkxT1pNwir", "sid": 21, "sentence": "5. \u201cDo we really need a large data set or just a subset that covers the data distribution?\u201d"}, {"text_id": "SkxT1pNwir", "sid": 22, "sentence": "Via careful dataset sub-sampling (as noted by reviewer 3) we show that indeed more data *is* needed to improve performance (reduce error) while holding the class distribution fixed (in expectation), for a given architecture and scaling policy."}, {"text_id": "SkxT1pNwir", "sid": 23, "sentence": "For directly viewing the error manifolds decoupling the dependency on model and data size, see figure 1 and in appendix C."}, {"text_id": "SkxT1pNwir", "sid": 24, "sentence": "6. \u201cWhat's the relation between the size of a model and that of a data set? \u201c"}, {"text_id": "SkxT1pNwir", "sid": 25, "sentence": "The joint form in Eq. 5 captures the relation between data-size and model-size (and error) completely."}, {"text_id": "SkxT1pNwir", "sid": 26, "sentence": "7. \u201cBy increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?\u201d"}, {"text_id": "SkxT1pNwir", "sid": 27, "sentence": "For example, from Eq. 5, it is clear that a sweet-spot in terms of balancing the effect of the data/model sizes on limiting the error is $n^{-\\alpha} \\approx bm^{-\\beta}$ ."}, {"text_id": "SkxT1pNwir", "sid": 28, "sentence": "When considering this sweet spot for example, increasing depth/width/both such that the model size $m$ is increased by a factor $f$ to a new size is $m\u2019 = mf$, the corresponding increase in data maintaining the sweet-spot is $n\u2019 = nf^{\\beta/\\alpha}$"}, {"text_id": "SkxT1pNwir", "sid": 29, "sentence": "8. How about the gain of the task performance?\u201d"}, {"text_id": "SkxT1pNwir", "sid": 30, "sentence": "The effect on the performance is given by evaluating Eq.5 for the initial and scaled $m,n$."}, {"text_id": "SkxT1pNwir", "sid": 31, "sentence": "For example, in the powerlaw region ($c_\\infty \\ll n^{-\\alpha} + bm^{-\\beta} \\ll \\eta$):"}, {"text_id": "SkxT1pNwir", "sid": 32, "sentence": "The effect on the performance is $\\epsilon\u2019 = \\epsilon f^{-\\beta}$"}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 0}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 1}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 2}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 3}, {"labels": {"alignments": [3], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 4}, {"labels": {"alignments": [3], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 5}, {"labels": {"alignments": [3], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 6}, {"labels": {"alignments": [4], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 7}, {"labels": {"alignments": [4], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 8}, {"labels": {"alignments": [4], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 9}, {"labels": {"alignments": [4], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 10}, {"labels": {"alignments": [4], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 11}, {"labels": {"alignments": [4], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 12}, {"labels": {"alignments": [4], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 13}, {"labels": {"alignments": [4, 5], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 14}, {"labels": {"alignments": [4, 5], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 15}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 16}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 17}, {"labels": {"alignments": [4, 5], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 18}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 19}, {"labels": {"alignments": [4, 5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 20}, {"labels": {"alignments": [5], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 21}, {"labels": {"alignments": [5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 22}, {"labels": {"alignments": [5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 23}, {"labels": {"alignments": [6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 24}, {"labels": {"alignments": [6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 25}, {"labels": {"alignments": [6], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 26}, {"labels": {"alignments": [6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 27}, {"labels": {"alignments": [6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 28}, {"labels": {"alignments": [7], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SkxT1pNwir", "sid": 29}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 30}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 31}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SkxT1pNwir", "sid": 32}], "metadata": {"anno": "anno10", "review": "HJl_1dRl5r", "rebuttal": "SkxT1pNwir", "conference": "ICLR2020", "title": "A Constructive Prediction of the Generalization Error Across Scales", "reviewer": "AnonReviewer1", "forum_id": "ryenvpEKDr", "rating": "1: Reject", "experience_assessment": "I have read many papers in this area."}}