{"review": [{"text_id": "HJlhGPM9hm", "sid": 0, "sentence": "A model compression framework, DeepTwist, was proposed which makes the weights zero if they are small in magnitude."}, {"text_id": "HJlhGPM9hm", "sid": 1, "sentence": "They used different model compression techniques in this framework to show the effectiveness of the proposed method."}, {"text_id": "HJlhGPM9hm", "sid": 2, "sentence": "This paper proposes a framework intending to use fewer hardware resources without compromising the model accuracy."}, {"text_id": "HJlhGPM9hm", "sid": 3, "sentence": "However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node."}, {"text_id": "HJlhGPM9hm", "sid": 4, "sentence": "Therefore, it is not clear how the proposed framework is helping the model compression techniques."}], "reviewlabels": [{"text_id": "HJlhGPM9hm", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlhGPM9hm", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlhGPM9hm", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlhGPM9hm", "sid": 3, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlhGPM9hm", "sid": 4, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "HJe2kdKNTm", "sid": 0, "sentence": "Thank you for the review."}, {"text_id": "HJe2kdKNTm", "sid": 1, "sentence": "First, we want to mention that DeepTwist is proposed not only for weight pruning, but also for other compression techniques, such as quantization and low-rank approximation, as we discussed in Section 4.2 and 4.3"}, {"text_id": "HJe2kdKNTm", "sid": 2, "sentence": "After weight pruning is performed and zero weights are removed, we usually obtain a sparse matrix to represent non-zero weights."}, {"text_id": "HJe2kdKNTm", "sid": 3, "sentence": "There are lots of existing sparse matrix computation libraries to support SpMV (sparse matrix-vector multiplication) and so on."}, {"text_id": "HJe2kdKNTm", "sid": 4, "sentence": "If a matrix is highly sparse, then we would reduce memory footprint and amount of computations (for example, we can skip zero weights during computation) significantly."}, {"text_id": "HJe2kdKNTm", "sid": 5, "sentence": "There have been extensive studies of efficient hardware implementation after weight pruning, and we want you to refer to the paper \u201cEIE: efficient inference engine on compressed deep neural network\u201d or \u201cDeep compression: compressing deep neural networks with pruning, trained quatization and Huffman coding.\u201d"}, {"text_id": "HJe2kdKNTm", "sid": 6, "sentence": "In this paper, we have not discussed particular sparse matrix implementation methods which are not our focus in this paper."}, {"text_id": "HJe2kdKNTm", "sid": 7, "sentence": "We would greatly appreciate if you can reconsider your decision based on our comments and other methods we also discussed (i.e., quantization and low-rank approximation)."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HJe2kdKNTm", "sid": 0}, {"labels": {"alignments": [3, 4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJe2kdKNTm", "sid": 1}, {"labels": {"alignments": [3, 4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJe2kdKNTm", "sid": 2}, {"labels": {"alignments": [3, 4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJe2kdKNTm", "sid": 3}, {"labels": {"alignments": [3, 4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJe2kdKNTm", "sid": 4}, {"labels": {"alignments": [3, 4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJe2kdKNTm", "sid": 5}, {"labels": {"alignments": [3, 4], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "HJe2kdKNTm", "sid": 6}, {"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "HJe2kdKNTm", "sid": 7}], "metadata": {"anno": "anno10", "review": "HJlhGPM9hm", "rebuttal": "HJe2kdKNTm", "conference": "ICLR2019", "title": "DeepTwist: Learning Model Compression via Occasional Weight Distortion", "reviewer": "AnonReviewer1", "forum_id": "HJzLdjR9FX", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}