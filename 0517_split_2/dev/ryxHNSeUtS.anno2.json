{"review": [{"text_id": "ryxHNSeUtS", "sid": 0, "sentence": "Summary:"}, {"text_id": "ryxHNSeUtS", "sid": 1, "sentence": "The authors propose quantize the weights of a neural network by enabling a fractional number of bits per weight."}, {"text_id": "ryxHNSeUtS", "sid": 2, "sentence": "They use a network of differentiable XOR gates that maps encrypted weights to higher-dimensional decrypted weights to decode the parameters on-the-fly and learn both the encrypted weights and the scaling factors involved in the XOR networks by gradient descent."}, {"text_id": "ryxHNSeUtS", "sid": 3, "sentence": "Strengths of the paper:"}, {"text_id": "ryxHNSeUtS", "sid": 4, "sentence": "- The method allows for a fractional number of bits per weights and relies of well-known differentiable approximations of the sign function."}, {"text_id": "ryxHNSeUtS", "sid": 5, "sentence": "Indeed, virtually any number of bits/weights can be attained by varying the ratio N_in/N_out."}, {"text_id": "ryxHNSeUtS", "sid": 6, "sentence": "- The papers displays good results on ImageNet for a ResNet-18."}, {"text_id": "ryxHNSeUtS", "sid": 7, "sentence": "Weaknesses of the paper:"}, {"text_id": "ryxHNSeUtS", "sid": 8, "sentence": "- Some arguments that are presented could deserve a bit more precision."}, {"text_id": "ryxHNSeUtS", "sid": 9, "sentence": "For instance, quantizing to a fractional number of bits per weights per layer is in itself interesting."}, {"text_id": "ryxHNSeUtS", "sid": 10, "sentence": "However, if we were to quantize different layers of the same network with distinct integer  ratio of bits per weights (say 1 bit per weight for some particular layers and 2 bits per weight for the other layers), the average ratio would also be fractional (see for instance \"Hardware-aware Automated Quantization with Mixed Precision\", Wang et al., where the authors find the right (integer) number of bits/weights per layer using RL)."}, {"text_id": "ryxHNSeUtS", "sid": 11, "sentence": "Similarly, using vector quantization does allow for on-chip low memory: we do not need to re-instantiate the compressed layer but we can compute the forward in the compressed domain (by splitting the activations into similar block sizes and computing dot products)."}, {"text_id": "ryxHNSeUtS", "sid": 12, "sentence": "- More extensive and thorough experiments could improve the impact of the paper."}, {"text_id": "ryxHNSeUtS", "sid": 13, "sentence": "For instance, authors could compress the widely used (and more challenging) ResNet-50 architecture, or try other tasks such as image detection (Mask R-CNN)."}, {"text_id": "ryxHNSeUtS", "sid": 14, "sentence": "The table is missing results from: \"Hardware Automated Quantization\", Wang et al ; \"Trained Ternary Quantization\", Zhu et al ; \"Deep Compression\",  Han et al; \"Ternary weight networks\", Li et al (not an extensive list)."}, {"text_id": "ryxHNSeUtS", "sid": 15, "sentence": "- Similarly, providing some code and numbers for inference time would greatly strengthen the paper and the possible usage of this method by the community."}, {"text_id": "ryxHNSeUtS", "sid": 16, "sentence": "Indeed, I wonder what the overhead of decrypting the weights on-the-fly is (although it only involves XOR operations and products)"}, {"text_id": "ryxHNSeUtS", "sid": 17, "sentence": "- Small typos: for instance, two points at the very end of section 5."}, {"text_id": "ryxHNSeUtS", "sid": 18, "sentence": "Justification fo rating:"}, {"text_id": "ryxHNSeUtS", "sid": 19, "sentence": "The proposed method is well presented and illustrated."}, {"text_id": "ryxHNSeUtS", "sid": 20, "sentence": "However, I think the paper would need either (1) more thorough experimental results (see comments above, points 2 and 3 of weaknesses) or (2) more justifications for its existence (see comments above, point 1 of weaknesses)."}], "reviewlabels": [{"text_id": "ryxHNSeUtS", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 6, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 8, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 9, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 10, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 11, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 12, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 13, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 14, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Meaningful Comparison", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 15, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Replicability", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 16, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 17, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 18, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 19, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Clarity", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "ryxHNSeUtS", "sid": 20, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "H1xBls0PjH", "sid": 0, "sentence": "We would like to thank you for the review and comments."}, {"text_id": "H1xBls0PjH", "sid": 1, "sentence": "We revised the manuscript to address your concerns."}, {"text_id": "H1xBls0PjH", "sid": 2, "sentence": "Below we summarized your concerns/questions with our answers."}, {"text_id": "H1xBls0PjH", "sid": 3, "sentence": "Q1: Some arguments that are presented could deserve a bit more precision."}, {"text_id": "H1xBls0PjH", "sid": 4, "sentence": "A1: We acknowledge that quantization is a very active research area in model compression and there are numerous quantization techniques with unique and distinct characteristics."}, {"text_id": "H1xBls0PjH", "sid": 5, "sentence": "We could not introduce and discuss lots of exciting quantization techniques such as vector quantization due to the limited space."}, {"text_id": "H1xBls0PjH", "sid": 6, "sentence": "We feel that introducing other quantization techniques in details would make the paper distracted since those techniques cannot be compared with compression ratio only (i.e., inference architecture, computation methods, and storage design would be different)."}, {"text_id": "H1xBls0PjH", "sid": 7, "sentence": "Instead, we added more thorough introduction to binary codes in Section 1 to explain unique computational advantages of using binary codes."}, {"text_id": "H1xBls0PjH", "sid": 8, "sentence": "We introduced \"Hardware-aware Automated Quantization with Mixed Precision\" in Section 1 since fractional quantization on average is available as you pointed out, while FleXOR can also employ different quantization bits for each layer (i.e., we believe HAQ method can be applied on top of FleXOR)."}, {"text_id": "H1xBls0PjH", "sid": 9, "sentence": "Q2: More extensive and thorough experiments could improve the impact of the paper."}, {"text_id": "H1xBls0PjH", "sid": 10, "sentence": "A2: We agree that including extensive quantization methods and model architectures would greatly improve the impact of the paper."}, {"text_id": "H1xBls0PjH", "sid": 11, "sentence": "Unfortunately, as we discussed above, our goal in this paper is to improve quantization schemes based on binary codes."}, {"text_id": "H1xBls0PjH", "sid": 12, "sentence": "Including quantization methods of different assumptions may require much lengthy discussions that make comparisons a lot complicated."}, {"text_id": "H1xBls0PjH", "sid": 13, "sentence": "For example, \"Hardware-aware Automated Quantization\" could be additionally applied to binary codes, and FleXOR is not conflicted with such an architectural techniques to improve compression ratio."}, {"text_id": "H1xBls0PjH", "sid": 14, "sentence": "Deep compression, TTQ, and TWN involve weight pruning that deserves large space for discussions (nonetheless, we compared TWN, TTQ, and BinaryRelax using ternary quantization scheme in Table 5 of Appendix)."}, {"text_id": "H1xBls0PjH", "sid": 15, "sentence": "Deep compression also includes CSR format and Huffman coding which would make comparisons more complicated."}, {"text_id": "H1xBls0PjH", "sid": 16, "sentence": "We chose a few representative quantization methods mainly based on binary codes to facilitate fair and focused comparisons, and correspondingly, ResNet models on CIFAR-10 and ImageNet are selected for our experiments since most previous works (of binary codes) commonly include those models."}, {"text_id": "H1xBls0PjH", "sid": 17, "sentence": "For example, we could not include HAQ in the paper for experimental results, because HAQ chooses MobileNet and ResNet-50 only as model architectures while comparisons are made with only PACT and Deep Compression methods."}, {"text_id": "H1xBls0PjH", "sid": 18, "sentence": "Q3: Providing some code and numbers for inference time would be great."}, {"text_id": "H1xBls0PjH", "sid": 19, "sentence": "A3: Due to the internal policy of our organization, we cannot open our codes publicly at this moment. Hence, we provide a link to anonymous code to the reviewers only until we get an approval for public release."}, {"text_id": "H1xBls0PjH", "sid": 20, "sentence": "Please refer to our message available to the reviewers only."}, {"text_id": "H1xBls0PjH", "sid": 21, "sentence": "Overhead of weight decryption on-the-fly is extremely small even with CPUs or GPUs, since decryption involves only a binary matrix multiplication over GF(2), which can be easily supported by existing SIMD or vector operations."}, {"text_id": "H1xBls0PjH", "sid": 22, "sentence": "Since a binary matrix is too small (e.g., 10x8), computational overhead is just ignorable compared with other computations."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "H1xBls0PjH", "sid": 0}, {"labels": {"alignments": [], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 1}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1xBls0PjH", "sid": 2}, {"labels": {"alignments": [8, 9, 10, 11], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1xBls0PjH", "sid": 3}, {"labels": {"alignments": [8, 9, 10, 11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 4}, {"labels": {"alignments": [8, 9, 10, 11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 5}, {"labels": {"alignments": [8, 9, 10, 11], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 6}, {"labels": {"alignments": [8, 9, 10, 11], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 7}, {"labels": {"alignments": [8, 9, 10, 11], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 8}, {"labels": {"alignments": [12, 13, 14], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1xBls0PjH", "sid": 9}, {"labels": {"alignments": [12, 13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 10}, {"labels": {"alignments": [12, 13, 14], "responsetype": "reject-request_scope_No", "coarseresponse": "dispute"}, "text_id": "H1xBls0PjH", "sid": 11}, {"labels": {"alignments": [12, 13, 14], "responsetype": "reject-request_scope_No", "coarseresponse": "dispute"}, "text_id": "H1xBls0PjH", "sid": 12}, {"labels": {"alignments": [12, 13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 13}, {"labels": {"alignments": [12, 13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 14}, {"labels": {"alignments": [12, 13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 15}, {"labels": {"alignments": [12, 13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 16}, {"labels": {"alignments": [12, 13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 17}, {"labels": {"alignments": [15, 16], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "H1xBls0PjH", "sid": 18}, {"labels": {"alignments": [15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 19}, {"labels": {"alignments": [15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 20}, {"labels": {"alignments": [15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 21}, {"labels": {"alignments": [15, 16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "H1xBls0PjH", "sid": 22}], "metadata": {"anno": "anno2", "review": "ryxHNSeUtS", "rebuttal": "H1xBls0PjH", "conference": "ICLR2020", "title": "FleXOR: Trainable Fractional Quantization", "reviewer": "AnonReviewer1", "forum_id": "HJlQ96EtPr", "rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area."}}