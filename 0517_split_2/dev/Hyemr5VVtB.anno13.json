{"review": [{"text_id": "Hyemr5VVtB", "sid": 0, "sentence": "The paper gives a big picture view on training objectives used to obtain static and contextualized word embeddings."}, {"text_id": "Hyemr5VVtB", "sid": 1, "sentence": "This is very handy since classical static word embeddings, such as SGNS and GloVe, have been studied theoretically in a number of works (e.g., Levy and Goldberg, 2014; Arora et al., 2016; Hashimoto et al., 2016; Gittens et al., 2017; Allen and Hospedales, 2019; Assylbekov and Takhanov, 2019), but not much has been done for the modern contextualized embedding models such ELMo and BERT - I personally know only the work of Wang and Cho (2019), and please correct me if I am wrong."}, {"text_id": "Hyemr5VVtB", "sid": 2, "sentence": "\"There is nothing as practical as a good theory\", and the authors confirm this statement: their theory suggests them to modify the training objective of the masked language modeling in a certain way and this modification proves to benefit the embeddings in general when evaluated on standard tasks."}, {"text_id": "Hyemr5VVtB", "sid": 3, "sentence": "I don't have any major issues to raise."}, {"text_id": "Hyemr5VVtB", "sid": 4, "sentence": "A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it."}], "reviewlabels": [{"text_id": "Hyemr5VVtB", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hyemr5VVtB", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hyemr5VVtB", "sid": 2, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hyemr5VVtB", "sid": 3, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Hyemr5VVtB", "sid": 4, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "B1eMJ9-msH", "sid": 0, "sentence": "Thank you for your thoughtful review."}, {"text_id": "B1eMJ9-msH", "sid": 1, "sentence": "We have updated Equation 1 and the paragraph above so that I(...) is consistently a function of two variables."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "B1eMJ9-msH", "sid": 0}, {"labels": {"alignments": [4], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "B1eMJ9-msH", "sid": 1}], "metadata": {"anno": "anno13", "review": "Hyemr5VVtB", "rebuttal": "B1eMJ9-msH", "conference": "ICLR2020", "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "reviewer": "AnonReviewer1", "forum_id": "Syx79eBKwr", "rating": "8: Accept", "experience_assessment": "I have published in this field for several years."}}