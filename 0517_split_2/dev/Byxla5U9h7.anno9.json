{"review": [{"text_id": "Byxla5U9h7", "sid": 0, "sentence": "This work considers a version of importance sampling of states from the replay buffer."}, {"text_id": "Byxla5U9h7", "sid": 1, "sentence": "Each trajectory is assigned a rank, inversely proportional to its probability according to a GMM."}, {"text_id": "Byxla5U9h7", "sid": 2, "sentence": "The trajectories with lower rank are preferred at sampling."}, {"text_id": "Byxla5U9h7", "sid": 3, "sentence": "Main issues:"}, {"text_id": "Byxla5U9h7", "sid": 4, "sentence": "1. Estimating rank from a density estimator"}, {"text_id": "Byxla5U9h7", "sid": 5, "sentence": "- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted."}, {"text_id": "Byxla5U9h7", "sid": 6, "sentence": "- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)"}, {"text_id": "Byxla5U9h7", "sid": 7, "sentence": "2. Generalization issues"}, {"text_id": "Byxla5U9h7", "sid": 8, "sentence": "- the method is not applicable to episodes of different length"}, {"text_id": "Byxla5U9h7", "sid": 9, "sentence": "- the approach assumes existence of a state to goal function f(s)->g"}, {"text_id": "Byxla5U9h7", "sid": 10, "sentence": "- although the paper does not expose this point (it is discussed the HER paper)"}, {"text_id": "Byxla5U9h7", "sid": 11, "sentence": "3. Scaling issues"}, {"text_id": "Byxla5U9h7", "sid": 12, "sentence": "- length of the vector grows linearly with the episode length"}, {"text_id": "Byxla5U9h7", "sid": 13, "sentence": "- length of the vector grows linearly with the size of the goal vector"}, {"text_id": "Byxla5U9h7", "sid": 14, "sentence": "For long episodes or episodes with large goal vectors it is quite possible that there will not be enough data to fit the GMM model or one would need to collect many samples prior."}, {"text_id": "Byxla5U9h7", "sid": 15, "sentence": "4. Minor issues"}, {"text_id": "Byxla5U9h7", "sid": 16, "sentence": "- 3.3 \"It is known that PER can become very expensive in computational time\" - please supply a reference"}, {"text_id": "Byxla5U9h7", "sid": 17, "sentence": "- 3.3 \"After each update of the model, the agent needs to update the priorities of the transitions in the replay buffer with the new TD-errors\" - However the method only renews priorities of randomly selected transitions (why would there be a large overhead?)."}, {"text_id": "Byxla5U9h7", "sid": 18, "sentence": "Here is from the PER paper \"Our final implementation for rank-based prioritization produced an additional 2%-4% increase in running time and negligible additional memory usage\""}], "reviewlabels": [{"text_id": "Byxla5U9h7", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 5, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 6, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 7, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 9, "labels": {"coarse": "Structuring", "fine": "Structuring.Quote", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 11, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 12, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 13, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 15, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 16, "labels": {"coarse": "Request", "fine": "Request.Edit", "asp": "Clarity", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 17, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Soundness/Correctness", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "Byxla5U9h7", "sid": 18, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "SyeblhC2pQ", "sid": 0, "sentence": "Thank you for the valuable feedback!"}, {"text_id": "SyeblhC2pQ", "sid": 1, "sentence": "We uploaded a revised version of the paper based on the comments."}, {"text_id": "SyeblhC2pQ", "sid": 2, "sentence": "- The reason behind using V-GMM is that V-GMM is much faster than KDE in inference and has a better generalization ability compared to GMM."}, {"text_id": "SyeblhC2pQ", "sid": 3, "sentence": "We use V-GMM as a proof of concept for the idea \u201cCuriosity-Driven Experience Prioritization via Density Estimation\u201d."}, {"text_id": "SyeblhC2pQ", "sid": 4, "sentence": "Other density estimation methods can also be applied."}, {"text_id": "SyeblhC2pQ", "sid": 5, "sentence": "We now clarify these reasons in Section \u201c2.3 Density Estimation Methods\u201d of the revised paper."}, {"text_id": "SyeblhC2pQ", "sid": 6, "sentence": "- We concatenate the goals and estimate the trajectory density instead of state density because HER needs to sample a future state in the trajectory as a virtual goal for training."}, {"text_id": "SyeblhC2pQ", "sid": 7, "sentence": "- For episodes of different length, we can pad or truncate the trajectories into same lengths and apply V-GMM."}, {"text_id": "SyeblhC2pQ", "sid": 8, "sentence": "Another method is to use PCA or auto-encoder to reduce the dimension into a fixed size and then apply CDP."}, {"text_id": "SyeblhC2pQ", "sid": 9, "sentence": "- Similarly, to handle scaling issues, for very high dimension vectors, we can first apply dimension reduction methods, such as PCA and auto-encoder, and then use CDP."}, {"text_id": "SyeblhC2pQ", "sid": 10, "sentence": "- The reference for \"It is known that PER can become very expensive in computational time\u201d is actually the \u201cPrioritized Experience Replay\u201d paper itself."}, {"text_id": "SyeblhC2pQ", "sid": 11, "sentence": "On page three of the PER paper, it writes \u201cImplementation: To scale to large memory sizes N , we use a binary heap data structure for the priority queue, for which"}, {"text_id": "SyeblhC2pQ", "sid": 12, "sentence": "finding the maximum priority transition when sampling is O(1) and updating priorities (with the new TD-error after a learning step) is O(log N). See Appendix B.2.1 for more details. \u201c"}, {"text_id": "SyeblhC2pQ", "sid": 13, "sentence": "In their Atari case, the memory size N is of 1e4 transitions."}, {"text_id": "SyeblhC2pQ", "sid": 14, "sentence": "In our hand manipulation environment cases, the memory size N is of 1e6 trajectories, and each trajectory has 100 transitions."}, {"text_id": "SyeblhC2pQ", "sid": 15, "sentence": "Thus, the memory size is 1e4 (theirs) vs 1e8 (ours)."}, {"text_id": "SyeblhC2pQ", "sid": 16, "sentence": "The complexity of updating priorities is O(log N)."}, {"text_id": "SyeblhC2pQ", "sid": 17, "sentence": "Therefore, PER is very expensive in computational time, at least in our case."}, {"text_id": "SyeblhC2pQ", "sid": 18, "sentence": "The memory buffer size N can be found in OpenAI Baselines link: https://github.com/openai/baselines"}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SyeblhC2pQ", "sid": 0}, {"labels": {"alignments": [], "responsetype": "summary", "coarseresponse": "nonarg"}, "text_id": "SyeblhC2pQ", "sid": 1}, {"labels": {"alignments": [5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 2}, {"labels": {"alignments": [5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 3}, {"labels": {"alignments": [5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 4}, {"labels": {"alignments": [5], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 5}, {"labels": {"alignments": [6], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 6}, {"labels": {"alignments": [7, 8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 7}, {"labels": {"alignments": [7, 8], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 8}, {"labels": {"alignments": [11, 12, 13, 14], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 9}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 10}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 11}, {"labels": {"alignments": [16], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 12}, {"labels": {"alignments": [17, 18], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 13}, {"labels": {"alignments": [17, 18], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 14}, {"labels": {"alignments": [17, 18], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 15}, {"labels": {"alignments": [17, 18], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 16}, {"labels": {"alignments": [17, 18], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 17}, {"labels": {"alignments": [17, 18], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeblhC2pQ", "sid": 18}], "metadata": {"anno": "anno9", "review": "Byxla5U9h7", "rebuttal": "SyeblhC2pQ", "conference": "ICLR2019", "title": "Curiosity-Driven Experience Prioritization via Density Estimation", "reviewer": "AnonReviewer3", "forum_id": "SklXvs0qt7", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}