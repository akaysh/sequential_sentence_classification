{"review": [{"text_id": "HJlqXnE5nQ", "sid": 0, "sentence": "The authors study properties of the learning behavior of non-linear (ReLu) neural networks."}, {"text_id": "HJlqXnE5nQ", "sid": 1, "sentence": "In particular, their main focus is on binary classification for the linear-separable case, when optimization is done using gradient descent minimizing either binary entropy or hinge loss."}, {"text_id": "HJlqXnE5nQ", "sid": 2, "sentence": "There are 3 main results in the paper:"}, {"text_id": "HJlqXnE5nQ", "sid": 3, "sentence": "1) During learning, each neuron only activates on data points of one class: hence (due to ReLu), each neuron only updates its weights when seeing data points from that class."}, {"text_id": "HJlqXnE5nQ", "sid": 4, "sentence": "The authors refer to this property as \"Independent modes of learning\", suggesting that the learning of parameters of the network is decoupled between the two classes."}, {"text_id": "HJlqXnE5nQ", "sid": 5, "sentence": "2) The classification error, with respect to the number of iterations of gradient descent, exhibits a sigmoidal shape: slow improvement at the beginning, followed by a period of fast improvement, followed by another plateau."}, {"text_id": "HJlqXnE5nQ", "sid": 6, "sentence": "3) Most frequent features, if discriminative, can prevent learning of other, less frequent, features."}, {"text_id": "HJlqXnE5nQ", "sid": 7, "sentence": "Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating \"at the beginning of training data points from different classes do not activate the same neurons\"."}, {"text_id": "HJlqXnE5nQ", "sid": 8, "sentence": "Even for a shallow net, the authors are essentially assuming that the first layer of weights W is such that each row w is already a hyperplane separating the two classes after initialization (wx > 0 for all x belonging to one class and wx' < 0 for x' in the other class)."}, {"text_id": "HJlqXnE5nQ", "sid": 9, "sentence": "In other words, at initialization, the first layer is already correctly classifying all data points."}, {"text_id": "HJlqXnE5nQ", "sid": 10, "sentence": "This is of course an extremely stringent assumption that doesn't hold in practice (eg, the probability of such an initialization shrinks to zero exponentially in the number of dimensions and in the number of neurons)."}, {"text_id": "HJlqXnE5nQ", "sid": 11, "sentence": "Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification."}, {"text_id": "HJlqXnE5nQ", "sid": 12, "sentence": "Pros:"}, {"text_id": "HJlqXnE5nQ", "sid": 13, "sentence": "- Authors consider a non-linear (ReLu) neural network, as opposed to the analysis of Save et al which only considers linear nets."}, {"text_id": "HJlqXnE5nQ", "sid": 14, "sentence": "- The fundamentally different behavior between Hinge and binary entropy loss is interesting, and worth analyzing further."}, {"text_id": "HJlqXnE5nQ", "sid": 15, "sentence": "- Sigmoidal shape of classification error as a function of number of iterations is inline with what is seen in practice."}, {"text_id": "HJlqXnE5nQ", "sid": 16, "sentence": "However, I believe the assumptions needed to show this point force the analysis to only characterize learning close to convergence."}, {"text_id": "HJlqXnE5nQ", "sid": 17, "sentence": "Minor Cons (apart from major concern above):"}, {"text_id": "HJlqXnE5nQ", "sid": 18, "sentence": "- Theorem 3.2: \"[...] converges at a speed proportional to [...]\". Isn't \\bar{u}_t logarithmic (non-linear) in t?"}, {"text_id": "HJlqXnE5nQ", "sid": 19, "sentence": "- Theorem 3.2: Even if strong, I don't mind the assumption on a dataset merely consisting of two (weighted) data points. I would suggest to simulate this case without putting any condition on the initialization of the weights (ie, without assumptions H1-H2), and compare the empirical shape of the classification error with the one you obtain analytically in Figure 2 Right."}, {"text_id": "HJlqXnE5nQ", "sid": 20, "sentence": "- Theorem 3.2 Interpretation: unfinished sentence \"We can characterize the convergence speeds more quantitatively with the\""}, {"text_id": "HJlqXnE5nQ", "sid": 21, "sentence": "- Theorem 4.1: Can you give an intuition or lower/upper bounds for u(t) for the Hinge case, to make evident its difference from the binary entropy case (where u(t) ~ log(t))"}, {"text_id": "HJlqXnE5nQ", "sid": 22, "sentence": "- Gradient starvation, Kaggle experiment: I'm not too convinced about the novelty/usefulness of this result. In the end, even a decision tree stump would stop growing after learning the dark/light feature as a discriminator."}, {"text_id": "HJlqXnE5nQ", "sid": 23, "sentence": "What I'm trying to say is that \"gradient starvation\" is a more general problem that really doesn't have to do with gradient descent."}, {"text_id": "HJlqXnE5nQ", "sid": 24, "sentence": "Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set."}], "reviewlabels": [{"text_id": "HJlqXnE5nQ", "sid": 0, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 1, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 2, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 3, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "HJlqXnE5nQ", "sid": 4, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "HJlqXnE5nQ", "sid": 5, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "HJlqXnE5nQ", "sid": 6, "labels": {"coarse": "Structuring", "fine": "Structuring.Summary", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 7, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "HJlqXnE5nQ", "sid": 8, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": true}, {"text_id": "HJlqXnE5nQ", "sid": 9, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 10, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 11, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 12, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 13, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Substance", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 14, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Motivation/Impact", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 15, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "P-Positive"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 16, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Replicability", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 17, "labels": {"coarse": "Structuring", "fine": "Structuring.Heading", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 18, "labels": {"coarse": "Request", "fine": "Request.Clarification", "asp": "Substance", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 19, "labels": {"coarse": "Request", "fine": "Request.Experiment", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 20, "labels": {"coarse": "Request", "fine": "Request.Typo", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 21, "labels": {"coarse": "Request", "fine": "Request.Explanation", "asp": "Substance", "pol": "U-Neutral"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 22, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Originality", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 23, "labels": {"coarse": "Fact", "fine": "Fact", "asp": "", "pol": ""}, "secondarylabels": [], "merge-with-prior": false}, {"text_id": "HJlqXnE5nQ", "sid": 24, "labels": {"coarse": "Evaluative", "fine": "Evaluative", "asp": "Soundness/Correctness", "pol": "N-Negative"}, "secondarylabels": [], "merge-with-prior": false}], "rebuttal": [{"text_id": "SyeGJ1qEA7", "sid": 0, "sentence": "We thank you for your thorough review, which has undoubtedly helped improve the paper."}, {"text_id": "SyeGJ1qEA7", "sid": 1, "sentence": "First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper."}, {"text_id": "SyeGJ1qEA7", "sid": 2, "sentence": "For more details, please see the comment above entitled: \u201cRelaxing Assumption (H2)\u201d."}, {"text_id": "SyeGJ1qEA7", "sid": 3, "sentence": "Nevertheless, we wish to emphasize that even under Assumption (H2), learning can still fail."}, {"text_id": "SyeGJ1qEA7", "sid": 4, "sentence": "Fig 2. Left and Section 3.3 show that any initialization in the top left red region will lead (after a finite number of updates) to a confidence of 0.5 on the corresponding class."}, {"text_id": "SyeGJ1qEA7", "sid": 5, "sentence": "The network does not provide correct classification at the end of training even though it does at the beginning."}, {"text_id": "SyeGJ1qEA7", "sid": 6, "sentence": "Here are responses to your other concerns:"}, {"text_id": "SyeGJ1qEA7", "sid": 7, "sentence": "- Indeed, our intent in the statement of Theorem 3.2 was to describe the scaling of the solution with respect to those two quantities, but it can be misinterpreted. We have clarified it in the new version of the paper."}, {"text_id": "SyeGJ1qEA7", "sid": 8, "sentence": "- We have run that experiment and included it in Fig 3. Right among our other recent findings."}, {"text_id": "SyeGJ1qEA7", "sid": 9, "sentence": "- Corrected in the new version."}, {"text_id": "SyeGJ1qEA7", "sid": 10, "sentence": "- We have added a line in the last paragraph of Section 4 stating that for the Hinge loss, u(t) grows exponentially in t."}, {"text_id": "SyeGJ1qEA7", "sid": 11, "sentence": "- We agree that the observed phenomenon can appear in other machine learning methods and is not specific to gradient descent."}, {"text_id": "SyeGJ1qEA7", "sid": 12, "sentence": "However, in the case of deep neural networks, it is the prevalence of certain gradient directions that determine the final classifier."}, {"text_id": "SyeGJ1qEA7", "sid": 13, "sentence": "Our results suggests that models converge to solutions that privilege the \u201csimplest\u201d explanation, in an Occam\u2019s razor fashion, which provides an explanation to the \u201cimplicit generalization\u201d of deep nets characterized by Zhang et al."}, {"text_id": "SyeGJ1qEA7", "sid": 14, "sentence": "Our Kaggle experiment\u2019s aim is to emphasize potential failure modes of current architectures/algorithms (one can think of a self-driving car trained on a road with clear lane markings and operating on a road without such markings)."}, {"text_id": "SyeGJ1qEA7", "sid": 15, "sentence": "The ability to transfer knowledge to test sets coming from a different distribution is key to building more intelligent and robust systems."}], "rebuttallabels": [{"labels": {"alignments": [], "responsetype": "social", "coarseresponse": "nonarg"}, "text_id": "SyeGJ1qEA7", "sid": 0}, {"labels": {"alignments": [7], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 1}, {"labels": {"alignments": [7], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SyeGJ1qEA7", "sid": 2}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 3}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 4}, {"labels": {"alignments": [7], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 5}, {"labels": {"alignments": [], "responsetype": "structuring", "coarseresponse": "nonarg"}, "text_id": "SyeGJ1qEA7", "sid": 6}, {"labels": {"alignments": [18], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 7}, {"labels": {"alignments": [19], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 8}, {"labels": {"alignments": [20], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 9}, {"labels": {"alignments": [21], "responsetype": "done_manu_Yes", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 10}, {"labels": {"alignments": [22], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "SyeGJ1qEA7", "sid": 11}, {"labels": {"alignments": [22], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "SyeGJ1qEA7", "sid": 12}, {"labels": {"alignments": [23, 24], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 13}, {"labels": {"alignments": [23, 24], "responsetype": "answer", "coarseresponse": "concur"}, "text_id": "SyeGJ1qEA7", "sid": 14}, {"labels": {"alignments": [24], "responsetype": "reject-criticism", "coarseresponse": "dispute"}, "text_id": "SyeGJ1qEA7", "sid": 15}], "metadata": {"anno": "anno3", "review": "HJlqXnE5nQ", "rebuttal": "SyeGJ1qEA7", "conference": "ICLR2019", "title": "Convergence Properties of Deep Neural Networks on Separable Data", "reviewer": "AnonReviewer1", "forum_id": "HJfQrs0qt7", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}